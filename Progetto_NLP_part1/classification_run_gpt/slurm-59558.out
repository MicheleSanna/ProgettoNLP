Node IP: 10.128.2.151
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 1983
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.151:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 1983
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.151:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_b6_z3b8u/1983_zob9s5gn
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_2s6mf0br/1983_pwhac_nx
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu001.hpc
  master_port=43011
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu001.hpc
  master_port=43011
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_b6_z3b8u/1983_zob9s5gn/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_b6_z3b8u/1983_zob9s5gn/attempt_0/1/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_2s6mf0br/1983_pwhac_nx/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_2s6mf0br/1983_pwhac_nx/attempt_0/1/error.json
/u/dssc/msanna00/.conda/envs/deeplearning3/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/u/dssc/msanna00/.conda/envs/deeplearning3/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
PORT:  43011
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  0
My rank is:  0
PORT:  43011
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  0
My rank is:  1
PORT:  43011
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  1
My rank is:  2
PORT:  43011
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  1
My rank is:  3
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
------------------------

------------------------

------------------------

------------------------

Loading checkpoint...
Loading checkpoint...
Loading checkpoint...Loading checkpoint...

Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 0 using GPU 0
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 1 using GPU 1
Labels:  tensor([2, 2, 1, 1, 1, 4, 4, 4, 3, 2, 0, 3, 3, 4, 0, 3], device='cuda:0')
Preds:  tensor([4, 3, 2, 1, 0, 4, 4, 4, 2, 2, 1, 2, 4, 4, 0, 3], device='cuda:0')
Outputs:  tensor([[    0.0002,     0.0007,     0.0277,     0.4322,     0.5392],
        [    0.0004,     0.0031,     0.0628,     0.8041,     0.1297],
        [    0.0124,     0.0762,     0.5001,     0.3358,     0.0755],
        [    0.0949,     0.4975,     0.3801,     0.0264,     0.0011],
        [    0.6612,     0.3163,     0.0217,     0.0005,     0.0003],
        [    0.0002,     0.0003,     0.0057,     0.1182,     0.8756],
        [    0.0009,     0.0003,     0.0027,     0.0560,     0.9400],
        [    0.0025,     0.0027,     0.0160,     0.1302,     0.8485],
        [    0.0094,     0.0593,     0.6898,     0.2040,     0.0374],
        [    0.0057,     0.0839,     0.5617,     0.3346,     0.0139],
        [    0.0386,     0.5210,     0.3844,     0.0447,     0.0114],
        [    0.0029,     0.1266,     0.5285,     0.2567,     0.0853],
        [    0.0002,     0.0013,     0.0156,     0.3516,     0.6313],
        [    0.0001,     0.0006,     0.0021,     0.1285,     0.8686],
        [    0.9396,     0.0594,     0.0010,     0.0000,     0.0000],
        [    0.0000,     0.0001,     0.0270,     0.8755,     0.0974]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 2 using GPU 0
LOADED!
I'm process 3 using GPU 1
Labels:  tensor([0, 4, 2, 3, 0, 1, 2, 2, 3, 2, 2, 4, 4, 1, 1, 2], device='cuda:1')
Preds:  tensor([0, 4, 1, 4, 0, 2, 4, 2, 1, 2, 0, 4, 4, 1, 1, 4], device='cuda:1')
Outputs:  tensor([[    0.5910,     0.3958,     0.0131,     0.0001,     0.0001],
        [    0.0006,     0.0003,     0.0014,     0.0856,     0.9121],
        [    0.0164,     0.7001,     0.1900,     0.0718,     0.0217],
        [    0.0001,     0.0000,     0.0002,     0.0203,     0.9794],
        [    0.9959,     0.0036,     0.0003,     0.0000,     0.0002],
        [    0.0767,     0.3306,     0.4481,     0.1352,     0.0093],
        [    0.0029,     0.0159,     0.0755,     0.3369,     0.5688],
        [    0.0012,     0.0278,     0.5996,     0.3614,     0.0100],
        [    0.1256,     0.5087,     0.3441,     0.0207,     0.0010],
        [    0.0071,     0.0846,     0.5692,     0.3078,     0.0314],
        [    0.6584,     0.2712,     0.0620,     0.0036,     0.0048],
        [    0.0002,     0.0002,     0.0032,     0.1303,     0.8661],
        [    0.0004,     0.0001,     0.0002,     0.0024,     0.9968],
        [    0.0096,     0.5409,     0.3872,     0.0548,     0.0075],
        [    0.0008,     0.9980,     0.0008,     0.0003,     0.0001],
        [    0.0377,     0.0224,     0.0545,     0.2528,     0.6326]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([3, 0, 0, 1, 0, 3, 0, 0, 3, 1, 4, 4, 4, 1, 2, 0], device='cuda:1')
Preds:  tensor([3, 0, 2, 1, 0, 2, 0, 1, 3, 1, 0, 2, 4, 1, 2, 0], device='cuda:1')
Outputs:  tensor([[    0.0004,     0.0011,     0.0366,     0.5727,     0.3892],
        [    0.7017,     0.2556,     0.0359,     0.0025,     0.0044],
        [    0.1911,     0.3601,     0.4029,     0.0390,     0.0070],
        [    0.2163,     0.5771,     0.2014,     0.0045,     0.0007],
        [    0.8087,     0.1660,     0.0251,     0.0002,     0.0000],
        [    0.0944,     0.3684,     0.4774,     0.0530,     0.0068],
        [    0.8966,     0.0977,     0.0057,     0.0000,     0.0000],
        [    0.2578,     0.6989,     0.0407,     0.0022,     0.0005],
        [    0.0000,     0.0016,     0.1606,     0.7416,     0.0962],
        [    0.1577,     0.5844,     0.2477,     0.0091,     0.0010],
        [    0.2756,     0.1727,     0.2729,     0.1116,     0.1671],
        [    0.1134,     0.1016,     0.3810,     0.2518,     0.1522],
        [    0.0005,     0.0004,     0.0049,     0.1764,     0.8178],
        [    0.2291,     0.5394,     0.1327,     0.0563,     0.0426],
        [    0.0073,     0.1846,     0.7465,     0.0577,     0.0039],
        [    0.9892,     0.0107,     0.0001,     0.0000,     0.0000]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([3, 1, 1, 4, 0, 2, 4, 1, 3, 0, 2, 3, 2, 2, 4, 4], device='cuda:0')
Preds:  tensor([3, 1, 1, 4, 0, 2, 4, 1, 2, 0, 1, 4, 1, 2, 4, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0028,     0.0307,     0.9658,     0.0006],
        [    0.4415,     0.4904,     0.0667,     0.0010,     0.0003],
        [    0.3490,     0.4554,     0.1816,     0.0120,     0.0020],
        [    0.0015,     0.0006,     0.0042,     0.0748,     0.9188],
        [    0.5573,     0.1555,     0.1217,     0.0670,     0.0985],
        [    0.0030,     0.1608,     0.7032,     0.1304,     0.0026],
        [    0.0060,     0.0030,     0.0143,     0.1839,     0.7928],
        [    0.4073,     0.5467,     0.0458,     0.0001,     0.0000],
        [    0.0002,     0.0046,     0.5008,     0.4751,     0.0193],
        [    0.5301,     0.2459,     0.1884,     0.0351,     0.0006],
        [    0.1176,     0.5312,     0.3432,     0.0071,     0.0009],
        [    0.0004,     0.0003,     0.0038,     0.2129,     0.7826],
        [    0.0405,     0.6691,     0.2880,     0.0024,     0.0000],
        [    0.0484,     0.2608,     0.4943,     0.1467,     0.0498],
        [    0.0002,     0.0004,     0.0169,     0.4417,     0.5408],
        [    0.0004,     0.0003,     0.0030,     0.1172,     0.8791]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 1, 1, 2, 1, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Preds:  tensor([3, 2, 2, 4, 1, 2, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Outputs:  tensor([[    0.0027,     0.0292,     0.3770,     0.4708,     0.1204],
        [    0.2689,     0.3329,     0.3593,     0.0351,     0.0039],
        [    0.0492,     0.4547,     0.4682,     0.0262,     0.0016],
        [    0.2262,     0.1289,     0.1694,     0.2204,     0.2552],
        [    0.0210,     0.4561,     0.4407,     0.0729,     0.0093],
        [    0.0545,     0.3519,     0.5052,     0.0646,     0.0238],
        [    0.9290,     0.0572,     0.0099,     0.0012,     0.0027],
        [    0.8573,     0.1391,     0.0034,     0.0001,     0.0001],
        [    0.0015,     0.0008,     0.0048,     0.1570,     0.8359],
        [    0.0005,     0.0002,     0.0012,     0.0444,     0.9538],
        [    0.0095,     0.2004,     0.7341,     0.0553,     0.0007],
        [    0.3067,     0.5193,     0.1603,     0.0089,     0.0048],
        [    0.0001,     0.0021,     0.1478,     0.8495,     0.0004],
        [    0.5216,     0.4455,     0.0319,     0.0010,     0.0000],
        [    0.7048,     0.1906,     0.0604,     0.0175,     0.0267],
        [    0.1740,     0.5960,     0.2244,     0.0052,     0.0004]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 0, 4, 0, 0, 0, 1, 2], device='cuda:1')
Preds:  tensor([3, 0, 4, 4, 4, 0, 0, 2, 3, 1, 3, 4, 0, 0, 2, 4], device='cuda:1')
Outputs:  tensor([[    0.0524,     0.0958,     0.3616,     0.4005,     0.0897],
        [    0.5153,     0.4322,     0.0496,     0.0019,     0.0011],
        [    0.0002,     0.0007,     0.0003,     0.0170,     0.9818],
        [    0.0002,     0.0001,     0.0008,     0.0803,     0.9186],
        [    0.0010,     0.0014,     0.0140,     0.2660,     0.7176],
        [    0.6376,     0.2889,     0.0676,     0.0041,     0.0019],
        [    0.7350,     0.2102,     0.0481,     0.0054,     0.0014],
        [    0.0475,     0.3078,     0.6242,     0.0201,     0.0004],
        [    0.0009,     0.0078,     0.1423,     0.5484,     0.3007],
        [    0.0629,     0.4894,     0.4339,     0.0130,     0.0009],
        [    0.0014,     0.0011,     0.0204,     0.6165,     0.3605],
        [    0.0918,     0.0870,     0.1267,     0.1659,     0.5286],
        [    0.5943,     0.3881,     0.0164,     0.0003,     0.0010],
        [    0.8861,     0.1123,     0.0015,     0.0000,     0.0000],
        [    0.0120,     0.2729,     0.6822,     0.0326,     0.0003],
        [    0.1068,     0.1443,     0.2619,     0.1607,     0.3263]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([0, 1, 4, 2, 3, 2, 1, 0, 2, 2, 2, 0, 2, 4, 3, 4], device='cuda:0')
Preds:  tensor([0, 1, 4, 1, 0, 2, 1, 0, 1, 3, 3, 0, 2, 4, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.5074,     0.4742,     0.0183,     0.0001,     0.0000],
        [    0.1093,     0.8666,     0.0237,     0.0003,     0.0000],
        [    0.0006,     0.0012,     0.0200,     0.2923,     0.6859],
        [    0.3465,     0.5289,     0.1217,     0.0026,     0.0003],
        [    0.5885,     0.2778,     0.1211,     0.0097,     0.0029],
        [    0.0110,     0.0200,     0.9436,     0.0192,     0.0062],
        [    0.1276,     0.4700,     0.3947,     0.0073,     0.0004],
        [    0.9342,     0.0619,     0.0035,     0.0002,     0.0003],
        [    0.3598,     0.4106,     0.2037,     0.0183,     0.0075],
        [    0.0005,     0.0046,     0.1586,     0.5952,     0.2411],
        [    0.0023,     0.0289,     0.3443,     0.5411,     0.0834],
        [    0.9520,     0.0468,     0.0011,     0.0001,     0.0000],
        [    0.0021,     0.0442,     0.5028,     0.4409,     0.0101],
        [    0.0004,     0.0006,     0.0107,     0.2164,     0.7718],
        [    0.4773,     0.4150,     0.1063,     0.0012,     0.0002],
        [    0.0003,     0.0006,     0.0004,     0.0192,     0.9796]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 1, 3, 4, 3, 0, 3, 1, 1, 0, 0, 3, 2, 0], device='cuda:1')
Preds:  tensor([3, 2, 3, 0, 2, 4, 3, 1, 4, 1, 1, 0, 0, 2, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0024,     0.0186,     0.3383,     0.6160,     0.0247],
        [    0.0012,     0.0380,     0.6267,     0.3190,     0.0152],
        [    0.0021,     0.0360,     0.2560,     0.6824,     0.0235],
        [    0.4860,     0.4495,     0.0636,     0.0008,     0.0002],
        [    0.0048,     0.1364,     0.8316,     0.0260,     0.0011],
        [    0.0005,     0.0006,     0.0079,     0.1930,     0.7980],
        [    0.0100,     0.0608,     0.3574,     0.4336,     0.1382],
        [    0.1522,     0.5984,     0.2452,     0.0041,     0.0001],
        [    0.0000,     0.0001,     0.0047,     0.3171,     0.6780],
        [    0.0450,     0.8257,     0.1094,     0.0169,     0.0030],
        [    0.1383,     0.5325,     0.3173,     0.0109,     0.0010],
        [    0.9221,     0.0698,     0.0075,     0.0003,     0.0003],
        [    0.6650,     0.2893,     0.0433,     0.0020,     0.0004],
        [    0.0059,     0.0817,     0.4559,     0.4212,     0.0352],
        [    0.0998,     0.3444,     0.4086,     0.1167,     0.0305],
        [    0.0401,     0.2347,     0.5493,     0.1696,     0.0063]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([4, 0, 4, 0, 4, 1, 1, 3, 0, 4, 4, 3, 0, 2, 3, 0], device='cuda:0')
Preds:  tensor([4, 0, 4, 0, 4, 1, 1, 4, 0, 4, 4, 3, 1, 1, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0007,     0.0005,     0.0052,     0.1320,     0.8616],
        [    0.7522,     0.2113,     0.0353,     0.0010,     0.0002],
        [    0.0005,     0.0006,     0.0103,     0.2980,     0.6907],
        [    0.7749,     0.2149,     0.0099,     0.0001,     0.0001],
        [    0.0286,     0.0212,     0.0698,     0.1980,     0.6825],
        [    0.0022,     0.9844,     0.0118,     0.0014,     0.0002],
        [    0.1071,     0.5801,     0.3053,     0.0069,     0.0005],
        [    0.0346,     0.2053,     0.1549,     0.1592,     0.4461],
        [    0.9861,     0.0135,     0.0004,     0.0000,     0.0000],
        [    0.0037,     0.0158,     0.1893,     0.3814,     0.4099],
        [    0.0004,     0.0168,     0.0076,     0.2303,     0.7449],
        [    0.0026,     0.0290,     0.4341,     0.4553,     0.0790],
        [    0.1617,     0.7807,     0.0572,     0.0003,     0.0000],
        [    0.1173,     0.5300,     0.3126,     0.0380,     0.0021],
        [    0.0002,     0.0003,     0.0062,     0.3311,     0.6622],
        [    0.9246,     0.0690,     0.0063,     0.0001,     0.0000]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Labels:  tensor([2, 3, 4, 0, 1, 3, 4, 3, 4, 2, 1, 0, 4, 4, 4, 4], device='cuda:1')
Preds:  tensor([2, 3, 4, 1, 1, 4, 4, 2, 4, 2, 0, 0, 4, 4, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0252,     0.4439,     0.5187,     0.0118,     0.0005],
        [    0.0001,     0.0012,     0.0747,     0.7458,     0.1781],
        [    0.0019,     0.0006,     0.0014,     0.0153,     0.9808],
        [    0.2204,     0.7109,     0.0664,     0.0015,     0.0007],
        [    0.1728,     0.3541,     0.3287,     0.1089,     0.0355],
        [    0.0045,     0.0059,     0.0454,     0.3051,     0.6390],
        [    0.0029,     0.0007,     0.0029,     0.0243,     0.9692],
        [    0.0062,     0.3676,     0.6013,     0.0242,     0.0007],
        [    0.0002,     0.0004,     0.0001,     0.0040,     0.9954],
        [    0.0013,     0.0424,     0.7810,     0.1691,     0.0063],
        [    0.5182,     0.4358,     0.0447,     0.0009,     0.0004],
        [    0.5959,     0.3624,     0.0409,     0.0006,     0.0001],
        [    0.0002,     0.0002,     0.0027,     0.1329,     0.8641],
        [    0.0016,     0.0014,     0.0114,     0.2131,     0.7724],
        [    0.0030,     0.0108,     0.1636,     0.5195,     0.3030],
        [    0.0005,     0.0004,     0.0034,     0.1568,     0.8390]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([3, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 3, 3, 4, 4, 2], device='cuda:0')
Preds:  tensor([3, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 4, 2, 4, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0007,     0.0283,     0.6532,     0.3178],
        [    0.0018,     0.0276,     0.8021,     0.1562,     0.0123],
        [    0.0020,     0.0323,     0.6283,     0.3214,     0.0160],
        [    0.0364,     0.2044,     0.6001,     0.1544,     0.0047],
        [    0.2797,     0.6448,     0.0736,     0.0015,     0.0004],
        [    0.0695,     0.1694,     0.4744,     0.2373,     0.0493],
        [    0.2034,     0.7167,     0.0790,     0.0009,     0.0001],
        [    0.9890,     0.0107,     0.0003,     0.0000,     0.0000],
        [    0.0009,     0.0204,     0.5532,     0.3993,     0.0262],
        [    0.0681,     0.2963,     0.4107,     0.1937,     0.0311],
        [    0.6048,     0.3531,     0.0418,     0.0003,     0.0000],
        [    0.0001,     0.0001,     0.0003,     0.0110,     0.9885],
        [    0.0021,     0.0640,     0.7217,     0.2066,     0.0056],
        [    0.0002,     0.0003,     0.0079,     0.3172,     0.6744],
        [    0.0052,     0.0064,     0.0562,     0.2691,     0.6631],
        [    0.7069,     0.2678,     0.0234,     0.0011,     0.0008]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 2, 3, 0, 2, 4, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:1')
Preds:  tensor([4, 2, 2, 2, 4, 0, 1, 4, 2, 0, 1, 2, 0, 0, 0, 0], device='cuda:1')
Outputs:  tensor([[    0.0012,     0.0017,     0.0186,     0.2182,     0.7603],
        [    0.0736,     0.2644,     0.5482,     0.1081,     0.0058],
        [    0.0011,     0.0399,     0.8794,     0.0790,     0.0007],
        [    0.0015,     0.0159,     0.9649,     0.0168,     0.0008],
        [    0.0004,     0.0005,     0.0059,     0.2323,     0.7609],
        [    0.6549,     0.3344,     0.0105,     0.0002,     0.0001],
        [    0.2887,     0.4550,     0.2290,     0.0229,     0.0044],
        [    0.0034,     0.0019,     0.0089,     0.1297,     0.8561],
        [    0.0153,     0.3747,     0.5182,     0.0841,     0.0077],
        [    0.7745,     0.0848,     0.0378,     0.0250,     0.0779],
        [    0.2011,     0.6313,     0.1651,     0.0024,     0.0001],
        [    0.2922,     0.2451,     0.4207,     0.0350,     0.0070],
        [    0.9778,     0.0151,     0.0052,     0.0007,     0.0013],
        [    0.6770,     0.2885,     0.0325,     0.0012,     0.0008],
        [    0.9869,     0.0129,     0.0002,     0.0000,     0.0000],
        [    0.9993,     0.0006,     0.0001,     0.0000,     0.0000]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([2, 0, 1, 3, 4, 2, 0, 0, 2, 1, 0, 1, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([4, 0, 2, 2, 4, 2, 1, 0, 1, 0, 0, 0, 0, 1, 4, 4], device='cuda:0')
Outputs:  tensor([[    0.0003,     0.0003,     0.0039,     0.1436,     0.8519],
        [    0.7870,     0.1767,     0.0337,     0.0019,     0.0007],
        [    0.0058,     0.1477,     0.7791,     0.0671,     0.0003],
        [    0.0025,     0.0934,     0.8592,     0.0443,     0.0005],
        [    0.0030,     0.0013,     0.0044,     0.0394,     0.9520],
        [    0.0057,     0.2319,     0.6179,     0.1399,     0.0046],
        [    0.3030,     0.5485,     0.1446,     0.0038,     0.0001],
        [    0.8446,     0.1313,     0.0235,     0.0005,     0.0001],
        [    0.0493,     0.7362,     0.1619,     0.0403,     0.0124],
        [    0.5314,     0.3405,     0.1017,     0.0130,     0.0135],
        [    0.9944,     0.0055,     0.0001,     0.0000,     0.0000],
        [    0.5964,     0.2777,     0.1180,     0.0078,     0.0002],
        [    0.6443,     0.2196,     0.1167,     0.0140,     0.0054],
        [    0.0145,     0.7618,     0.1749,     0.0404,     0.0085],
        [    0.0096,     0.0211,     0.1160,     0.3120,     0.5412],
        [    0.0001,     0.0001,     0.0002,     0.0373,     0.9623]],
       device='cuda:0')
Metric:  tensor(0.3750, device='cuda:0')
------------------------
Labels:  tensor([4, 4, 4, 2, 3, 2, 4, 1, 2, 1, 1, 2, 0, 1, 0, 1], device='cuda:1')
Preds:  tensor([4, 4, 4, 1, 2, 3, 4, 1, 2, 2, 1, 2, 0, 0, 0, 2], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0007,     0.0129,     0.1621,     0.8240],
        [    0.0004,     0.0003,     0.0054,     0.1811,     0.8129],
        [    0.0002,     0.0002,     0.0014,     0.0428,     0.9554],
        [    0.1604,     0.5038,     0.3300,     0.0057,     0.0002],
        [    0.0065,     0.0506,     0.6072,     0.2830,     0.0526],
        [    0.0009,     0.0076,     0.1552,     0.6937,     0.1425],
        [    0.0007,     0.0003,     0.0013,     0.0129,     0.9848],
        [    0.0747,     0.4412,     0.4138,     0.0659,     0.0045],
        [    0.0017,     0.0868,     0.8596,     0.0512,     0.0007],
        [    0.0271,     0.3190,     0.5733,     0.0791,     0.0014],
        [    0.2883,     0.3714,     0.2806,     0.0548,     0.0049],
        [    0.0055,     0.2312,     0.7422,     0.0208,     0.0004],
        [    0.9935,     0.0065,     0.0000,     0.0000,     0.0000],
        [    0.6274,     0.3407,     0.0311,     0.0007,     0.0001],
        [    0.7730,     0.2201,     0.0068,     0.0001,     0.0001],
        [    0.0082,     0.1370,     0.6657,     0.1857,     0.0033]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([4, 1, 1, 0, 0, 1, 2, 4, 2, 4, 4, 1, 0, 1, 2, 1], device='cuda:0')
Preds:  tensor([4, 1, 0, 0, 0, 1, 2, 4, 1, 4, 4, 4, 0, 0, 2, 0], device='cuda:0')
Outputs:  tensor([[    0.0015,     0.0014,     0.0071,     0.1474,     0.8427],
        [    0.2362,     0.6212,     0.1384,     0.0034,     0.0007],
        [    0.9213,     0.0761,     0.0025,     0.0000,     0.0000],
        [    0.8085,     0.1772,     0.0137,     0.0003,     0.0002],
        [    0.6570,     0.3233,     0.0193,     0.0004,     0.0001],
        [    0.0728,     0.7265,     0.1999,     0.0007,     0.0000],
        [    0.0434,     0.2861,     0.4998,     0.1483,     0.0224],
        [    0.0000,     0.0001,     0.0005,     0.1813,     0.8181],
        [    0.0369,     0.4901,     0.4624,     0.0106,     0.0000],
        [    0.0006,     0.0006,     0.0052,     0.2873,     0.7063],
        [    0.0458,     0.0344,     0.0949,     0.1151,     0.7099],
        [    0.2899,     0.1112,     0.1273,     0.1237,     0.3479],
        [    0.9090,     0.0899,     0.0011,     0.0000,     0.0000],
        [    0.5547,     0.3334,     0.1040,     0.0063,     0.0016],
        [    0.2286,     0.2929,     0.3132,     0.1122,     0.0531],
        [    0.5789,     0.3384,     0.0771,     0.0043,     0.0012]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 2, 1, 1, 4, 1, 4, 4, 1, 3, 2, 4, 3, 3, 4], device='cuda:1')
Preds:  tensor([2, 2, 2, 0, 1, 4, 0, 3, 4, 1, 2, 2, 4, 2, 4, 4], device='cuda:1')
Outputs:  tensor([[    0.0290,     0.1452,     0.4819,     0.2583,     0.0857],
        [    0.0657,     0.3588,     0.4904,     0.0815,     0.0037],
        [    0.0731,     0.3660,     0.4924,     0.0676,     0.0009],
        [    0.5311,     0.3585,     0.1082,     0.0020,     0.0003],
        [    0.2908,     0.6147,     0.0936,     0.0008,     0.0001],
        [    0.0081,     0.0081,     0.0137,     0.0920,     0.8781],
        [    0.5674,     0.3975,     0.0342,     0.0007,     0.0002],
        [    0.0000,     0.0001,     0.0089,     0.6823,     0.3087],
        [    0.0009,     0.0006,     0.0016,     0.0628,     0.9341],
        [    0.2291,     0.6722,     0.0964,     0.0021,     0.0001],
        [    0.0092,     0.1140,     0.7183,     0.1581,     0.0004],
        [    0.0016,     0.1238,     0.6351,     0.2304,     0.0091],
        [    0.0004,     0.0008,     0.0101,     0.3013,     0.6874],
        [    0.0012,     0.0408,     0.7780,     0.1734,     0.0065],
        [    0.0021,     0.0014,     0.0124,     0.1344,     0.8497],
        [    0.0003,     0.0011,     0.0241,     0.3680,     0.6065]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([4, 2, 2, 0, 0, 3, 3, 0, 0, 0, 0, 1, 4, 2, 0, 4], device='cuda:0')
Preds:  tensor([4, 4, 2, 0, 1, 3, 3, 1, 0, 0, 4, 0, 3, 2, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0010,     0.0021,     0.1230,     0.8738],
        [    0.0018,     0.0030,     0.0244,     0.2415,     0.7292],
        [    0.1201,     0.3656,     0.4200,     0.0867,     0.0076],
        [    0.9655,     0.0338,     0.0008,     0.0000,     0.0000],
        [    0.4263,     0.5522,     0.0207,     0.0004,     0.0004],
        [    0.0001,     0.0001,     0.0011,     0.9930,     0.0057],
        [    0.0001,     0.0023,     0.2015,     0.7400,     0.0561],
        [    0.3134,     0.5504,     0.0924,     0.0129,     0.0309],
        [    0.9261,     0.0730,     0.0009,     0.0000,     0.0000],
        [    0.7888,     0.2004,     0.0103,     0.0002,     0.0003],
        [    0.1172,     0.2065,     0.0535,     0.1364,     0.4863],
        [    0.5925,     0.3952,     0.0119,     0.0003,     0.0002],
        [    0.0014,     0.0048,     0.0301,     0.7474,     0.2164],
        [    0.1883,     0.2986,     0.3902,     0.0766,     0.0463],
        [    0.7895,     0.1818,     0.0242,     0.0024,     0.0021],
        [    0.0000,     0.0002,     0.0013,     0.1214,     0.8770]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Mean loss[0.9342590550523435] | Mean metric[0.5935822352367008]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.transformer.wte.weight
Freezed:  module.transformer.wpe.weight
Freezed:  module.transformer.h.0.ln_1.weight
Freezed:  module.transformer.h.0.ln_1.bias
Freezed:  module.transformer.h.0.attn.c_attn.weight
Freezed:  module.transformer.h.0.attn.c_attn.bias
Freezed:  module.transformer.h.0.attn.c_proj.weight
Freezed:  module.transformer.h.0.attn.c_proj.bias
Freezed:  module.transformer.h.0.ln_2.weight
Freezed:  module.transformer.h.0.ln_2.bias
Freezed:  module.transformer.h.0.mlp.c_fc.weight
Freezed:  module.transformer.h.0.mlp.c_fc.bias
Freezed:  module.transformer.h.0.mlp.c_proj.weight
Freezed:  module.transformer.h.0.mlp.c_proj.bias
Freezed:  module.transformer.h.1.ln_1.weight
Freezed:  module.transformer.h.1.ln_1.bias
Freezed:  module.transformer.h.1.attn.c_attn.weight
Freezed:  module.transformer.h.1.attn.c_attn.bias
Freezed:  module.transformer.h.1.attn.c_proj.weight
Freezed:  module.transformer.h.1.attn.c_proj.bias
Freezed:  module.transformer.h.1.ln_2.weight
Freezed:  module.transformer.h.1.ln_2.bias
Freezed:  module.transformer.h.1.mlp.c_fc.weight
Freezed:  module.transformer.h.1.mlp.c_fc.bias
Freezed:  module.transformer.h.1.mlp.c_proj.weight
Freezed:  module.transformer.h.1.mlp.c_proj.bias
Freezed:  module.transformer.h.2.ln_1.weight
Freezed:  module.transformer.h.2.ln_1.bias
Freezed:  module.transformer.h.2.attn.c_attn.weight
Freezed:  module.transformer.h.2.attn.c_attn.bias
Freezed:  module.transformer.h.2.attn.c_proj.weight
Freezed:  module.transformer.h.2.attn.c_proj.bias
Freezed:  module.transformer.h.2.ln_2.weight
Freezed:  module.transformer.h.2.ln_2.bias
Freezed:  module.transformer.h.2.mlp.c_fc.weight
Freezed:  module.transformer.h.2.mlp.c_fc.bias
Freezed:  module.transformer.h.2.mlp.c_proj.weight
Freezed:  module.transformer.h.2.mlp.c_proj.bias
Freezed:  module.transformer.h.3.ln_1.weight
Freezed:  module.transformer.h.3.ln_1.bias
Freezed:  module.transformer.h.3.attn.c_attn.weight
Freezed:  module.transformer.h.3.attn.c_attn.bias
Freezed:  module.transformer.h.3.attn.c_proj.weight
Freezed:  module.transformer.h.3.attn.c_proj.bias
Freezed:  module.transformer.h.3.ln_2.weight
Freezed:  module.transformer.h.3.ln_2.bias
Freezed:  module.transformer.h.3.mlp.c_fc.weight
Freezed:  module.transformer.h.3.mlp.c_fc.bias
Freezed:  module.transformer.h.3.mlp.c_proj.weight
Freezed:  module.transformer.h.3.mlp.c_proj.bias
Freezed:  module.transformer.h.4.ln_1.weight
Freezed:  module.transformer.h.4.ln_1.bias
Freezed:  module.transformer.h.4.attn.c_attn.weight
Freezed:  module.transformer.h.4.attn.c_attn.bias
Freezed:  module.transformer.h.4.attn.c_proj.weight
Freezed:  module.transformer.h.4.attn.c_proj.bias
Freezed:  module.transformer.h.4.ln_2.weight
Freezed:  module.transformer.h.4.ln_2.bias
Freezed:  module.transformer.h.4.mlp.c_fc.weight
Freezed:  module.transformer.h.4.mlp.c_fc.bias
Freezed:  module.transformer.h.4.mlp.c_proj.weight
Freezed:  module.transformer.h.4.mlp.c_proj.bias
Freezed:  module.transformer.h.5.ln_1.weight
Freezed:  module.transformer.h.5.ln_1.bias
Freezed:  module.transformer.h.5.attn.c_attn.weight
Freezed:  module.transformer.h.5.attn.c_attn.bias
Freezed:  module.transformer.h.5.attn.c_proj.weight
Freezed:  module.transformer.h.5.attn.c_proj.bias
Freezed:  module.transformer.h.5.ln_2.weight
Freezed:  module.transformer.h.5.ln_2.bias
Freezed:  module.transformer.h.5.mlp.c_fc.weight
Freezed:  module.transformer.h.5.mlp.c_fc.bias
Freezed:  module.transformer.h.5.mlp.c_proj.weight
Freezed:  module.transformer.h.5.mlp.c_proj.bias
EPOCH 1
--------------
Labels:  tensor([4, 4, 2, 0, 0, 4, 4, 1, 4, 2, 0, 1, 1, 2, 2, 1], device='cuda:1')
Preds:  tensor([4, 4, 1, 3, 0, 4, 4, 1, 4, 2, 3, 1, 2, 1, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0018,     0.0009,     0.0065,     0.1074,     0.8834],
        [    0.0013,     0.0011,     0.0041,     0.0949,     0.8987],
        [    0.2047,     0.6800,     0.1108,     0.0041,     0.0004],
        [    0.0016,     0.0059,     0.0897,     0.4561,     0.4467],
        [    0.3988,     0.2166,     0.2521,     0.0733,     0.0593],
        [    0.0010,     0.0005,     0.0020,     0.0497,     0.9468],
        [    0.0017,     0.0013,     0.0085,     0.1646,     0.8239],
        [    0.1593,     0.6935,     0.1434,     0.0035,     0.0003],
        [    0.0004,     0.0008,     0.0186,     0.2381,     0.7422],
        [    0.0861,     0.2468,     0.5205,     0.1340,     0.0127],
        [    0.0018,     0.0216,     0.3047,     0.6107,     0.0612],
        [    0.2963,     0.4224,     0.2125,     0.0501,     0.0187],
        [    0.0043,     0.0824,     0.8384,     0.0717,     0.0033],
        [    0.1403,     0.5380,     0.2948,     0.0252,     0.0016],
        [    0.0284,     0.2150,     0.4635,     0.2388,     0.0544],
        [    0.0140,     0.3419,     0.6037,     0.0386,     0.0018]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Mean loss[0.9315869401378595] | Mean metric[0.5923621278672523]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.transformer.wte.weight
Freezed:  module.transformer.wpe.weight
Freezed:  module.transformer.h.0.ln_1.weight
Freezed:  module.transformer.h.0.ln_1.bias
Freezed:  module.transformer.h.0.attn.c_attn.weight
Freezed:  module.transformer.h.0.attn.c_attn.bias
Freezed:  module.transformer.h.0.attn.c_proj.weight
Freezed:  module.transformer.h.0.attn.c_proj.bias
Freezed:  module.transformer.h.0.ln_2.weight
Freezed:  module.transformer.h.0.ln_2.bias
Freezed:  module.transformer.h.0.mlp.c_fc.weight
Freezed:  module.transformer.h.0.mlp.c_fc.bias
Freezed:  module.transformer.h.0.mlp.c_proj.weight
Freezed:  module.transformer.h.0.mlp.c_proj.bias
Freezed:  module.transformer.h.1.ln_1.weight
Freezed:  module.transformer.h.1.ln_1.bias
Freezed:  module.transformer.h.1.attn.c_attn.weight
Freezed:  module.transformer.h.1.attn.c_attn.bias
Freezed:  module.transformer.h.1.attn.c_proj.weight
Freezed:  module.transformer.h.1.attn.c_proj.bias
Freezed:  module.transformer.h.1.ln_2.weight
Freezed:  module.transformer.h.1.ln_2.bias
Freezed:  module.transformer.h.1.mlp.c_fc.weight
Freezed:  module.transformer.h.1.mlp.c_fc.bias
Freezed:  module.transformer.h.1.mlp.c_proj.weight
Freezed:  module.transformer.h.1.mlp.c_proj.bias
Freezed:  module.transformer.h.2.ln_1.weight
Freezed:  module.transformer.h.2.ln_1.bias
Freezed:  module.transformer.h.2.attn.c_attn.weight
Freezed:  module.transformer.h.2.attn.c_attn.bias
Freezed:  module.transformer.h.2.attn.c_proj.weight
Freezed:  module.transformer.h.2.attn.c_proj.bias
Freezed:  module.transformer.h.2.ln_2.weight
Freezed:  module.transformer.h.2.ln_2.bias
Labels:  tensor([1, 1, 2, 4, 2, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 0], device='cuda:0')
Freezed:  module.transformer.h.2.mlp.c_fc.weight
Preds:  tensor([1, 2, 2, 4, 0, 4, 0, 4, 4, 2, 1, 2, 0, 3, 4, 1], device='cuda:0')
Freezed:  module.transformer.h.2.mlp.c_fc.bias
Outputs:  tensor([[    0.1516,     0.5267,     0.3186,     0.0028,     0.0003],
        [    0.0628,     0.2565,     0.4052,     0.1778,     0.0977],
        [    0.0001,     0.0032,     0.9955,     0.0013,     0.0000],
        [    0.0001,     0.0002,     0.0003,     0.0582,     0.9413],
        [    0.5609,     0.3080,     0.1182,     0.0110,     0.0019],
        [    0.0004,     0.0002,     0.0005,     0.0170,     0.9819],
        [    0.9734,     0.0263,     0.0002,     0.0000,     0.0001],
        [    0.0003,     0.0002,     0.0023,     0.0938,     0.9034],
        [    0.0001,     0.0001,     0.0046,     0.3549,     0.6403],
        [    0.0483,     0.4561,     0.4881,     0.0073,     0.0001],
        [    0.3494,     0.3677,     0.2167,     0.0635,     0.0027],
        [    0.0131,     0.1660,     0.6642,     0.1505,     0.0062],
        [    0.8108,     0.1393,     0.0287,     0.0056,     0.0156],
        [    0.0017,     0.0026,     0.0324,     0.5460,     0.4172],
Freezed:  module.transformer.h.2.mlp.c_proj.weight
        [    0.0082,     0.0024,     0.0071,     0.0314,     0.9509],
        [    0.0725,     0.6062,     0.3078,     0.0133,     0.0002]],
       device='cuda:0')
Freezed:  module.transformer.h.2.mlp.c_proj.bias
Metric:  tensor(0.4375, device='cuda:0')
------------------------
Freezed:  module.transformer.h.3.ln_1.weight
Mean loss[0.9331044855933471] | Mean metric[0.5967850170815032]
Freezed:  module.transformer.h.3.ln_1.bias
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.transformer.h.3.attn.c_attn.weight
Freezed:  module.transformer.wte.weight
Freezed:  module.transformer.h.3.attn.c_attn.bias
Freezed:  module.transformer.wpe.weight
Freezed:  module.transformer.h.3.attn.c_proj.weight
Freezed:  module.transformer.h.0.ln_1.weight
Freezed:  module.transformer.h.3.attn.c_proj.bias
Freezed:  module.transformer.h.0.ln_1.bias
Freezed:  module.transformer.h.3.ln_2.weight
Freezed:  module.transformer.h.0.attn.c_attn.weight
Freezed:  module.transformer.h.3.ln_2.bias
Freezed:  module.transformer.h.0.attn.c_attn.bias
Freezed:  module.transformer.h.3.mlp.c_fc.weight
Freezed:  module.transformer.h.0.attn.c_proj.weight
Freezed:  module.transformer.h.3.mlp.c_fc.bias
Freezed:  module.transformer.h.0.attn.c_proj.bias
Freezed:  module.transformer.h.3.mlp.c_proj.weight
Freezed:  module.transformer.h.0.ln_2.weight
Freezed:  module.transformer.h.3.mlp.c_proj.bias
Freezed:  module.transformer.h.0.ln_2.bias
Freezed:  module.transformer.h.4.ln_1.weight
Freezed:  module.transformer.h.0.mlp.c_fc.weight
Freezed:  module.transformer.h.4.ln_1.bias
Freezed:  module.transformer.h.0.mlp.c_fc.bias
Freezed:  module.transformer.h.4.attn.c_attn.weight
Freezed:  module.transformer.h.0.mlp.c_proj.weight
Freezed:  module.transformer.h.4.attn.c_attn.bias
Freezed:  module.transformer.h.0.mlp.c_proj.bias
Freezed:  module.transformer.h.4.attn.c_proj.weight
Freezed:  module.transformer.h.1.ln_1.weight
Freezed:  module.transformer.h.4.attn.c_proj.bias
Freezed:  module.transformer.h.1.ln_1.bias
Freezed:  module.transformer.h.4.ln_2.weight
Freezed:  module.transformer.h.1.attn.c_attn.weight
Freezed:  module.transformer.h.4.ln_2.bias
Freezed:  module.transformer.h.1.attn.c_attn.bias
Freezed:  module.transformer.h.4.mlp.c_fc.weight
Freezed:  module.transformer.h.1.attn.c_proj.weight
Freezed:  module.transformer.h.4.mlp.c_fc.bias
Freezed:  module.transformer.h.1.attn.c_proj.bias
Freezed:  module.transformer.h.4.mlp.c_proj.weight
Freezed:  module.transformer.h.1.ln_2.weight
Freezed:  module.transformer.h.4.mlp.c_proj.bias
Freezed:  module.transformer.h.1.ln_2.bias
Freezed:  module.transformer.h.5.ln_1.weight
Freezed:  module.transformer.h.1.mlp.c_fc.weight
Freezed:  module.transformer.h.5.ln_1.bias
Freezed:  module.transformer.h.1.mlp.c_fc.bias
Freezed:  module.transformer.h.5.attn.c_attn.weight
Freezed:  module.transformer.h.1.mlp.c_proj.weight
Freezed:  module.transformer.h.1.mlp.c_proj.bias
Freezed:  module.transformer.h.5.attn.c_attn.bias
Freezed:  module.transformer.h.2.ln_1.weight
Freezed:  module.transformer.h.5.attn.c_proj.weight
Freezed:  module.transformer.h.2.ln_1.bias
Freezed:  module.transformer.h.5.attn.c_proj.bias
Freezed:  module.transformer.h.2.attn.c_attn.weight
Freezed:  module.transformer.h.5.ln_2.weight
Freezed:  module.transformer.h.2.attn.c_attn.bias
Freezed:  module.transformer.h.5.ln_2.bias
Freezed:  module.transformer.h.2.attn.c_proj.weight
Freezed:  module.transformer.h.2.attn.c_proj.bias
Freezed:  module.transformer.h.5.mlp.c_fc.weight
Freezed:  module.transformer.h.2.ln_2.weight
Freezed:  module.transformer.h.5.mlp.c_fc.bias
Freezed:  module.transformer.h.2.ln_2.bias
Freezed:  module.transformer.h.2.mlp.c_fc.weight
Freezed:  module.transformer.h.2.mlp.c_fc.bias
Freezed:  module.transformer.h.5.mlp.c_proj.weight
Freezed:  module.transformer.h.2.mlp.c_proj.weight
Freezed:  module.transformer.h.5.mlp.c_proj.bias
Freezed:  module.transformer.h.2.mlp.c_proj.bias
EPOCH 1
--------------
Freezed:  module.transformer.h.3.ln_1.weight
Freezed:  module.transformer.h.3.ln_1.bias
Freezed:  module.transformer.h.3.attn.c_attn.weight
Freezed:  module.transformer.h.3.attn.c_attn.bias
Freezed:  module.transformer.h.3.attn.c_proj.weight
Freezed:  module.transformer.h.3.attn.c_proj.bias
Freezed:  module.transformer.h.3.ln_2.weight
Freezed:  module.transformer.h.3.ln_2.bias
Freezed:  module.transformer.h.3.mlp.c_fc.weight
Freezed:  module.transformer.h.3.mlp.c_fc.bias
Freezed:  module.transformer.h.3.mlp.c_proj.weight
Freezed:  module.transformer.h.3.mlp.c_proj.bias
Freezed:  module.transformer.h.4.ln_1.weight
Freezed:  module.transformer.h.4.ln_1.bias
Freezed:  module.transformer.h.4.attn.c_attn.weight
Freezed:  module.transformer.h.4.attn.c_attn.bias
Freezed:  module.transformer.h.4.attn.c_proj.weight
Freezed:  module.transformer.h.4.attn.c_proj.bias
Freezed:  module.transformer.h.4.ln_2.weight
Freezed:  module.transformer.h.4.ln_2.bias
Freezed:  module.transformer.h.4.mlp.c_fc.weight
Freezed:  module.transformer.h.4.mlp.c_fc.bias
Freezed:  module.transformer.h.4.mlp.c_proj.weight
Freezed:  module.transformer.h.4.mlp.c_proj.bias
Freezed:  module.transformer.h.5.ln_1.weight
Freezed:  module.transformer.h.5.ln_1.bias
Freezed:  module.transformer.h.5.attn.c_attn.weight
Freezed:  module.transformer.h.5.attn.c_attn.bias
Freezed:  module.transformer.h.5.attn.c_proj.weight
Freezed:  module.transformer.h.5.attn.c_proj.bias
Freezed:  module.transformer.h.5.ln_2.weight
Freezed:  module.transformer.h.5.ln_2.bias
Freezed:  module.transformer.h.5.mlp.c_fc.weight
Freezed:  module.transformer.h.5.mlp.c_fc.bias
Freezed:  module.transformer.h.5.mlp.c_proj.weight
Freezed:  module.transformer.h.5.mlp.c_proj.bias
EPOCH 1
--------------
Labels:  tensor([0, 4, 0, 3, 0, 1, 2, 1, 4, 2, 1, 1, 4, 3, 1, 3], device='cuda:1')
Preds:  tensor([1, 4, 0, 3, 0, 0, 1, 2, 2, 2, 2, 2, 4, 3, 1, 2], device='cuda:1')
Outputs:  tensor([[    0.4375,     0.5407,     0.0179,     0.0007,     0.0032],
        [    0.0019,     0.0037,     0.0189,     0.1489,     0.8266],
        [    0.8503,     0.1351,     0.0123,     0.0016,     0.0008],
        [    0.0000,     0.0019,     0.0426,     0.5976,     0.3579],
        [    0.7594,     0.2378,     0.0028,     0.0000,     0.0000],
        [    0.6993,     0.2682,     0.0308,     0.0015,     0.0002],
        [    0.1508,     0.4250,     0.3010,     0.0784,     0.0448],
        [    0.0301,     0.4113,     0.5408,     0.0162,     0.0017],
        [    0.1183,     0.1974,     0.4186,     0.2261,     0.0396],
        [    0.0018,     0.0358,     0.5453,     0.3844,     0.0328],
        [    0.0317,     0.4683,     0.4715,     0.0252,     0.0034],
        [    0.0199,     0.1810,     0.7177,     0.0777,     0.0036],
        [    0.0002,     0.0002,     0.0027,     0.2288,     0.7681],
        [    0.0000,     0.0004,     0.0840,     0.8568,     0.0587],
        [    0.1265,     0.5154,     0.3306,     0.0220,     0.0055],
        [    0.0087,     0.2894,     0.3111,     0.2787,     0.1122]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Mean loss[0.9259531545807758] | Mean metric[0.5950158613958029]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.transformer.wte.weight
Freezed:  module.transformer.wpe.weight
Freezed:  module.transformer.h.0.ln_1.weight
Freezed:  module.transformer.h.0.ln_1.bias
Freezed:  module.transformer.h.0.attn.c_attn.weight
Freezed:  module.transformer.h.0.attn.c_attn.bias
Freezed:  module.transformer.h.0.attn.c_proj.weight
Freezed:  module.transformer.h.0.attn.c_proj.bias
Freezed:  module.transformer.h.0.ln_2.weight
Freezed:  module.transformer.h.0.ln_2.bias
Freezed:  module.transformer.h.0.mlp.c_fc.weight
Freezed:  module.transformer.h.0.mlp.c_fc.bias
Freezed:  module.transformer.h.0.mlp.c_proj.weight
Freezed:  module.transformer.h.0.mlp.c_proj.bias
Freezed:  module.transformer.h.1.ln_1.weight
Freezed:  module.transformer.h.1.ln_1.bias
Freezed:  module.transformer.h.1.attn.c_attn.weight
Freezed:  module.transformer.h.1.attn.c_attn.bias
Freezed:  module.transformer.h.1.attn.c_proj.weight
Freezed:  module.transformer.h.1.attn.c_proj.bias
Freezed:  module.transformer.h.1.ln_2.weight
Freezed:  module.transformer.h.1.ln_2.bias
Freezed:  module.transformer.h.1.mlp.c_fc.weight
Freezed:  module.transformer.h.1.mlp.c_fc.bias
Freezed:  module.transformer.h.1.mlp.c_proj.weight
Freezed:  module.transformer.h.1.mlp.c_proj.bias
Freezed:  module.transformer.h.2.ln_1.weight
Freezed:  module.transformer.h.2.ln_1.bias
Freezed:  module.transformer.h.2.attn.c_attn.weight
Freezed:  module.transformer.h.2.attn.c_attn.bias
Freezed:  module.transformer.h.2.attn.c_proj.weight
Freezed:  module.transformer.h.2.attn.c_proj.bias
Freezed:  module.transformer.h.2.ln_2.weight
Freezed:  module.transformer.h.2.ln_2.bias
Freezed:  module.transformer.h.2.mlp.c_fc.weight
Freezed:  module.transformer.h.2.mlp.c_fc.bias
Freezed:  module.transformer.h.2.mlp.c_proj.weight
Freezed:  module.transformer.h.2.mlp.c_proj.bias
Freezed:  module.transformer.h.3.ln_1.weight
Freezed:  module.transformer.h.3.ln_1.bias
Freezed:  module.transformer.h.3.attn.c_attn.weight
Freezed:  module.transformer.h.3.attn.c_attn.bias
Freezed:  module.transformer.h.3.attn.c_proj.weight
Freezed:  module.transformer.h.3.attn.c_proj.bias
Freezed:  module.transformer.h.3.ln_2.weight
Freezed:  module.transformer.h.3.ln_2.bias
Freezed:  module.transformer.h.3.mlp.c_fc.weight
Freezed:  module.transformer.h.3.mlp.c_fc.bias
Freezed:  module.transformer.h.3.mlp.c_proj.weight
Freezed:  module.transformer.h.3.mlp.c_proj.bias
Freezed:  module.transformer.h.4.ln_1.weight
Freezed:  module.transformer.h.4.ln_1.bias
Freezed:  module.transformer.h.4.attn.c_attn.weight
Freezed:  module.transformer.h.4.attn.c_attn.bias
Freezed:  module.transformer.h.4.attn.c_proj.weight
Freezed:  module.transformer.h.4.attn.c_proj.bias
Freezed:  module.transformer.h.4.ln_2.weight
Freezed:  module.transformer.h.4.ln_2.bias
Freezed:  module.transformer.h.4.mlp.c_fc.weight
Freezed:  module.transformer.h.4.mlp.c_fc.bias
Freezed:  module.transformer.h.4.mlp.c_proj.weight
Freezed:  module.transformer.h.4.mlp.c_proj.bias
Freezed:  module.transformer.h.5.ln_1.weight
Freezed:  module.transformer.h.5.ln_1.bias
Freezed:  module.transformer.h.5.attn.c_attn.weight
Freezed:  module.transformer.h.5.attn.c_attn.bias
Freezed:  module.transformer.h.5.attn.c_proj.weight
Freezed:  module.transformer.h.5.attn.c_proj.bias
Freezed:  module.transformer.h.5.ln_2.weight
Freezed:  module.transformer.h.5.ln_2.bias
Freezed:  module.transformer.h.5.mlp.c_fc.weight
Freezed:  module.transformer.h.5.mlp.c_fc.bias
Freezed:  module.transformer.h.5.mlp.c_proj.weight
Freezed:  module.transformer.h.5.mlp.c_proj.bias
EPOCH 1
--------------
Step[500] | Loss[0.7736924290657043] | Lr[1e-05]
Step[500] | Loss[0.8653640747070312] | Lr[1e-05]
Step[500] | Loss[0.553242027759552] | Lr[1e-05]
Step[500] | Loss[0.8276160359382629] | Lr[1e-05]
Step[1000] | Loss[0.722880482673645] | Lr[1e-05]
Step[1000] | Loss[0.42790669202804565] | Lr[1e-05]
Step[1000] | Loss[0.504793107509613] | Lr[1e-05]Step[1000] | Loss[0.6242160201072693] | Lr[1e-05]

Step[1500] | Loss[1.0990681648254395] | Lr[1e-05]Step[1500] | Loss[0.5373383164405823] | Lr[1e-05]

Step[1500] | Loss[1.1704367399215698] | Lr[1e-05]
Step[1500] | Loss[0.6780098080635071] | Lr[1e-05]
Step[2000] | Loss[0.7035991549491882] | Lr[1e-05]Step[2000] | Loss[0.6281797885894775] | Lr[1e-05]

Step[2000] | Loss[0.9629892706871033] | Lr[1e-05]Step[2000] | Loss[0.6258407831192017] | Lr[1e-05]

Step[2500] | Loss[1.04021155834198] | Lr[1e-05]
Step[2500] | Loss[1.2406370639801025] | Lr[1e-05]
Step[2500] | Loss[0.7107875347137451] | Lr[1e-05]
Step[2500] | Loss[0.6865240335464478] | Lr[1e-05]
Step[3000] | Loss[0.4447827935218811] | Lr[1e-05]Step[3000] | Loss[0.724614143371582] | Lr[1e-05]

Step[3000] | Loss[0.6895843148231506] | Lr[1e-05]Step[3000] | Loss[0.7297785878181458] | Lr[1e-05]

Step[3500] | Loss[0.48200079798698425] | Lr[1e-05]
Step[3500] | Loss[0.4293799102306366] | Lr[1e-05]
Step[3500] | Loss[0.721459686756134] | Lr[1e-05]Step[3500] | Loss[0.3662327826023102] | Lr[1e-05]

Step[4000] | Loss[0.371309369802475] | Lr[1e-05]
Step[4000] | Loss[0.7646092772483826] | Lr[1e-05]
Step[4000] | Loss[0.5636659860610962] | Lr[1e-05]
Step[4000] | Loss[0.7173333764076233] | Lr[1e-05]
Step[4500] | Loss[0.6805301308631897] | Lr[1e-05]
Step[4500] | Loss[1.0352920293807983] | Lr[1e-05]
Step[4500] | Loss[0.637721598148346] | Lr[1e-05]
Step[4500] | Loss[0.5649447441101074] | Lr[1e-05]
Step[5000] | Loss[1.0622382164001465] | Lr[1e-05]
Step[5000] | Loss[0.7467045187950134] | Lr[1e-05]
Step[5000] | Loss[0.6440024375915527] | Lr[1e-05]
Step[5000] | Loss[0.897978663444519] | Lr[1e-05]
Step[5500] | Loss[0.6349905729293823] | Lr[1e-05]Step[5500] | Loss[0.6927272081375122] | Lr[1e-05]

Step[5500] | Loss[0.7644302248954773] | Lr[1e-05]Step[5500] | Loss[1.0150370597839355] | Lr[1e-05]

Step[6000] | Loss[0.5987485647201538] | Lr[1e-05]Step[6000] | Loss[0.4033575654029846] | Lr[1e-05]

Step[6000] | Loss[0.7950000762939453] | Lr[1e-05]
Step[6000] | Loss[0.5770153403282166] | Lr[1e-05]
Step[6500] | Loss[0.7858580350875854] | Lr[1e-05]
Step[6500] | Loss[0.8730322122573853] | Lr[1e-05]
Step[6500] | Loss[0.5237575173377991] | Lr[1e-05]
Step[6500] | Loss[1.0907005071640015] | Lr[1e-05]
Step[7000] | Loss[0.9577046632766724] | Lr[1e-05]
Step[7000] | Loss[0.7788853645324707] | Lr[1e-05]
Step[7000] | Loss[0.4815284311771393] | Lr[1e-05]Step[7000] | Loss[1.4425359964370728] | Lr[1e-05]

Step[7500] | Loss[0.8341615200042725] | Lr[1e-05]
Step[7500] | Loss[0.4570951461791992] | Lr[1e-05]
Step[7500] | Loss[0.872514009475708] | Lr[1e-05]
Step[7500] | Loss[1.1248186826705933] | Lr[1e-05]
Step[8000] | Loss[0.693885087966919] | Lr[1e-05]
Step[8000] | Loss[0.661200225353241] | Lr[1e-05]
Step[8000] | Loss[0.6310194134712219] | Lr[1e-05]Step[8000] | Loss[0.5809717178344727] | Lr[1e-05]

Step[8500] | Loss[0.6106048822402954] | Lr[1e-05]Step[8500] | Loss[0.7641887664794922] | Lr[1e-05]

Step[8500] | Loss[0.3564555048942566] | Lr[1e-05]
Step[8500] | Loss[0.696678638458252] | Lr[1e-05]
Step[9000] | Loss[0.630363404750824] | Lr[1e-05]Step[9000] | Loss[0.5322920680046082] | Lr[1e-05]

Step[9000] | Loss[0.927401065826416] | Lr[1e-05]
Step[9000] | Loss[1.0181565284729004] | Lr[1e-05]
Step[9500] | Loss[0.7230244874954224] | Lr[1e-05]
Step[9500] | Loss[0.44549164175987244] | Lr[1e-05]Step[9500] | Loss[0.37503403425216675] | Lr[1e-05]

Step[9500] | Loss[0.4415690302848816] | Lr[1e-05]
Step[10000] | Loss[0.5235189199447632] | Lr[1e-05]
Step[10000] | Loss[0.6372909545898438] | Lr[1e-05]
Step[10000] | Loss[0.3939177095890045] | Lr[1e-05]
Step[10000] | Loss[0.4865504205226898] | Lr[1e-05]
Step[10500] | Loss[0.8083361983299255] | Lr[1e-05]Step[10500] | Loss[0.5705123543739319] | Lr[1e-05]

Step[10500] | Loss[1.0704271793365479] | Lr[1e-05]Step[10500] | Loss[0.46935927867889404] | Lr[1e-05]

Step[11000] | Loss[0.5992283225059509] | Lr[1e-05]Step[11000] | Loss[0.602363646030426] | Lr[1e-05]

Step[11000] | Loss[0.6332243084907532] | Lr[1e-05]Step[11000] | Loss[0.8665520548820496] | Lr[1e-05]

Step[11500] | Loss[0.7277123928070068] | Lr[1e-05]
Step[11500] | Loss[0.5879294276237488] | Lr[1e-05]
Step[11500] | Loss[0.7502713799476624] | Lr[1e-05]
Step[11500] | Loss[0.9162794351577759] | Lr[1e-05]
Step[12000] | Loss[0.6950786113739014] | Lr[1e-05]Step[12000] | Loss[0.6606147289276123] | Lr[1e-05]

Step[12000] | Loss[0.810876190662384] | Lr[1e-05]
Step[12000] | Loss[0.5746261477470398] | Lr[1e-05]
Step[12500] | Loss[0.7908691763877869] | Lr[1e-05]Step[12500] | Loss[0.7598252296447754] | Lr[1e-05]

Step[12500] | Loss[0.5845819115638733] | Lr[1e-05]Step[12500] | Loss[0.7060706615447998] | Lr[1e-05]

Step[13000] | Loss[1.4006844758987427] | Lr[1e-05]Step[13000] | Loss[0.9239965081214905] | Lr[1e-05]

Step[13000] | Loss[0.5946579575538635] | Lr[1e-05]Step[13000] | Loss[0.6751799583435059] | Lr[1e-05]

Step[13500] | Loss[0.5909678339958191] | Lr[1e-05]Step[13500] | Loss[0.4409528076648712] | Lr[1e-05]

Step[13500] | Loss[1.0189530849456787] | Lr[1e-05]Step[13500] | Loss[0.4892357587814331] | Lr[1e-05]

Step[14000] | Loss[0.7171370387077332] | Lr[1e-05]
Step[14000] | Loss[0.595423698425293] | Lr[1e-05]
Step[14000] | Loss[0.5832299590110779] | Lr[1e-05]
Step[14000] | Loss[0.9590393304824829] | Lr[1e-05]
Step[14500] | Loss[0.8221583962440491] | Lr[1e-05]
Step[14500] | Loss[0.5418825745582581] | Lr[1e-05]
Step[14500] | Loss[0.5569630265235901] | Lr[1e-05]
Step[14500] | Loss[0.869927704334259] | Lr[1e-05]
Step[15000] | Loss[0.8343995809555054] | Lr[1e-05]
Step[15000] | Loss[0.9405246376991272] | Lr[1e-05]
Step[15000] | Loss[1.031112551689148] | Lr[1e-05]
Step[15000] | Loss[0.4743921160697937] | Lr[1e-05]
Step[15500] | Loss[1.050803780555725] | Lr[1e-05]
Step[15500] | Loss[0.9997481107711792] | Lr[1e-05]
Step[15500] | Loss[1.1268184185028076] | Lr[1e-05]
Step[15500] | Loss[0.9947772026062012] | Lr[1e-05]
Step[16000] | Loss[0.8671037554740906] | Lr[1e-05]Step[16000] | Loss[0.5857128500938416] | Lr[1e-05]

Step[16000] | Loss[0.4800989031791687] | Lr[1e-05]
Step[16000] | Loss[0.3399140238761902] | Lr[1e-05]
Step[16500] | Loss[0.90034419298172] | Lr[1e-05]Step[16500] | Loss[0.7238631248474121] | Lr[1e-05]

Step[16500] | Loss[0.7434631586074829] | Lr[1e-05]
Step[16500] | Loss[0.5691378116607666] | Lr[1e-05]
Step[17000] | Loss[1.0544686317443848] | Lr[1e-05]Step[17000] | Loss[0.9535397291183472] | Lr[1e-05]

Step[17000] | Loss[0.6545217633247375] | Lr[1e-05]Step[17000] | Loss[0.5507025718688965] | Lr[1e-05]

Step[17500] | Loss[0.7551472187042236] | Lr[1e-05]Step[17500] | Loss[0.8225297927856445] | Lr[1e-05]

Step[17500] | Loss[1.1100393533706665] | Lr[1e-05]
Step[17500] | Loss[1.1153781414031982] | Lr[1e-05]
Step[18000] | Loss[0.6444112062454224] | Lr[1e-05]
Step[18000] | Loss[0.8968471884727478] | Lr[1e-05]
Step[18000] | Loss[0.747861921787262] | Lr[1e-05]
Step[18000] | Loss[0.7803361415863037] | Lr[1e-05]
Step[18500] | Loss[0.670307993888855] | Lr[1e-05]
Step[18500] | Loss[0.6634096503257751] | Lr[1e-05]
Step[18500] | Loss[0.5723691582679749] | Lr[1e-05]
Step[18500] | Loss[0.6947451233863831] | Lr[1e-05]
Step[19000] | Loss[0.49833643436431885] | Lr[1e-05]
Step[19000] | Loss[0.6588969230651855] | Lr[1e-05]
Step[19000] | Loss[0.848206639289856] | Lr[1e-05]
Step[19000] | Loss[0.6626289486885071] | Lr[1e-05]
Step[19500] | Loss[0.48482534289360046] | Lr[1e-05]Step[19500] | Loss[0.34127768874168396] | Lr[1e-05]

Step[19500] | Loss[0.5407525897026062] | Lr[1e-05]
Step[19500] | Loss[0.5123019814491272] | Lr[1e-05]
Step[20000] | Loss[0.5698394775390625] | Lr[1e-05]Step[20000] | Loss[0.6593860387802124] | Lr[1e-05]

Step[20000] | Loss[0.5782435536384583] | Lr[1e-05]
Step[20000] | Loss[0.8007206320762634] | Lr[1e-05]
Step[20500] | Loss[0.9134302735328674] | Lr[1e-05]
Step[20500] | Loss[1.0730226039886475] | Lr[1e-05]
Step[20500] | Loss[1.1470067501068115] | Lr[1e-05]
Step[20500] | Loss[0.48816943168640137] | Lr[1e-05]
Step[21000] | Loss[0.37540990114212036] | Lr[1e-05]Step[21000] | Loss[0.42520296573638916] | Lr[1e-05]

Step[21000] | Loss[0.4644615948200226] | Lr[1e-05]Step[21000] | Loss[0.8738580942153931] | Lr[1e-05]

Step[21500] | Loss[1.2250304222106934] | Lr[1e-05]Step[21500] | Loss[0.8525562286376953] | Lr[1e-05]

Step[21500] | Loss[0.6736294031143188] | Lr[1e-05]
Step[21500] | Loss[0.7318743467330933] | Lr[1e-05]
Step[22000] | Loss[0.658938467502594] | Lr[1e-05]
Step[22000] | Loss[0.7646799087524414] | Lr[1e-05]
Step[22000] | Loss[0.43010008335113525] | Lr[1e-05]Step[22000] | Loss[0.7510097622871399] | Lr[1e-05]

Step[22500] | Loss[1.234554409980774] | Lr[1e-05]Step[22500] | Loss[0.6951547861099243] | Lr[1e-05]

Step[22500] | Loss[0.6644752621650696] | Lr[1e-05]
Step[22500] | Loss[0.7552316188812256] | Lr[1e-05]
Step[23000] | Loss[0.8065447807312012] | Lr[1e-05]
Step[23000] | Loss[0.4832648038864136] | Lr[1e-05]
Step[23000] | Loss[0.6368448734283447] | Lr[1e-05]
Step[23000] | Loss[0.37287238240242004] | Lr[1e-05]
Step[23500] | Loss[0.9528307318687439] | Lr[1e-05]Step[23500] | Loss[0.57731032371521] | Lr[1e-05]

Step[23500] | Loss[0.6731310486793518] | Lr[1e-05]Step[23500] | Loss[0.6550024747848511] | Lr[1e-05]

Step[24000] | Loss[0.8095288276672363] | Lr[1e-05]
Step[24000] | Loss[0.6961579322814941] | Lr[1e-05]
Step[24000] | Loss[0.8479991555213928] | Lr[1e-05]
Step[24000] | Loss[0.5212029218673706] | Lr[1e-05]
Step[24500] | Loss[0.4354226291179657] | Lr[1e-05]
Step[24500] | Loss[0.4774591624736786] | Lr[1e-05]
Step[24500] | Loss[0.8090918064117432] | Lr[1e-05]
Step[24500] | Loss[0.697930097579956] | Lr[1e-05]
Step[25000] | Loss[0.6710168123245239] | Lr[1e-05]Step[25000] | Loss[0.5579535365104675] | Lr[1e-05]

Step[25000] | Loss[0.47563302516937256] | Lr[1e-05]
Step[25000] | Loss[0.6888445019721985] | Lr[1e-05]
Step[25500] | Loss[0.6615961194038391] | Lr[1e-05]Step[25500] | Loss[0.8391839861869812] | Lr[1e-05]

Step[25500] | Loss[0.7904712557792664] | Lr[1e-05]
Step[25500] | Loss[0.6811550855636597] | Lr[1e-05]
Step[26000] | Loss[0.701156735420227] | Lr[1e-05]
Step[26000] | Loss[0.46978771686553955] | Lr[1e-05]
Step[26000] | Loss[0.571843147277832] | Lr[1e-05]
Step[26000] | Loss[0.6685839891433716] | Lr[1e-05]
Step[26500] | Loss[0.5187943577766418] | Lr[1e-05]Step[26500] | Loss[0.4246060252189636] | Lr[1e-05]

Step[26500] | Loss[0.2822445333003998] | Lr[1e-05]Step[26500] | Loss[0.6349906921386719] | Lr[1e-05]

Step[27000] | Loss[1.0069282054901123] | Lr[1e-05]Step[27000] | Loss[0.45717325806617737] | Lr[1e-05]

Step[27000] | Loss[0.7305023074150085] | Lr[1e-05]
Step[27000] | Loss[0.8835919499397278] | Lr[1e-05]
Step[27500] | Loss[0.8801302313804626] | Lr[1e-05]
Step[27500] | Loss[0.693627119064331] | Lr[1e-05]
Step[27500] | Loss[0.5922809839248657] | Lr[1e-05]
Step[27500] | Loss[0.5744917392730713] | Lr[1e-05]
Step[28000] | Loss[0.7809295058250427] | Lr[1e-05]
Step[28000] | Loss[0.6440619826316833] | Lr[1e-05]
Step[28000] | Loss[0.5200859904289246] | Lr[1e-05]
Step[28000] | Loss[0.8113689422607422] | Lr[1e-05]
Step[28500] | Loss[0.6120249032974243] | Lr[1e-05]
Step[28500] | Loss[0.6744264364242554] | Lr[1e-05]
Step[28500] | Loss[0.8239226341247559] | Lr[1e-05]
Step[28500] | Loss[0.4376522898674011] | Lr[1e-05]
Step[29000] | Loss[0.7047381401062012] | Lr[1e-05]Step[29000] | Loss[0.534546971321106] | Lr[1e-05]

Step[29000] | Loss[0.5499864816665649] | Lr[1e-05]Step[29000] | Loss[0.4805057942867279] | Lr[1e-05]

Step[29500] | Loss[0.6152471899986267] | Lr[1e-05]Step[29500] | Loss[1.0027570724487305] | Lr[1e-05]

Step[29500] | Loss[0.7400678396224976] | Lr[1e-05]
Step[29500] | Loss[0.4818522036075592] | Lr[1e-05]
Step[30000] | Loss[0.6187058687210083] | Lr[1e-05]Step[30000] | Loss[0.6809288859367371] | Lr[1e-05]

Step[30000] | Loss[1.018373727798462] | Lr[1e-05]Step[30000] | Loss[0.5991507172584534] | Lr[1e-05]

Step[30500] | Loss[0.896196186542511] | Lr[1e-05]
Step[30500] | Loss[0.7242605686187744] | Lr[1e-05]
Step[30500] | Loss[0.7602294087409973] | Lr[1e-05]Step[30500] | Loss[0.7896475791931152] | Lr[1e-05]

Step[31000] | Loss[0.557117760181427] | Lr[1e-05]
Step[31000] | Loss[0.9595209360122681] | Lr[1e-05]
Step[31000] | Loss[0.5310714840888977] | Lr[1e-05]Step[31000] | Loss[0.7907421588897705] | Lr[1e-05]

Step[31500] | Loss[0.6928241848945618] | Lr[1e-05]
Step[31500] | Loss[0.9504057168960571] | Lr[1e-05]
Step[31500] | Loss[1.0437740087509155] | Lr[1e-05]
Step[31500] | Loss[0.8329303860664368] | Lr[1e-05]
Step[32000] | Loss[0.6539020538330078] | Lr[1e-05]
Step[32000] | Loss[0.7614065408706665] | Lr[1e-05]
Step[32000] | Loss[0.8131914734840393] | Lr[1e-05]
Step[32000] | Loss[0.6632012128829956] | Lr[1e-05]
Step[32500] | Loss[0.5927574038505554] | Lr[1e-05]
Step[32500] | Loss[0.5109724402427673] | Lr[1e-05]
Step[32500] | Loss[0.7566308975219727] | Lr[1e-05]
Step[32500] | Loss[0.6096723079681396] | Lr[1e-05]
Step[33000] | Loss[0.6163243055343628] | Lr[1e-05]
Step[33000] | Loss[0.46847105026245117] | Lr[1e-05]
Step[33000] | Loss[0.7144498825073242] | Lr[1e-05]Step[33000] | Loss[0.7208764553070068] | Lr[1e-05]

Step[33500] | Loss[0.7324448227882385] | Lr[1e-05]
Step[33500] | Loss[0.4572959840297699] | Lr[1e-05]
Step[33500] | Loss[1.5334481000900269] | Lr[1e-05]
Step[33500] | Loss[0.627345085144043] | Lr[1e-05]
Step[34000] | Loss[1.0099916458129883] | Lr[1e-05]
Step[34000] | Loss[0.7092172503471375] | Lr[1e-05]
Step[34000] | Loss[0.6303727626800537] | Lr[1e-05]
Step[34000] | Loss[0.5066297650337219] | Lr[1e-05]
Step[34500] | Loss[0.8704387545585632] | Lr[1e-05]Step[34500] | Loss[0.8942208290100098] | Lr[1e-05]

Step[34500] | Loss[0.5485272407531738] | Lr[1e-05]
Step[34500] | Loss[0.49403104186058044] | Lr[1e-05]
Step[35000] | Loss[0.7867777943611145] | Lr[1e-05]
Step[35000] | Loss[0.8705242872238159] | Lr[1e-05]
Step[35000] | Loss[0.7650131583213806] | Lr[1e-05]
Step[35000] | Loss[0.6655166745185852] | Lr[1e-05]
Step[35500] | Loss[0.384811669588089] | Lr[1e-05]
Step[35500] | Loss[0.8118371963500977] | Lr[1e-05]
Step[35500] | Loss[0.6299295425415039] | Lr[1e-05]
Step[35500] | Loss[0.9915252923965454] | Lr[1e-05]
Step[36000] | Loss[0.6941591501235962] | Lr[1e-05]Step[36000] | Loss[0.8687986135482788] | Lr[1e-05]

Step[36000] | Loss[0.6739201545715332] | Lr[1e-05]
Step[36000] | Loss[0.4797093868255615] | Lr[1e-05]
Step[36500] | Loss[0.5005543828010559] | Lr[1e-05]
Step[36500] | Loss[0.6072373986244202] | Lr[1e-05]
Step[36500] | Loss[0.7701371312141418] | Lr[1e-05]
Step[36500] | Loss[0.5843327641487122] | Lr[1e-05]
Step[37000] | Loss[0.6260100603103638] | Lr[1e-05]Step[37000] | Loss[0.5278618335723877] | Lr[1e-05]

Step[37000] | Loss[0.36430445313453674] | Lr[1e-05]
Step[37000] | Loss[1.017739176750183] | Lr[1e-05]
Step[37500] | Loss[0.7356901168823242] | Lr[1e-05]Step[37500] | Loss[1.028411865234375] | Lr[1e-05]

Step[37500] | Loss[0.9730492234230042] | Lr[1e-05]
Step[37500] | Loss[0.7250798940658569] | Lr[1e-05]
Step[38000] | Loss[0.506771981716156] | Lr[1e-05]
Step[38000] | Loss[0.8388842344284058] | Lr[1e-05]
Step[38000] | Loss[0.5032926797866821] | Lr[1e-05]
Step[38000] | Loss[0.7863284349441528] | Lr[1e-05]
Step[38500] | Loss[0.8426229357719421] | Lr[1e-05]
Step[38500] | Loss[0.6987186074256897] | Lr[1e-05]
Step[38500] | Loss[0.6996303796768188] | Lr[1e-05]Step[38500] | Loss[0.7999382019042969] | Lr[1e-05]

Step[39000] | Loss[0.7546553015708923] | Lr[1e-05]
Step[39000] | Loss[0.7557449340820312] | Lr[1e-05]
Step[39000] | Loss[0.4267127215862274] | Lr[1e-05]
Step[39000] | Loss[0.5144220590591431] | Lr[1e-05]
Step[39500] | Loss[0.7173484563827515] | Lr[1e-05]
Step[39500] | Loss[0.81959068775177] | Lr[1e-05]
Step[39500] | Loss[0.6292979121208191] | Lr[1e-05]
Step[39500] | Loss[0.7852113842964172] | Lr[1e-05]
Step[40000] | Loss[0.8852776885032654] | Lr[1e-05]
Step[40000] | Loss[0.8196895718574524] | Lr[1e-05]
Step[40000] | Loss[0.6904539465904236] | Lr[1e-05]Step[40000] | Loss[0.571603000164032] | Lr[1e-05]

Step[40500] | Loss[0.784801721572876] | Lr[1e-05]
Step[40500] | Loss[0.4725303053855896] | Lr[1e-05]
Step[40500] | Loss[0.8169832825660706] | Lr[1e-05]
Step[40500] | Loss[0.7017743587493896] | Lr[1e-05]
Step[41000] | Loss[0.9322750568389893] | Lr[1e-05]
Step[41000] | Loss[0.7457888126373291] | Lr[1e-05]
Step[41000] | Loss[0.661335289478302] | Lr[1e-05]Step[41000] | Loss[0.9081624150276184] | Lr[1e-05]

Step[41500] | Loss[0.5297461748123169] | Lr[1e-05]
Step[41500] | Loss[0.5687916874885559] | Lr[1e-05]
Step[41500] | Loss[0.5024642944335938] | Lr[1e-05]
Step[41500] | Loss[0.4420141279697418] | Lr[1e-05]
Step[42000] | Loss[0.7247298955917358] | Lr[1e-05]
Step[42000] | Loss[0.7456570267677307] | Lr[1e-05]
Step[42000] | Loss[0.6858289837837219] | Lr[1e-05]
Step[42000] | Loss[0.722295880317688] | Lr[1e-05]
Step[42500] | Loss[0.9714395999908447] | Lr[1e-05]Step[42500] | Loss[0.9287077188491821] | Lr[1e-05]

Step[42500] | Loss[0.6753920912742615] | Lr[1e-05]
Step[42500] | Loss[0.9517147541046143] | Lr[1e-05]
Step[43000] | Loss[0.5126736164093018] | Lr[1e-05]
Step[43000] | Loss[0.756000280380249] | Lr[1e-05]
Step[43000] | Loss[0.8555606603622437] | Lr[1e-05]
Step[43000] | Loss[1.014480710029602] | Lr[1e-05]
Step[43500] | Loss[0.5562908053398132] | Lr[1e-05]Step[43500] | Loss[0.5258238315582275] | Lr[1e-05]

Step[43500] | Loss[1.0070459842681885] | Lr[1e-05]
Step[43500] | Loss[0.9133444428443909] | Lr[1e-05]
Step[44000] | Loss[0.9207019209861755] | Lr[1e-05]Step[44000] | Loss[0.5075920224189758] | Lr[1e-05]

Step[44000] | Loss[0.6533699035644531] | Lr[1e-05]
Step[44000] | Loss[0.5378470420837402] | Lr[1e-05]
Step[44500] | Loss[0.7326152324676514] | Lr[1e-05]
Step[44500] | Loss[0.590272068977356] | Lr[1e-05]
Step[44500] | Loss[0.9051810503005981] | Lr[1e-05]Step[44500] | Loss[0.7567588090896606] | Lr[1e-05]

Step[45000] | Loss[0.5260978937149048] | Lr[1e-05]Step[45000] | Loss[0.7290294766426086] | Lr[1e-05]

Step[45000] | Loss[1.0586168766021729] | Lr[1e-05]
Step[45000] | Loss[0.5808588862419128] | Lr[1e-05]
Step[45500] | Loss[0.6746482849121094] | Lr[1e-05]Step[45500] | Loss[0.44024068117141724] | Lr[1e-05]

Step[45500] | Loss[0.53077632188797] | Lr[1e-05]Step[45500] | Loss[0.5186454057693481] | Lr[1e-05]

Step[46000] | Loss[0.5844277739524841] | Lr[1e-05]
Step[46000] | Loss[0.5027367472648621] | Lr[1e-05]
Step[46000] | Loss[0.6588579416275024] | Lr[1e-05]
Step[46000] | Loss[0.656286358833313] | Lr[1e-05]
Step[46500] | Loss[0.5884614586830139] | Lr[1e-05]Step[46500] | Loss[0.9712805151939392] | Lr[1e-05]

Step[46500] | Loss[0.7404601573944092] | Lr[1e-05]
Step[46500] | Loss[0.6779505014419556] | Lr[1e-05]
Step[47000] | Loss[0.6049838662147522] | Lr[1e-05]
Step[47000] | Loss[0.7353467345237732] | Lr[1e-05]
Step[47000] | Loss[0.6576458811759949] | Lr[1e-05]
Step[47000] | Loss[0.48936617374420166] | Lr[1e-05]
Step[47500] | Loss[0.6663140654563904] | Lr[1e-05]Step[47500] | Loss[0.38020381331443787] | Lr[1e-05]

Step[47500] | Loss[0.8178696036338806] | Lr[1e-05]Step[47500] | Loss[0.5560001134872437] | Lr[1e-05]

Step[48000] | Loss[0.815427303314209] | Lr[1e-05]
Step[48000] | Loss[0.8442203402519226] | Lr[1e-05]
Step[48000] | Loss[0.5409146547317505] | Lr[1e-05]
Step[48000] | Loss[0.6921900510787964] | Lr[1e-05]
Step[48500] | Loss[0.6056755185127258] | Lr[1e-05]
Step[48500] | Loss[0.5128596425056458] | Lr[1e-05]
Step[48500] | Loss[0.5300717353820801] | Lr[1e-05]Step[48500] | Loss[0.9070242047309875] | Lr[1e-05]

Step[49000] | Loss[0.6866715550422668] | Lr[1e-05]Step[49000] | Loss[1.0884398221969604] | Lr[1e-05]

Step[49000] | Loss[0.36937323212623596] | Lr[1e-05]
Step[49000] | Loss[0.7386335730552673] | Lr[1e-05]
Step[49500] | Loss[0.7605378031730652] | Lr[1e-05]Step[49500] | Loss[0.526436448097229] | Lr[1e-05]

Step[49500] | Loss[0.5040655136108398] | Lr[1e-05]
Step[49500] | Loss[0.8700530529022217] | Lr[1e-05]
Step[50000] | Loss[0.7965986728668213] | Lr[1e-05]Step[50000] | Loss[0.8066279888153076] | Lr[1e-05]

Step[50000] | Loss[0.622300922870636] | Lr[1e-05]Step[50000] | Loss[0.5798038840293884] | Lr[1e-05]

Step[50500] | Loss[0.5209754705429077] | Lr[1e-05]
Step[50500] | Loss[0.5894476175308228] | Lr[1e-05]
Step[50500] | Loss[0.62360680103302] | Lr[1e-05]Step[50500] | Loss[0.8913122415542603] | Lr[1e-05]

Step[51000] | Loss[0.8031526207923889] | Lr[1e-05]Step[51000] | Loss[0.5791775584220886] | Lr[1e-05]

Step[51000] | Loss[0.6490693688392639] | Lr[1e-05]
Step[51000] | Loss[0.5822321772575378] | Lr[1e-05]
Step[51500] | Loss[0.6678926348686218] | Lr[1e-05]
Step[51500] | Loss[0.5740039944648743] | Lr[1e-05]
Step[51500] | Loss[0.6382215023040771] | Lr[1e-05]Step[51500] | Loss[0.5498892068862915] | Lr[1e-05]

Step[52000] | Loss[0.4449121057987213] | Lr[1e-05]
Step[52000] | Loss[0.5471187233924866] | Lr[1e-05]
Step[52000] | Loss[0.739629864692688] | Lr[1e-05]Step[52000] | Loss[0.5317258238792419] | Lr[1e-05]

Step[52500] | Loss[0.8233276009559631] | Lr[1e-05]
Step[52500] | Loss[0.5608341097831726] | Lr[1e-05]
Step[52500] | Loss[0.5646877288818359] | Lr[1e-05]
Step[52500] | Loss[0.5949872732162476] | Lr[1e-05]
Step[53000] | Loss[0.8997170329093933] | Lr[1e-05]Step[53000] | Loss[0.6027588248252869] | Lr[1e-05]

Step[53000] | Loss[0.9923823475837708] | Lr[1e-05]
Step[53000] | Loss[0.40438926219940186] | Lr[1e-05]
Step[53500] | Loss[0.7344620823860168] | Lr[1e-05]
Step[53500] | Loss[0.9756582379341125] | Lr[1e-05]
Step[53500] | Loss[0.8803766965866089] | Lr[1e-05]Step[53500] | Loss[0.5675216317176819] | Lr[1e-05]

Step[54000] | Loss[0.8720537424087524] | Lr[1e-05]
Step[54000] | Loss[0.36112484335899353] | Lr[1e-05]
Step[54000] | Loss[0.4144725799560547] | Lr[1e-05]Step[54000] | Loss[0.43227896094322205] | Lr[1e-05]

Step[54500] | Loss[0.41667991876602173] | Lr[1e-05]
Step[54500] | Loss[0.6745563745498657] | Lr[1e-05]
Step[54500] | Loss[0.3379465937614441] | Lr[1e-05]Step[54500] | Loss[0.5417128801345825] | Lr[1e-05]

Step[55000] | Loss[0.7533288598060608] | Lr[1e-05]
Step[55000] | Loss[0.5349246263504028] | Lr[1e-05]
Step[55000] | Loss[0.7278980612754822] | Lr[1e-05]
Step[55000] | Loss[0.5998727083206177] | Lr[1e-05]
Step[55500] | Loss[0.5220605731010437] | Lr[1e-05]
Step[55500] | Loss[0.5341652631759644] | Lr[1e-05]
Step[55500] | Loss[0.4730204641819] | Lr[1e-05]
Step[55500] | Loss[1.2918658256530762] | Lr[1e-05]
Step[56000] | Loss[1.41511869430542] | Lr[1e-05]Step[56000] | Loss[0.7110575437545776] | Lr[1e-05]

Step[56000] | Loss[0.49837416410446167] | Lr[1e-05]
Step[56000] | Loss[0.3443300426006317] | Lr[1e-05]
Step[56500] | Loss[0.7269675135612488] | Lr[1e-05]
Step[56500] | Loss[1.0418566465377808] | Lr[1e-05]
Step[56500] | Loss[0.7504321932792664] | Lr[1e-05]Step[56500] | Loss[0.6644161343574524] | Lr[1e-05]

Step[57000] | Loss[0.5200515985488892] | Lr[1e-05]
Step[57000] | Loss[0.675193727016449] | Lr[1e-05]
Step[57000] | Loss[0.5234434008598328] | Lr[1e-05]
Step[57000] | Loss[0.6688861846923828] | Lr[1e-05]
Step[57500] | Loss[0.8201172351837158] | Lr[1e-05]Step[57500] | Loss[0.8758728504180908] | Lr[1e-05]

Step[57500] | Loss[0.40989673137664795] | Lr[1e-05]
Step[57500] | Loss[0.7139802575111389] | Lr[1e-05]
Step[58000] | Loss[0.9493650197982788] | Lr[1e-05]
Step[58000] | Loss[0.5730404853820801] | Lr[1e-05]
Step[58000] | Loss[0.8578722476959229] | Lr[1e-05]Step[58000] | Loss[0.8402775526046753] | Lr[1e-05]

Step[58500] | Loss[1.2295223474502563] | Lr[1e-05]Step[58500] | Loss[0.43488067388534546] | Lr[1e-05]

Step[58500] | Loss[0.6454418897628784] | Lr[1e-05]
Step[58500] | Loss[0.6280183792114258] | Lr[1e-05]
Step[59000] | Loss[0.5221986174583435] | Lr[1e-05]
Step[59000] | Loss[0.8032045364379883] | Lr[1e-05]
Step[59000] | Loss[0.9158569574356079] | Lr[1e-05]
Step[59000] | Loss[0.6339969635009766] | Lr[1e-05]
Step[59500] | Loss[0.8884929418563843] | Lr[1e-05]
Step[59500] | Loss[0.6396873593330383] | Lr[1e-05]
Step[59500] | Loss[0.4064941704273224] | Lr[1e-05]
Step[59500] | Loss[0.7242212295532227] | Lr[1e-05]
Step[60000] | Loss[1.1231627464294434] | Lr[1e-05]
Step[60000] | Loss[0.6432203054428101] | Lr[1e-05]
Step[60000] | Loss[0.37021008133888245] | Lr[1e-05]Step[60000] | Loss[0.3211938738822937] | Lr[1e-05]

Step[60500] | Loss[0.339507520198822] | Lr[1e-05]Step[60500] | Loss[0.5626490116119385] | Lr[1e-05]

Step[60500] | Loss[0.3894166350364685] | Lr[1e-05]Step[60500] | Loss[0.784024178981781] | Lr[1e-05]

Step[61000] | Loss[0.4844772517681122] | Lr[1e-05]Step[61000] | Loss[0.6554223895072937] | Lr[1e-05]

Step[61000] | Loss[0.5919498801231384] | Lr[1e-05]
Step[61000] | Loss[0.8640552163124084] | Lr[1e-05]
Step[61500] | Loss[0.7742965221405029] | Lr[1e-05]
Step[61500] | Loss[1.1037309169769287] | Lr[1e-05]
Step[61500] | Loss[0.6464638710021973] | Lr[1e-05]
Step[61500] | Loss[0.3895629644393921] | Lr[1e-05]
Step[62000] | Loss[0.9623996615409851] | Lr[1e-05]
Step[62000] | Loss[0.6500177383422852] | Lr[1e-05]
Step[62000] | Loss[0.7042129039764404] | Lr[1e-05]
Step[62000] | Loss[0.5847662091255188] | Lr[1e-05]
Step[62500] | Loss[0.7194423675537109] | Lr[1e-05]
Step[62500] | Loss[0.5466238260269165] | Lr[1e-05]
Step[62500] | Loss[0.8394932150840759] | Lr[1e-05]
Step[62500] | Loss[0.4093928933143616] | Lr[1e-05]
Step[63000] | Loss[0.5806747078895569] | Lr[1e-05]Step[63000] | Loss[0.6756219267845154] | Lr[1e-05]

Step[63000] | Loss[0.7882186770439148] | Lr[1e-05]Step[63000] | Loss[0.5494213104248047] | Lr[1e-05]

Step[63500] | Loss[0.9400473237037659] | Lr[1e-05]
Step[63500] | Loss[0.5042257308959961] | Lr[1e-05]
Step[63500] | Loss[0.9202110767364502] | Lr[1e-05]
Step[63500] | Loss[0.695785403251648] | Lr[1e-05]
Step[64000] | Loss[0.8846330642700195] | Lr[1e-05]Step[64000] | Loss[0.7252750396728516] | Lr[1e-05]

Step[64000] | Loss[0.6843615770339966] | Lr[1e-05]Step[64000] | Loss[0.4716883897781372] | Lr[1e-05]

Step[64500] | Loss[0.9139437675476074] | Lr[1e-05]Step[64500] | Loss[0.4559648931026459] | Lr[1e-05]

Step[64500] | Loss[0.6135561466217041] | Lr[1e-05]Step[64500] | Loss[0.4784138798713684] | Lr[1e-05]

Step[65000] | Loss[0.8269433379173279] | Lr[1e-05]
Step[65000] | Loss[0.5105538964271545] | Lr[1e-05]
Step[65000] | Loss[0.40595903992652893] | Lr[1e-05]
Step[65000] | Loss[0.7982240915298462] | Lr[1e-05]
Step[65500] | Loss[0.497396856546402] | Lr[1e-05]
Step[65500] | Loss[0.6819995045661926] | Lr[1e-05]
Step[65500] | Loss[0.4496852159500122] | Lr[1e-05]
Step[65500] | Loss[0.45118042826652527] | Lr[1e-05]
Step[66000] | Loss[0.5827933549880981] | Lr[1e-05]
Step[66000] | Loss[0.3890639841556549] | Lr[1e-05]
Step[66000] | Loss[0.6525120139122009] | Lr[1e-05]
Step[66000] | Loss[0.4523763954639435] | Lr[1e-05]
Step[66500] | Loss[0.7523535490036011] | Lr[1e-05]Step[66500] | Loss[0.8193848133087158] | Lr[1e-05]

Step[66500] | Loss[0.5984956622123718] | Lr[1e-05]Step[66500] | Loss[0.7762322425842285] | Lr[1e-05]

Step[67000] | Loss[1.1368921995162964] | Lr[1e-05]Step[67000] | Loss[0.7600092887878418] | Lr[1e-05]

Step[67000] | Loss[0.6351822018623352] | Lr[1e-05]
Step[67000] | Loss[0.5928252339363098] | Lr[1e-05]
Step[67500] | Loss[0.41950875520706177] | Lr[1e-05]
Step[67500] | Loss[0.7722752094268799] | Lr[1e-05]
Step[67500] | Loss[0.4956906735897064] | Lr[1e-05]Step[67500] | Loss[0.6209007501602173] | Lr[1e-05]

Step[68000] | Loss[0.6309235692024231] | Lr[1e-05]Step[68000] | Loss[0.6737183928489685] | Lr[1e-05]

Step[68000] | Loss[0.8248746395111084] | Lr[1e-05]Step[68000] | Loss[0.6992435455322266] | Lr[1e-05]

Step[68500] | Loss[1.2570383548736572] | Lr[1e-05]
Step[68500] | Loss[1.2533494234085083] | Lr[1e-05]
Step[68500] | Loss[0.8921188712120056] | Lr[1e-05]
Step[68500] | Loss[0.8142013549804688] | Lr[1e-05]
Step[69000] | Loss[1.1111936569213867] | Lr[1e-05]
Step[69000] | Loss[0.9518347978591919] | Lr[1e-05]
Step[69000] | Loss[0.6588545441627502] | Lr[1e-05]
Step[69000] | Loss[0.6150776147842407] | Lr[1e-05]
Step[69500] | Loss[0.7280134558677673] | Lr[1e-05]
Step[69500] | Loss[0.657143771648407] | Lr[1e-05]
Step[69500] | Loss[0.8996042013168335] | Lr[1e-05]Step[69500] | Loss[0.9577426314353943] | Lr[1e-05]

Step[70000] | Loss[0.6627761125564575] | Lr[1e-05]
Step[70000] | Loss[0.7397035956382751] | Lr[1e-05]
Step[70000] | Loss[0.6825071573257446] | Lr[1e-05]Step[70000] | Loss[0.7165711522102356] | Lr[1e-05]

Step[70500] | Loss[0.641189455986023] | Lr[1e-05]Step[70500] | Loss[0.6153441071510315] | Lr[1e-05]

Step[70500] | Loss[0.5004145503044128] | Lr[1e-05]Step[70500] | Loss[0.7060290575027466] | Lr[1e-05]

Step[71000] | Loss[0.5692574381828308] | Lr[1e-05]Step[71000] | Loss[0.4003336727619171] | Lr[1e-05]

Step[71000] | Loss[0.7993777394294739] | Lr[1e-05]Step[71000] | Loss[0.5314011573791504] | Lr[1e-05]

Step[71500] | Loss[0.5647404193878174] | Lr[1e-05]
Step[71500] | Loss[0.688636064529419] | Lr[1e-05]
Step[71500] | Loss[0.7617135643959045] | Lr[1e-05]
Step[71500] | Loss[0.7270123362541199] | Lr[1e-05]
Step[72000] | Loss[0.5125985741615295] | Lr[1e-05]
Step[72000] | Loss[0.5329947471618652] | Lr[1e-05]
Step[72000] | Loss[0.7922736406326294] | Lr[1e-05]
Step[72000] | Loss[0.7732257843017578] | Lr[1e-05]
Step[72500] | Loss[0.4652027487754822] | Lr[1e-05]Step[72500] | Loss[0.7676725387573242] | Lr[1e-05]

Step[72500] | Loss[0.8406182527542114] | Lr[1e-05]
Step[72500] | Loss[0.43066632747650146] | Lr[1e-05]
Step[73000] | Loss[0.44282495975494385] | Lr[1e-05]Step[73000] | Loss[0.5602965950965881] | Lr[1e-05]

Step[73000] | Loss[0.5203357338905334] | Lr[1e-05]Step[73000] | Loss[0.5507405400276184] | Lr[1e-05]

Step[73500] | Loss[0.5492568016052246] | Lr[1e-05]
Step[73500] | Loss[0.43574801087379456] | Lr[1e-05]
Step[73500] | Loss[0.6740002632141113] | Lr[1e-05]
Step[73500] | Loss[0.40101951360702515] | Lr[1e-05]
Step[74000] | Loss[0.5412624478340149] | Lr[1e-05]Step[74000] | Loss[0.628255307674408] | Lr[1e-05]

Step[74000] | Loss[0.3747214078903198] | Lr[1e-05]Step[74000] | Loss[0.7943463921546936] | Lr[1e-05]

Step[74500] | Loss[0.6791831254959106] | Lr[1e-05]
Step[74500] | Loss[0.7533829808235168] | Lr[1e-05]
Step[74500] | Loss[0.4365740418434143] | Lr[1e-05]
Step[74500] | Loss[0.5456591248512268] | Lr[1e-05]
Step[75000] | Loss[0.8066258430480957] | Lr[1e-05]
Step[75000] | Loss[0.627800703048706] | Lr[1e-05]
Step[75000] | Loss[0.5587083101272583] | Lr[1e-05]
Step[75000] | Loss[0.5349265336990356] | Lr[1e-05]
Step[75500] | Loss[0.7593070864677429] | Lr[1e-05]Step[75500] | Loss[0.836509644985199] | Lr[1e-05]

Step[75500] | Loss[0.8206354379653931] | Lr[1e-05]
Step[75500] | Loss[0.6805208325386047] | Lr[1e-05]
Step[76000] | Loss[0.7875933647155762] | Lr[1e-05]
Step[76000] | Loss[0.7536222338676453] | Lr[1e-05]
Step[76000] | Loss[0.8831765651702881] | Lr[1e-05]
Step[76000] | Loss[0.6724348068237305] | Lr[1e-05]
Step[76500] | Loss[0.9107198715209961] | Lr[1e-05]
Step[76500] | Loss[0.5601147413253784] | Lr[1e-05]
Step[76500] | Loss[0.6738206148147583] | Lr[1e-05]
Step[76500] | Loss[0.7521736025810242] | Lr[1e-05]
Step[77000] | Loss[0.4262080788612366] | Lr[1e-05]Step[77000] | Loss[1.007058024406433] | Lr[1e-05]

Step[77000] | Loss[0.6941447854042053] | Lr[1e-05]
Step[77000] | Loss[0.657065212726593] | Lr[1e-05]
Step[77500] | Loss[0.5608736872673035] | Lr[1e-05]Step[77500] | Loss[0.3489535450935364] | Lr[1e-05]

Step[77500] | Loss[0.39304980635643005] | Lr[1e-05]
Step[77500] | Loss[0.7232331037521362] | Lr[1e-05]
Step[78000] | Loss[0.990843653678894] | Lr[1e-05]
Step[78000] | Loss[0.513627290725708] | Lr[1e-05]
Step[78000] | Loss[0.6065964698791504] | Lr[1e-05]Step[78000] | Loss[0.9488288760185242] | Lr[1e-05]

Labels:  tensor([2, 2, 1, 1, 1, 4, 4, 4, 3, 2, 0, 3, 3, 4, 0, 3], device='cuda:0')
Preds:  tensor([3, 3, 2, 1, 0, 4, 4, 4, 2, 2, 1, 2, 4, 4, 0, 3], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0004,     0.0356,     0.5580,     0.4060],
        [    0.0001,     0.0005,     0.0209,     0.6644,     0.3140],
        [    0.0089,     0.0703,     0.5461,     0.3230,     0.0517],
        [    0.1075,     0.5711,     0.3123,     0.0089,     0.0001],
        [    0.6881,     0.2876,     0.0233,     0.0007,     0.0003],
        [    0.0000,     0.0001,     0.0018,     0.0818,     0.9162],
        [    0.0002,     0.0001,     0.0013,     0.0337,     0.9647],
        [    0.0013,     0.0016,     0.0141,     0.1216,     0.8613],
        [    0.0045,     0.0273,     0.6377,     0.2809,     0.0495],
        [    0.0037,     0.0528,     0.5636,     0.3720,     0.0079],
        [    0.0439,     0.6171,     0.3145,     0.0210,     0.0035],
        [    0.0024,     0.0648,     0.7378,     0.1710,     0.0240],
        [    0.0001,     0.0003,     0.0073,     0.3586,     0.6338],
        [    0.0001,     0.0004,     0.0013,     0.0861,     0.9122],
        [    0.9479,     0.0512,     0.0009,     0.0000,     0.0000],
        [    0.0000,     0.0001,     0.0184,     0.8788,     0.1027]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Labels:  Labels:  tensor([3, 1, 1, 4, 0, 2, 4, 1, 3, 0, 2, 3, 2, 2, 4, 4], device='cuda:0')
Labels:  tensor([0, 4, 2, 3, 0, 1, 2, 2, 3, 2, 2, 4, 4, 1, 1, 2], device='cuda:1')
Preds:  tensor([3, 0, 1, 4, 0, 2, 4, 1, 3, 0, 1, 4, 1, 2, 4, 4], device='cuda:0')
Preds:  tensor([0, 4, 1, 4, 0, 2, 3, 2, 2, 2, 0, 4, 4, 1, 1, 4], device='cuda:1')
tensor([3, 0, 0, 1, 0, 3, 0, 0, 3, 1, 4, 4, 4, 1, 2, 0], device='cuda:1')
Outputs:  tensor([[    0.7115,     0.2747,     0.0137,     0.0001,     0.0000],
        [    0.0002,     0.0001,     0.0009,     0.0635,     0.9354],
        [    0.0337,     0.7347,     0.1826,     0.0427,     0.0063],
        [    0.0001,     0.0000,     0.0002,     0.0171,     0.9826],
        [    0.9912,     0.0067,     0.0007,     0.0003,     0.0012],
        [    0.0698,     0.3148,     0.4999,     0.1132,     0.0022],
        [    0.0009,     0.0069,     0.1138,     0.5211,     0.3574],
        [    0.0006,     0.0161,     0.5793,     0.3963,     0.0077],
        [    0.0955,     0.4100,     0.4545,     0.0396,     0.0005],
        [    0.0027,     0.0471,     0.6115,     0.3231,     0.0157],
        [    0.6614,     0.2902,     0.0466,     0.0012,     0.0006],
        [    0.0001,     0.0001,     0.0011,     0.0837,     0.9150],
        [    0.0002,     0.0000,     0.0001,     0.0010,     0.9987],
        [    0.0116,     0.5095,     0.4072,     0.0627,     0.0091],
Outputs:  Preds:  tensor([4, 0, 2, 1, 0, 2, 0, 1, 3, 1, 2, 3, 4, 1, 2, 0], device='cuda:1')
        [    0.0022,     0.9957,     0.0016,     0.0004,     0.0001],
        [    0.0028,     0.0019,     0.0212,     0.2851,     0.6890]],
       device='cuda:1')
Outputs:  tensor([[    0.0000,     0.0009,     0.0599,     0.9390,     0.0001],
        [    0.4779,     0.4531,     0.0679,     0.0009,     0.0002],
        [    0.3704,     0.4661,     0.1576,     0.0056,     0.0003],
        [    0.0006,     0.0002,     0.0017,     0.0412,     0.9563],
        [    0.7224,     0.1234,     0.0916,     0.0358,     0.0268],
        [    0.0012,     0.0704,     0.8171,     0.1107,     0.0006],
        [    0.0023,     0.0011,     0.0076,     0.1729,     0.8162],
        [    0.4715,     0.4834,     0.0451,     0.0001,     0.0000],
        [    0.0000,     0.0011,     0.3228,     0.6571,     0.0190],
        [    0.5069,     0.2521,     0.2247,     0.0161,     0.0001],
        [    0.1789,     0.5573,     0.2590,     0.0044,     0.0004],
        [    0.0001,     0.0001,     0.0031,     0.2412,     0.7554],
        [    0.0653,     0.6336,     0.2991,     0.0020,     0.0000],
        [    0.0230,     0.1441,     0.5212,     0.2483,     0.0634],
Metric:  tensor(0.5625, device='cuda:1')
        [    0.0002,     0.0002,     0.0113,     0.4234,     0.5650],
        [    0.0003,     0.0002,     0.0039,     0.1367,     0.8589]],
       device='cuda:0')
------------------------
Metric:  tensor([[    0.0002,     0.0005,     0.0166,     0.4291,     0.5536],
        [    0.6447,     0.3193,     0.0328,     0.0015,     0.0017],
        [    0.1635,     0.3628,     0.4456,     0.0267,     0.0013],
        [    0.3510,     0.5106,     0.1362,     0.0021,     0.0002],
        [    0.8437,     0.1382,     0.0180,     0.0001,     0.0000],
        [    0.0277,     0.1646,     0.6667,     0.1360,     0.0050],
        [    0.8812,     0.1110,     0.0078,     0.0000,     0.0000],
        [    0.3013,     0.6622,     0.0356,     0.0008,     0.0001],
        [    0.0000,     0.0011,     0.3351,     0.6141,     0.0496],
        [    0.2365,     0.4946,     0.2593,     0.0088,     0.0008],
        [    0.2753,     0.1622,     0.2754,     0.1277,     0.1593],
        [    0.0557,     0.0648,     0.3865,     0.4030,     0.0901],
        [    0.0003,     0.0002,     0.0033,     0.1611,     0.8352],
        [    0.2424,     0.5725,     0.1410,     0.0354,     0.0086],
        [    0.0035,     0.1230,     0.8408,     0.0323,     0.0004],
        [    0.9878,     0.0121,     0.0001,     0.0000,     0.0000]],
       device='cuda:1')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 0, 4, 0, 0, 0, 1, 2], device='cuda:1')
Preds:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 1, 4, 4, 0, 0, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0194,     0.0385,     0.5196,     0.3505,     0.0719],
        [    0.4826,     0.4548,     0.0602,     0.0019,     0.0005],
        [    0.0001,     0.0004,     0.0001,     0.0126,     0.9867],
        [    0.0001,     0.0000,     0.0004,     0.0479,     0.9516],
        [    0.0005,     0.0007,     0.0117,     0.2337,     0.7534],
        [    0.6194,     0.3165,     0.0609,     0.0026,     0.0006],
        [    0.7427,     0.2150,     0.0391,     0.0030,     0.0002],
        [    0.0326,     0.2423,     0.7042,     0.0207,     0.0002],
        [    0.0002,     0.0006,     0.0220,     0.3069,     0.6703],
        [    0.0924,     0.5098,     0.3885,     0.0090,     0.0003],
        [    0.0006,     0.0004,     0.0079,     0.2138,     0.7773],
        [    0.0628,     0.0500,     0.0696,     0.1311,     0.6864],
        [    0.6424,     0.3367,     0.0190,     0.0005,     0.0014],
        [    0.9176,     0.0812,     0.0012,     0.0000,     0.0000],
        [    0.0121,     0.2641,     0.7004,     0.0234,     0.0000],
        [    0.1088,     0.1757,     0.3013,     0.1321,     0.2821]],
       device='cuda:1')
Metric:  tensor(0.8125, device='cuda:1')
------------------------
Labels:  tensor([0, 1, 4, 2, 3, 2, 1, 0, 2, 2, 2, 0, 2, 4, 3, 4], device='cuda:0')
Labels:  tensor([1, 1, 1, 1, 2, 1, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Preds:  tensor([0, 1, 4, 1, 0, 2, 1, 0, 1, 3, 3, 0, 2, 4, 0, 4], device='cuda:0')
Preds:  tensor([3, 1, 2, 3, 2, 2, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Outputs:  tensor([[    0.5586,     0.4182,     0.0232,     0.0001,     0.0000],
        [    0.1723,     0.8009,     0.0266,     0.0001,     0.0000],
        [    0.0001,     0.0003,     0.0116,     0.2452,     0.7427],
        [    0.3368,     0.5339,     0.1277,     0.0015,     0.0001],
        [    0.6167,     0.2670,     0.1106,     0.0052,     0.0005],
        [    0.0452,     0.0506,     0.8258,     0.0526,     0.0258],
        [    0.1769,     0.4899,     0.3318,     0.0013,     0.0000],
        [    0.9334,     0.0640,     0.0023,     0.0001,     0.0001],
        [    0.3678,     0.3861,     0.2309,     0.0127,     0.0026],
        [    0.0002,     0.0014,     0.0886,     0.6766,     0.2331],
        [    0.0019,     0.0310,     0.4223,     0.5006,     0.0442],
        [    0.9233,     0.0737,     0.0029,     0.0002,     0.0000],
        [    0.0008,     0.0251,     0.5131,     0.4550,     0.0061],
        [    0.0002,     0.0002,     0.0076,     0.2186,     0.7734],
Outputs:  tensor([[    0.0013,     0.0174,     0.3244,     0.5439,     0.1131],
        [    0.3112,     0.3740,     0.3045,     0.0098,     0.0005],
        [    0.0520,     0.3822,     0.5295,     0.0351,     0.0011],
        [    0.1875,     0.1302,     0.2230,     0.2353,     0.2240],
        [    0.0146,     0.4036,     0.5114,     0.0678,     0.0026],
        [    0.0757,     0.4365,     0.4440,     0.0350,     0.0087],
        [    0.8384,     0.1360,     0.0191,     0.0024,     0.0041],
        [    0.7887,     0.2046,     0.0067,     0.0000,     0.0000],
        [    0.0009,     0.0004,     0.0036,     0.1549,     0.8403],
        [    0.0003,     0.0001,     0.0009,     0.0447,     0.9540],
        [    0.0053,     0.1165,     0.7923,     0.0855,     0.0004],
        [    0.4192,     0.4195,     0.1415,     0.0107,     0.0091],
        [    0.0001,     0.0029,     0.4532,     0.5438,     0.0000],
        [    0.6475,     0.3191,     0.0321,     0.0012,     0.0000],
        [    0.5928,     0.3364,     0.0702,     0.0005,     0.0001],
        [    0.0001,     0.0002,     0.0002,     0.0167,     0.9828]],
       device='cuda:0')
        [    0.7264,     0.2215,     0.0451,     0.0040,     0.0030],
        [    0.1884,     0.5766,     0.2315,     0.0034,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
------------------------
Labels:  tensor([3, 2, 3, 1, 3, 4, 3, 0, 3, 1, 1, 0, 0, 3, 2, 0], device='cuda:1')
Preds:  tensor([3, 2, 3, 1, 2, 4, 3, 1, 4, 1, 1, 0, 0, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0009,     0.0077,     0.3953,     0.5893,     0.0068],
        [    0.0004,     0.0179,     0.6249,     0.3501,     0.0068],
        [    0.0013,     0.0257,     0.4792,     0.4892,     0.0046],
        [    0.4219,     0.4696,     0.1072,     0.0012,     0.0001],
        [    0.0008,     0.0387,     0.9355,     0.0243,     0.0007],
        [    0.0002,     0.0003,     0.0053,     0.1785,     0.8157],
        [    0.0135,     0.0722,     0.4143,     0.4479,     0.0521],
        [    0.1812,     0.5740,     0.2430,     0.0017,     0.0000],
        [    0.0000,     0.0000,     0.0026,     0.2861,     0.7113],
        [    0.1456,     0.5299,     0.2777,     0.0419,     0.0050],
        [    0.1662,     0.5322,     0.2932,     0.0080,     0.0004],
        [    0.9384,     0.0587,     0.0028,     0.0001,     0.0000],
        [    0.6247,     0.3171,     0.0548,     0.0029,     0.0004],
        [    0.0016,     0.0303,     0.3923,     0.5405,     0.0352],
        [    0.1034,     0.3492,     0.4379,     0.0907,     0.0188],
        [    0.0413,     0.2160,     0.6113,     0.1297,     0.0018]],
       device='cuda:1')
Metric:  tensor(0.7500, device='cuda:1')
------------------------
Labels:  tensor([2, 3, 4, 0, 1, 3, 4, 3, 4, 2, 1, 0, 4, 4, 4, 4], device='cuda:1')
Preds:  tensor([1, 3, 4, 1, 2, 4, 4, 2, 4, 2, 0, 0, 4, 4, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0378,     0.5331,     0.4245,     0.0045,     0.0001],
        [    0.0001,     0.0004,     0.0541,     0.7748,     0.1707],
        [    0.0013,     0.0005,     0.0013,     0.0122,     0.9847],
        [    0.2340,     0.7060,     0.0595,     0.0005,     0.0001],
        [    0.0540,     0.2246,     0.4402,     0.2327,     0.0485],
        [    0.0012,     0.0017,     0.0268,     0.2539,     0.7164],
        [    0.0007,     0.0001,     0.0009,     0.0094,     0.9888],
        [    0.0044,     0.2314,     0.7519,     0.0122,     0.0001],
        [    0.0001,     0.0002,     0.0001,     0.0016,     0.9981],
        [    0.0010,     0.0297,     0.7173,     0.2461,     0.0060],
        [    0.5359,     0.4109,     0.0516,     0.0012,     0.0004],
        [    0.7623,     0.2190,     0.0184,     0.0003,     0.0000],
        [    0.0001,     0.0001,     0.0014,     0.0814,     0.9171],
        [    0.0012,     0.0009,     0.0197,     0.3985,     0.5797],
        [    0.0026,     0.0072,     0.1293,     0.5724,     0.2886],
        [    0.0003,     0.0001,     0.0020,     0.1483,     0.8492]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([4, 0, 4, 0, 4, 1, 1, 3, 0, 4, 4, 3, 0, 2, 3, 0], device='cuda:0')
Preds:  tensor([4, 0, 4, 0, 4, 1, 1, 1, 0, 3, 4, 3, 1, 1, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0004,     0.0002,     0.0039,     0.1383,     0.8572],
        [    0.7573,     0.2050,     0.0368,     0.0009,     0.0001],
        [    0.0004,     0.0003,     0.0082,     0.3199,     0.6712],
        [    0.8527,     0.1412,     0.0060,     0.0001,     0.0000],
        [    0.0382,     0.0330,     0.1050,     0.2133,     0.6104],
        [    0.0029,     0.9773,     0.0185,     0.0012,     0.0002],
        [    0.1458,     0.5446,     0.3025,     0.0067,     0.0003],
        [    0.0448,     0.3222,     0.1875,     0.1638,     0.2817],
        [    0.9875,     0.0121,     0.0004,     0.0000,     0.0000],
        [    0.0010,     0.0061,     0.2023,     0.4651,     0.3255],
        [    0.0001,     0.0017,     0.0043,     0.2445,     0.7493],
        [    0.0004,     0.0048,     0.2842,     0.6541,     0.0565],
        [    0.2999,     0.6399,     0.0600,     0.0002,     0.0000],
        [    0.1268,     0.5019,     0.3539,     0.0171,     0.0002],
        [    0.0001,     0.0001,     0.0036,     0.2747,     0.7215],
        [    0.9199,     0.0751,     0.0049,     0.0001,     0.0000]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
Labels:  tensor([3, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 3, 3, 4, 4, 2], device='cuda:0')
------------------------
Preds:  tensor([3, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 4, 2, 4, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0003,     0.0205,     0.6093,     0.3698],
        [    0.0005,     0.0079,     0.8248,     0.1497,     0.0172],
        [    0.0010,     0.0225,     0.7489,     0.2232,     0.0045],
        [    0.0284,     0.1837,     0.6876,     0.0994,     0.0009],
        [    0.4061,     0.5383,     0.0544,     0.0011,     0.0001],
        [    0.0315,     0.0886,     0.4410,     0.3604,     0.0785],
        [    0.2263,     0.6397,     0.1325,     0.0015,     0.0000],
        [    0.9891,     0.0106,     0.0002,     0.0000,     0.0000],
        [    0.0004,     0.0091,     0.5320,     0.4363,     0.0222],
        [    0.0616,     0.3046,     0.4735,     0.1527,     0.0076],
        [    0.5854,     0.3501,     0.0642,     0.0003,     0.0000],
        [    0.0000,     0.0000,     0.0001,     0.0098,     0.9900],
        [    0.0010,     0.0443,     0.7314,     0.2214,     0.0018],
        [    0.0001,     0.0001,     0.0031,     0.2214,     0.7754],
        [    0.0024,     0.0019,     0.0263,     0.1383,     0.8311],
        [    0.7452,     0.2325,     0.0215,     0.0006,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 2, 3, 0, 2, 4, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:1')
Preds:  tensor([4, 2, 2, 2, 4, 0, 1, 4, 2, 0, 1, 0, 0, 0, 0, 0], device='cuda:1')
Outputs:  tensor([[    0.0005,     0.0006,     0.0152,     0.2233,     0.7604],
        [    0.0245,     0.1338,     0.6386,     0.1973,     0.0058],
        [    0.0009,     0.0284,     0.9373,     0.0333,     0.0002],
        [    0.0031,     0.0284,     0.9546,     0.0136,     0.0004],
        [    0.0002,     0.0002,     0.0040,     0.1891,     0.8064],
        [    0.7604,     0.2313,     0.0081,     0.0001,     0.0000],
        [    0.3949,     0.4453,     0.1491,     0.0096,     0.0012],
        [    0.0007,     0.0006,     0.0036,     0.1140,     0.8811],
        [    0.0098,     0.1126,     0.6968,     0.1705,     0.0103],
        [    0.7417,     0.1182,     0.0479,     0.0233,     0.0689],
        [    0.2283,     0.6116,     0.1591,     0.0010,     0.0000],
        [    0.3701,     0.2662,     0.3429,     0.0185,     0.0022],
        [    0.9940,     0.0042,     0.0011,     0.0002,     0.0005],
        [    0.7149,     0.2472,     0.0353,     0.0018,     0.0008],
        [    0.9858,     0.0140,     0.0001,     0.0000,     0.0000],
        [    0.9995,     0.0004,     0.0001,     0.0000,     0.0000]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 4, 2, 3, 2, 4, 1, 2, 1, 1, 2, 0, 1, 0, 1], device='cuda:1')
Preds:  tensor([4, 4, 4, 1, 2, 3, 4, 1, 2, 2, 1, 2, 0, 0, 0, 2], device='cuda:1')
Outputs:  tensor([[    0.0001,     0.0003,     0.0112,     0.1563,     0.8321],
        [    0.0002,     0.0001,     0.0052,     0.2579,     0.7366],
        [    0.0001,     0.0001,     0.0012,     0.0416,     0.9569],
        [    0.2184,     0.5565,     0.2234,     0.0017,     0.0000],
        [    0.0035,     0.0285,     0.6554,     0.2623,     0.0502],
        [    0.0003,     0.0029,     0.1274,     0.7634,     0.1060],
        [    0.0006,     0.0002,     0.0009,     0.0075,     0.9908],
        [    0.1130,     0.4983,     0.3506,     0.0370,     0.0010],
        [    0.0014,     0.0896,     0.8611,     0.0477,     0.0002],
        [    0.0321,     0.2966,     0.5988,     0.0716,     0.0008],
        [    0.2916,     0.3396,     0.2946,     0.0715,     0.0028],
        [    0.0039,     0.1706,     0.7961,     0.0291,     0.0002],
        [    0.9926,     0.0073,     0.0001,     0.0000,     0.0000],
        [    0.7220,     0.2584,     0.0192,     0.0004,     0.0000],
        [    0.7961,     0.1960,     0.0077,     0.0002,     0.0000],
        [    0.0061,     0.1161,     0.7298,     0.1470,     0.0010]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([2, 0, 1, 3, 4, 2, 0, 0, 2, 1, 0, 1, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([4, 0, 2, 2, 4, 2, 1, 0, 1, 0, 0, 0, 0, 1, 4, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0001,     0.0027,     0.1631,     0.8340],
        [    0.8474,     0.1365,     0.0155,     0.0005,     0.0002],
        [    0.0025,     0.0727,     0.8304,     0.0943,     0.0001],
        [    0.0010,     0.0511,     0.9151,     0.0327,     0.0001],
        [    0.0010,     0.0004,     0.0024,     0.0265,     0.9696],
        [    0.0040,     0.1857,     0.6469,     0.1615,     0.0018],
        [    0.4023,     0.4933,     0.1027,     0.0017,     0.0000],
        [    0.8505,     0.1287,     0.0206,     0.0002,     0.0000],
        [    0.0408,     0.7519,     0.1762,     0.0277,     0.0034],
        [    0.5774,     0.3221,     0.0844,     0.0091,     0.0070],
        [    0.9930,     0.0069,     0.0001,     0.0000,     0.0000],
        [    0.6298,     0.2770,     0.0901,     0.0031,     0.0000],
        [    0.7486,     0.1700,     0.0710,     0.0078,     0.0026],
        [    0.0212,     0.7754,     0.1739,     0.0273,     0.0021],
        [    0.0012,     0.0036,     0.0559,     0.3553,     0.5840],
        [    0.0001,     0.0001,     0.0002,     0.0400,     0.9597]],
       device='cuda:0')
Metric:  tensor(0.3750, device='cuda:0')
Labels:  tensor([4, 1, 1, 0, 0, 1, 2, 4, 2, 4, 4, 1, 0, 1, 2, 1], device='cuda:0')
------------------------
Preds:  tensor([4, 1, 0, 0, 0, 1, 2, 4, 2, 4, 4, 4, 0, 0, 2, 0], device='cuda:0')
Outputs:  tensor([[    0.0008,     0.0006,     0.0026,     0.0690,     0.9270],
        [    0.2294,     0.6082,     0.1572,     0.0047,     0.0005],
        [    0.9554,     0.0436,     0.0010,     0.0000,     0.0000],
        [    0.8302,     0.1593,     0.0100,     0.0003,     0.0002],
        [    0.7354,     0.2490,     0.0150,     0.0004,     0.0002],
        [    0.0858,     0.6335,     0.2795,     0.0011,     0.0000],
        [    0.0542,     0.2919,     0.4984,     0.1400,     0.0156],
        [    0.0000,     0.0000,     0.0002,     0.1259,     0.8738],
        [    0.0240,     0.3668,     0.5912,     0.0181,     0.0000],
        [    0.0011,     0.0007,     0.0078,     0.3038,     0.6867],
        [    0.0139,     0.0124,     0.0475,     0.0770,     0.8493],
        [    0.2740,     0.0779,     0.0958,     0.1322,     0.4201],
        [    0.9178,     0.0808,     0.0014,     0.0000,     0.0000],
        [    0.5295,     0.3283,     0.1339,     0.0069,     0.0014],
        [    0.1628,     0.3309,     0.4416,     0.0611,     0.0037],
        [    0.6420,     0.2831,     0.0710,     0.0035,     0.0004]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 2, 1, 1, 4, 1, 4, 4, 1, 3, 2, 4, 3, 3, 4], device='cuda:1')
Preds:  tensor([2, 2, 2, 0, 1, 4, 0, 3, 4, 1, 2, 2, 4, 2, 4, 4], device='cuda:1')
Outputs:  tensor([[    0.0325,     0.1576,     0.5158,     0.2336,     0.0606],
        [    0.0988,     0.4096,     0.4535,     0.0373,     0.0008],
        [    0.0373,     0.2506,     0.6001,     0.1118,     0.0002],
        [    0.6084,     0.3055,     0.0852,     0.0009,     0.0001],
        [    0.3237,     0.5887,     0.0871,     0.0005,     0.0000],
        [    0.0016,     0.0015,     0.0043,     0.0656,     0.9271],
        [    0.6260,     0.3313,     0.0414,     0.0011,     0.0003],
        [    0.0001,     0.0001,     0.0070,     0.5201,     0.4728],
        [    0.0008,     0.0004,     0.0016,     0.0442,     0.9531],
        [    0.3357,     0.5633,     0.0993,     0.0017,     0.0001],
        [    0.0071,     0.1059,     0.7986,     0.0883,     0.0001],
        [    0.0009,     0.0755,     0.7078,     0.2119,     0.0039],
        [    0.0002,     0.0004,     0.0056,     0.1922,     0.8016],
        [    0.0003,     0.0100,     0.6632,     0.3158,     0.0107],
        [    0.0004,     0.0004,     0.0106,     0.1353,     0.8533],
        [    0.0001,     0.0003,     0.0127,     0.4217,     0.5653]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 2, 0, 0, 4, 4, 1, 4, 2, 0, 1, 1, 2, 2, 1], device='cuda:1')
Preds:  tensor([4, 4, 1, 3, 0, 4, 4, 1, 4, 2, 3, 1, 2, 1, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0009,     0.0004,     0.0056,     0.0943,     0.8987],
        [    0.0009,     0.0006,     0.0043,     0.1224,     0.8719],
        [    0.4388,     0.4774,     0.0805,     0.0031,     0.0002],
        [    0.0004,     0.0020,     0.0795,     0.5425,     0.3756],
        [    0.4623,     0.2038,     0.2239,     0.0657,     0.0443],
        [    0.0012,     0.0007,     0.0028,     0.0303,     0.9650],
        [    0.0006,     0.0005,     0.0072,     0.1907,     0.8010],
        [    0.2298,     0.6256,     0.1436,     0.0009,     0.0000],
        [    0.0001,     0.0002,     0.0075,     0.2070,     0.7852],
        [    0.0685,     0.2352,     0.5731,     0.1181,     0.0052],
        [    0.0006,     0.0088,     0.2718,     0.6328,     0.0860],
        [    0.3417,     0.3673,     0.2338,     0.0468,     0.0104],
        [    0.0035,     0.0734,     0.8905,     0.0320,     0.0006],
        [    0.1711,     0.4880,     0.3243,     0.0164,     0.0003],
        [    0.0244,     0.2680,     0.4616,     0.2306,     0.0153],
        [    0.0171,     0.2950,     0.6583,     0.0291,     0.0005]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Mean loss[0.9203105719860964] | Mean metric[0.6059053196681308]
Stupid loss[0.0] | Naive soulution metric[0.2]
EPOCH 2
--------------
Labels:  tensor([1, 1, 2, 4, 2, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 0], device='cuda:0')
Preds:  tensor([1, 2, 2, 4, 0, 4, 0, 4, 4, 2, 1, 2, 0, 3, 4, 1], device='cuda:0')
Outputs:  tensor([[    0.2448,     0.5265,     0.2279,     0.0007,     0.0000],
        [    0.0991,     0.3684,     0.4055,     0.0864,     0.0408],
        [    0.0001,     0.0027,     0.9957,     0.0015,     0.0000],
        [    0.0001,     0.0001,     0.0003,     0.0429,     0.9566],
        [    0.7465,     0.1866,     0.0628,     0.0036,     0.0004],
        [    0.0002,     0.0001,     0.0004,     0.0145,     0.9848],
        [    0.9786,     0.0212,     0.0002,     0.0000,     0.0000],
        [    0.0001,     0.0001,     0.0014,     0.1106,     0.8879],
        [    0.0000,     0.0000,     0.0030,     0.3389,     0.6580],
        [    0.0676,     0.4493,     0.4785,     0.0047,     0.0000],
        [    0.2998,     0.3539,     0.2832,     0.0620,     0.0011],
        [    0.0163,     0.2018,     0.6905,     0.0895,     0.0018],
        [    0.7045,     0.2197,     0.0400,     0.0088,     0.0270],
        [    0.0014,     0.0023,     0.0429,     0.6107,     0.3427],
Labels:  tensor([4, 2, 2, 0, 0, 3, 3, 0, 0, 0, 0, 1, 4, 2, 0, 4], device='cuda:0')
        [    0.0051,     0.0013,     0.0034,     0.0157,     0.9745],
        [    0.0797,     0.5360,     0.3672,     0.0170,     0.0001]],
       device='cuda:0')
Preds:  tensor([4, 4, 2, 0, 0, 3, 3, 1, 0, 0, 4, 0, 3, 2, 0, 4], device='cuda:0')
Metric:  tensor(0.4375, device='cuda:0')
Outputs:  tensor([[    0.0000,     0.0002,     0.0008,     0.1272,     0.8716],
        [    0.0006,     0.0013,     0.0188,     0.2591,     0.7201],
        [    0.1050,     0.3743,     0.4437,     0.0745,     0.0026],
        [    0.9661,     0.0331,     0.0008,     0.0000,     0.0000],
        [    0.6452,     0.3375,     0.0167,     0.0003,     0.0003],
        [    0.0000,     0.0000,     0.0010,     0.9965,     0.0024],
        [    0.0001,     0.0013,     0.1759,     0.7909,     0.0319],
        [    0.2676,     0.6121,     0.0980,     0.0106,     0.0117],
        [    0.9128,     0.0868,     0.0004,     0.0000,     0.0000],
        [    0.8193,     0.1694,     0.0105,     0.0003,     0.0004],
        [    0.0389,     0.0466,     0.0152,     0.1262,     0.7731],
        [    0.7022,     0.2858,     0.0116,     0.0003,     0.0001],
        [    0.0005,     0.0015,     0.0196,     0.8040,     0.1745],
        [    0.2173,     0.3076,     0.3390,     0.0821,     0.0539],
------------------------
        [    0.7147,     0.2412,     0.0396,     0.0029,     0.0016],
        [    0.0000,     0.0001,     0.0009,     0.1185,     0.8805]],
       device='cuda:0')
Mean loss[0.9233505789857774] | Mean metric[0.6079184968277208]
Metric:  tensor(0.6875, device='cuda:0')
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Stupid loss[0.0] | Naive soulution metric[0.2]
------------------------
EPOCH 2
--------------
Mean loss[0.9244014811027103] | Mean metric[0.6036786237188873]
Stupid loss[0.0] | Naive soulution metric[0.2]
EPOCH 2
--------------
Labels:  tensor([0, 4, 0, 3, 0, 1, 2, 1, 4, 2, 1, 1, 4, 3, 1, 3], device='cuda:1')
Preds:  tensor([0, 4, 0, 3, 0, 0, 1, 2, 2, 3, 1, 2, 4, 3, 1, 2], device='cuda:1')
Outputs:  tensor([[    0.5549,     0.4221,     0.0199,     0.0007,     0.0024],
        [    0.0010,     0.0023,     0.0135,     0.1264,     0.8568],
        [    0.8627,     0.1202,     0.0151,     0.0016,     0.0003],
        [    0.0000,     0.0005,     0.0464,     0.7576,     0.1955],
        [    0.8594,     0.1382,     0.0024,     0.0000,     0.0000],
        [    0.6352,     0.3104,     0.0522,     0.0021,     0.0001],
        [    0.1638,     0.3927,     0.3655,     0.0585,     0.0196],
        [    0.0337,     0.3993,     0.5483,     0.0179,     0.0009],
        [    0.0623,     0.1056,     0.4115,     0.3581,     0.0625],
        [    0.0003,     0.0061,     0.3232,     0.5980,     0.0725],
        [    0.0377,     0.5332,     0.4126,     0.0155,     0.0010],
        [    0.0121,     0.1376,     0.7620,     0.0861,     0.0021],
        [    0.0002,     0.0001,     0.0015,     0.1750,     0.8232],
        [    0.0000,     0.0001,     0.0439,     0.8979,     0.0581],
        [    0.0844,     0.4750,     0.4238,     0.0157,     0.0011],
        [    0.0048,     0.2061,     0.4251,     0.3202,     0.0438]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Mean loss[0.9162742498009423] | Mean metric[0.6056612981942411]
Stupid loss[0.0] | Naive soulution metric[0.2]
EPOCH 2
--------------
Step[500] | Loss[0.40449458360671997] | Lr[2.0000000000000003e-06]
Step[500] | Loss[0.8880000114440918] | Lr[2.0000000000000003e-06]
Step[500] | Loss[0.8149355053901672] | Lr[2.0000000000000003e-06]Step[500] | Loss[0.832601010799408] | Lr[2.0000000000000003e-06]

Step[1000] | Loss[0.7012632489204407] | Lr[2.0000000000000003e-06]Step[1000] | Loss[0.7910723686218262] | Lr[2.0000000000000003e-06]

Step[1000] | Loss[0.9541375637054443] | Lr[2.0000000000000003e-06]
Step[1000] | Loss[0.5768420696258545] | Lr[2.0000000000000003e-06]
Step[1500] | Loss[0.6881589889526367] | Lr[2.0000000000000003e-06]Step[1500] | Loss[0.44593918323516846] | Lr[2.0000000000000003e-06]

Step[1500] | Loss[0.8023754358291626] | Lr[2.0000000000000003e-06]
Step[1500] | Loss[0.43668332695961] | Lr[2.0000000000000003e-06]
Step[2000] | Loss[0.44735485315322876] | Lr[2.0000000000000003e-06]Step[2000] | Loss[1.0356286764144897] | Lr[2.0000000000000003e-06]

Step[2000] | Loss[0.6132370829582214] | Lr[2.0000000000000003e-06]
Step[2000] | Loss[0.46139001846313477] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.6770842671394348] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.4269472062587738] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.8779051899909973] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.7146540880203247] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.8498618602752686] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.5680732727050781] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.9374459981918335] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.632371187210083] | Lr[2.0000000000000003e-06]
Step[3500] | Loss[0.864260196685791] | Lr[2.0000000000000003e-06]Step[3500] | Loss[0.5751250982284546] | Lr[2.0000000000000003e-06]

Step[3500] | Loss[0.41997718811035156] | Lr[2.0000000000000003e-06]
Step[3500] | Loss[0.4439919590950012] | Lr[2.0000000000000003e-06]
Step[4000] | Loss[0.8446973562240601] | Lr[2.0000000000000003e-06]Step[4000] | Loss[0.981012225151062] | Lr[2.0000000000000003e-06]

Step[4000] | Loss[0.8055548667907715] | Lr[2.0000000000000003e-06]
Step[4000] | Loss[0.8968088030815125] | Lr[2.0000000000000003e-06]
Step[4500] | Loss[0.6968480348587036] | Lr[2.0000000000000003e-06]Step[4500] | Loss[0.9038554430007935] | Lr[2.0000000000000003e-06]

Step[4500] | Loss[0.9664742350578308] | Lr[2.0000000000000003e-06]Step[4500] | Loss[0.7876865267753601] | Lr[2.0000000000000003e-06]

Step[5000] | Loss[0.8814247250556946] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.7726638317108154] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.5525962114334106] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.426408052444458] | Lr[2.0000000000000003e-06]
Step[5500] | Loss[0.3702341318130493] | Lr[2.0000000000000003e-06]Step[5500] | Loss[0.8128411769866943] | Lr[2.0000000000000003e-06]

Step[5500] | Loss[0.56504887342453] | Lr[2.0000000000000003e-06]
Step[5500] | Loss[0.6150640249252319] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.4918704330921173] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.4146440029144287] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.9446513652801514] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.47919970750808716] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.5987488031387329] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.5385582447052002] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.828468382358551] | Lr[2.0000000000000003e-06]Step[6500] | Loss[0.8495050072669983] | Lr[2.0000000000000003e-06]

Step[7000] | Loss[0.6504190564155579] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[0.6014568209648132] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[0.9225051403045654] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[0.5554097890853882] | Lr[2.0000000000000003e-06]
Step[7500] | Loss[0.6447477340698242] | Lr[2.0000000000000003e-06]Step[7500] | Loss[0.560537576675415] | Lr[2.0000000000000003e-06]

Step[7500] | Loss[0.683301568031311] | Lr[2.0000000000000003e-06]
Step[7500] | Loss[0.5520828366279602] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.5640174150466919] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.3660520911216736] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.3556616008281708] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.7682569026947021] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.7521452307701111] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.868167519569397] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.6305244565010071] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.7588376402854919] | Lr[2.0000000000000003e-06]
Step[9000] | Loss[0.7404604554176331] | Lr[2.0000000000000003e-06]Step[9000] | Loss[0.6878905892372131] | Lr[2.0000000000000003e-06]

Step[9000] | Loss[0.8455063104629517] | Lr[2.0000000000000003e-06]Step[9000] | Loss[0.6461509466171265] | Lr[2.0000000000000003e-06]

Step[9500] | Loss[0.8761221170425415] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.907492995262146] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.4327520430088043] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.43058401346206665] | Lr[2.0000000000000003e-06]
Step[10000] | Loss[0.6178878545761108] | Lr[2.0000000000000003e-06]Step[10000] | Loss[1.032824993133545] | Lr[2.0000000000000003e-06]

Step[10000] | Loss[0.763196587562561] | Lr[2.0000000000000003e-06]
Step[10000] | Loss[0.24704191088676453] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[1.0519227981567383] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.8655335307121277] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.7487295866012573] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.4673643708229065] | Lr[2.0000000000000003e-06]
Step[11000] | Loss[0.6106418967247009] | Lr[2.0000000000000003e-06]Step[11000] | Loss[0.8527800440788269] | Lr[2.0000000000000003e-06]

Step[11000] | Loss[0.5822965502738953] | Lr[2.0000000000000003e-06]
Step[11000] | Loss[0.9497525095939636] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.749410092830658] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.7781678438186646] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.6331497430801392] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.5780490636825562] | Lr[2.0000000000000003e-06]
Step[12000] | Loss[0.5005283951759338] | Lr[2.0000000000000003e-06]Step[12000] | Loss[0.4982836842536926] | Lr[2.0000000000000003e-06]

Step[12000] | Loss[0.4996539056301117] | Lr[2.0000000000000003e-06]
Step[12000] | Loss[0.5598610639572144] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.4811684489250183] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.7810292840003967] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.5249514579772949] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.5911421179771423] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.5622819662094116] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.4430718421936035] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.4790535569190979] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.3324429988861084] | Lr[2.0000000000000003e-06]
Step[13500] | Loss[0.3916667103767395] | Lr[2.0000000000000003e-06]Step[13500] | Loss[0.6397125124931335] | Lr[2.0000000000000003e-06]

Step[13500] | Loss[0.5923504829406738] | Lr[2.0000000000000003e-06]
Step[13500] | Loss[0.7686647772789001] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[0.6211315393447876] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[0.8687602877616882] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[1.1368064880371094] | Lr[2.0000000000000003e-06]Step[14000] | Loss[0.6137251853942871] | Lr[2.0000000000000003e-06]

Step[14500] | Loss[0.5157033801078796] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.8810259699821472] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.8550419807434082] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.5609333515167236] | Lr[2.0000000000000003e-06]
Step[15000] | Loss[0.7038712501525879] | Lr[2.0000000000000003e-06]Step[15000] | Loss[0.7745915055274963] | Lr[2.0000000000000003e-06]

Step[15000] | Loss[0.665668249130249] | Lr[2.0000000000000003e-06]
Step[15000] | Loss[0.996113121509552] | Lr[2.0000000000000003e-06]
Step[15500] | Loss[0.6490330100059509] | Lr[2.0000000000000003e-06]Step[15500] | Loss[0.7354671955108643] | Lr[2.0000000000000003e-06]

Step[15500] | Loss[0.5728302597999573] | Lr[2.0000000000000003e-06]
Step[15500] | Loss[0.7673265933990479] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.6969304084777832] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.7853655815124512] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[1.0634398460388184] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.5978640913963318] | Lr[2.0000000000000003e-06]
Step[16500] | Loss[0.7221857905387878] | Lr[2.0000000000000003e-06]Step[16500] | Loss[0.7511730790138245] | Lr[2.0000000000000003e-06]

Step[16500] | Loss[0.5523639917373657] | Lr[2.0000000000000003e-06]
Step[16500] | Loss[0.7295194864273071] | Lr[2.0000000000000003e-06]
Step[17000] | Loss[0.38279494643211365] | Lr[2.0000000000000003e-06]Step[17000] | Loss[0.3928850591182709] | Lr[2.0000000000000003e-06]

Step[17000] | Loss[0.7872065305709839] | Lr[2.0000000000000003e-06]Step[17000] | Loss[0.805975079536438] | Lr[2.0000000000000003e-06]

Step[17500] | Loss[0.5794530510902405] | Lr[2.0000000000000003e-06]Step[17500] | Loss[0.9686764478683472] | Lr[2.0000000000000003e-06]

Step[17500] | Loss[0.805801510810852] | Lr[2.0000000000000003e-06]
Step[17500] | Loss[0.6943854689598083] | Lr[2.0000000000000003e-06]
Step[18000] | Loss[0.7767108678817749] | Lr[2.0000000000000003e-06]Step[18000] | Loss[0.5380058288574219] | Lr[2.0000000000000003e-06]

Step[18000] | Loss[0.8265140056610107] | Lr[2.0000000000000003e-06]
Step[18000] | Loss[0.5737048387527466] | Lr[2.0000000000000003e-06]
Step[18500] | Loss[0.46267151832580566] | Lr[2.0000000000000003e-06]Step[18500] | Loss[0.5939506888389587] | Lr[2.0000000000000003e-06]

Step[18500] | Loss[0.5918883681297302] | Lr[2.0000000000000003e-06]
Step[18500] | Loss[0.6493792533874512] | Lr[2.0000000000000003e-06]
Step[19000] | Loss[0.644899845123291] | Lr[2.0000000000000003e-06]Step[19000] | Loss[0.47323116660118103] | Lr[2.0000000000000003e-06]

Step[19000] | Loss[0.5530261993408203] | Lr[2.0000000000000003e-06]Step[19000] | Loss[0.8506267666816711] | Lr[2.0000000000000003e-06]

Step[19500] | Loss[0.5997377634048462] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.6545960903167725] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.463174432516098] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.4677765369415283] | Lr[2.0000000000000003e-06]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
slurmstepd: error: *** JOB 59558 ON gpu001 CANCELLED AT 2023-11-09T03:05:19 ***
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 736834 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 736835 closing signal SIGTERM
slurmstepd: error: *** STEP 59558.1 ON gpu001 CANCELLED AT 2023-11-09T03:05:19 ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 178473 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 178474 closing signal SIGTERM
