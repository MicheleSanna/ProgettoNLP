Node IP: 10.128.2.151
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 9307
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.151:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 9307
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.151:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_zc58gfn0/9307_x7n7r4cb
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_jrmjbwf4/9307_aa2d9gsf
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu001.hpc
  master_port=55363
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu001.hpc
  master_port=55363
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zc58gfn0/9307_x7n7r4cb/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zc58gfn0/9307_x7n7r4cb/attempt_0/1/error.json
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_jrmjbwf4/9307_aa2d9gsf/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_jrmjbwf4/9307_aa2d9gsf/attempt_0/1/error.json
PORT:  55363
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  1
My rank is:  3
PORT:  55363
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  1
My rank is:  2
PORT:  55363
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  0
My rank is:  1
PORT:  55363
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  0
My rank is:  0
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
------------------------

------------------------

------------------------

------------------------

Loading checkpoint...
Loading checkpoint...
Loading checkpoint...
Loading checkpoint...
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 0 using GPU 0
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 1 using GPU 1
LOADED!
I'm process 2 using GPU 0
LOADED!
I'm process 3 using GPU 1
Labels:  tensor([0.7500, 0.0000, 0.0000, 0.2500, 0.0000, 0.7500, 0.0000, 0.0000, 0.7500,
        0.2500, 1.0000, 1.0000, 1.0000, 0.2500, 0.5000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.7500, 0.2500, 0.2500, 1.0000, 0.0000, 0.5000, 1.0000, 0.2500, 0.7500,
        0.0000, 0.5000, 0.7500, 0.5000, 0.5000, 1.0000, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.5000, 0.5000, 0.2500, 0.2500, 0.2500, 1.0000, 1.0000, 1.0000, 0.7500,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 0.0000, 0.7500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.0000, 1.0000, 0.5000, 0.7500, 0.0000, 0.2500, 0.5000, 0.5000, 0.7500,
        0.5000, 0.5000, 1.0000, 1.0000, 0.2500, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.0000, 0.2500, 1.0000, 0.5000, 0.7500, 0.5000, 0.2500, 0.0000, 0.5000,
        0.5000, 0.5000, 0.0000, 0.5000, 1.0000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.2500, 0.7500, 1.0000, 0.7500, 0.0000, 0.7500,
        0.2500, 0.2500, 0.0000, 0.0000, 0.7500, 0.5000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.5000, 1.0000,
        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.2500, 0.0000, 0.0000, 1.0000,
        1.0000, 0.5000, 0.2500, 0.7500, 0.0000, 0.0000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.7500, 0.7500, 0.5000, 0.2500, 0.7500, 0.2500, 0.2500, 0.5000,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 1.0000, 0.5000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.5000, 0.7500, 1.0000, 0.0000, 0.2500, 0.7500, 1.0000, 0.7500, 1.0000,
        0.5000, 0.2500, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.5000, 0.7500, 0.0000, 0.5000, 1.0000, 0.2500,
        0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2500, 0.2500, 0.7500, 0.0000,
        1.0000, 1.0000, 0.7500, 0.0000, 0.5000, 0.7500, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([1.0000, 1.0000, 1.0000, 0.5000, 0.7500, 0.5000, 1.0000, 0.2500, 0.5000,
        0.2500, 0.2500, 0.5000, 0.0000, 0.2500, 0.0000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.5000, 1.0000, 0.5000,
        1.0000, 1.0000, 0.2500, 0.0000, 0.2500, 0.5000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 0.2500, 0.2500, 1.0000, 0.2500, 1.0000, 1.0000,
        0.2500, 0.7500, 0.5000, 1.0000, 0.7500, 0.7500, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.0000, 0.2500, 0.7500, 1.0000, 0.5000, 0.0000, 0.0000, 0.5000,
        0.2500, 0.0000, 0.2500, 0.2500, 0.5000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([1.0000, 1.0000, 0.5000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2500, 1.0000,
        0.5000, 0.0000, 0.2500, 0.2500, 0.5000, 0.5000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Mean loss[0.1256157465791807] | Mean r^2[-0.0734429138936666]
Stupid loss[0.12561577293801854] | Stupid r^2[-0.07344347893057246]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 1.0000, 0.5000, 1.0000, 0.0000, 0.5000, 0.7500,
        0.5000, 0.0000, 0.2500, 0.2500, 0.7500, 0.5000, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Mean loss[0.1255596315732754] | Mean r^2[-0.07335987482029034]
Stupid loss[0.1255604868228404] | Stupid r^2[-0.07336748809916849]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Labels:  tensor([1.0000, 0.5000, 0.5000, 0.0000, 0.0000, 0.7500, 0.7500, 0.0000, 0.0000,
        0.0000, 0.0000, 0.2500, 1.0000, 0.5000, 0.0000, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Mean loss[0.12420287277965851] | Mean r^2[-0.08124184148843373]
Stupid loss[0.12420311737432894] | Stupid r^2[-0.08124547332830695]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Labels:  tensor([0.0000, 1.0000, 0.0000, 0.7500, 0.0000, 0.2500, 0.5000, 0.2500, 1.0000,
        0.5000, 0.2500, 0.2500, 1.0000, 0.7500, 0.2500, 0.7500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Mean loss[0.12501527175394542] | Mean r^2[-0.077325422627685]
Stupid loss[0.1250152513421181] | Stupid r^2[-0.07732605078908046]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Step[500] | Loss[0.14456912875175476] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.12508520483970642] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.1367410123348236] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.12506334483623505] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.15614616870880127] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.12500786781311035] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.1288674771785736] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.1250496506690979] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.1603013426065445] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.08592338860034943] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.16783423721790314] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.08206171542406082] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.10547873377799988] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.07815232127904892] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.1601296216249466] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.08986204862594604] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.1444810926914215] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.13684509694576263] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.12502211332321167] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.1484614461660385] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.1563551127910614] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.1288939267396927] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.12502914667129517] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.14057698845863342] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.10943835228681564] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.1445242464542389] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.16411958634853363] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.15620863437652588] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.12107838690280914] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.16017869114875793] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.15225209295749664] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.09374800324440002] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.16428764164447784] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.11711196601390839] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.18357837200164795] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.15235671401023865] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.08200176805257797] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.11717310547828674] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.1485673487186432] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.14845594763755798] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.11329544335603714] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.15239538252353668] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.16014526784420013] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.11332065612077713] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.10152531415224075] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.14058275520801544] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.07424113154411316] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.13282474875450134] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.17197564244270325] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.16386766731739044] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.13287565112113953] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.09773948788642883] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.16404813528060913] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.17188972234725952] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.1132977157831192] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.11319869011640549] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.14059288799762726] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.12892715632915497] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.15230825543403625] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.13661929965019226] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.14453089237213135] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.1562870889902115] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.1444738805294037] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.11722628027200699] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.13282959163188934] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.13281036913394928] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.09777645766735077] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.14059016108512878] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.11708073318004608] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.09378358721733093] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.12114772200584412] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.16409888863563538] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.11718551814556122] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.17173674702644348] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.09760376811027527] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.1758045256137848] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.16808149218559265] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.1446429193019867] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.08985636383295059] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.09378424286842346] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.10163860023021698] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.1719989776611328] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.1406307816505432] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.11327098309993744] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.10153692960739136] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.10153572261333466] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.09371025115251541] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.11710746586322784] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.1133585274219513] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.15636101365089417] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.12507252395153046] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.11327029019594193] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.05073828250169754] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.17176216840744019] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.07805684208869934] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.11718900501728058] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.09765474498271942] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.10158554464578629] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.08985607326030731] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.12102639675140381] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.1757802963256836] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.12112564593553543] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.15236863493919373] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.12494075298309326] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.10546693205833435] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.10937350988388062] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.07810868322849274] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.15998393297195435] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.09374028444290161] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.14454956352710724] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.13673323392868042] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.1251002848148346] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.11711961030960083] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.15236368775367737] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.08990299701690674] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.16780880093574524] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.12898492813110352] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.11711932718753815] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.16015948355197906] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.08985982835292816] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.19149000942707062] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.08991077542304993] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.10157465934753418] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.11722087860107422] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.15227335691452026] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.12881478667259216] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.12875372171401978] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.13666410744190216] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.13278788328170776] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.09369371086359024] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.13674131035804749] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.16405202448368073] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.1758025884628296] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.09765660017728806] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.10936415195465088] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.12110324949026108] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.1601618230342865] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.10146957635879517] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.11713586747646332] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.11321468651294708] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.1718951165676117] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.15250220894813538] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.15233050286769867] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.08987540006637573] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.16783122718334198] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.19136640429496765] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.11329755932092667] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.05857960879802704] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.12115880846977234] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.10936480760574341] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.10152903944253922] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.09767438471317291] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.16404271125793457] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.09373912215232849] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.1328532099723816] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.12896597385406494] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.14841364324092865] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.1640961617231369] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.10154502838850021] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.12122742831707001] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.08980601280927658] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.14454792439937592] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.09755225479602814] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.13280397653579712] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.15249937772750854] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.042978864163160324] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.12501834332942963] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.13674284517765045] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.10934768617153168] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.15228687226772308] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.16418948769569397] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.08983507752418518] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.1327229142189026] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.1366470754146576] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.14835642278194427] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.12113068997859955] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.1406227946281433] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.11713135987520218] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.18365943431854248] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.1406494826078415] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.1289636492729187] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.10944944620132446] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.12108883261680603] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.13273128867149353] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.0820564404129982] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.10936951637268066] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.1678815335035324] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.12494099140167236] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.1211547777056694] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.13276991248130798] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.1446506232023239] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.1404246836900711] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.08206791430711746] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.105473093688488] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.1328214704990387] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.12108536064624786] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.09378203749656677] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.16403961181640625] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.07424145936965942] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.09367969632148743] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.12109936773777008] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.10157966613769531] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.11719425022602081] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.14448460936546326] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.1367238312959671] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.14060688018798828] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.17187905311584473] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.09765978157520294] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.1446753442287445] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.1250348836183548] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.13666749000549316] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.09367379546165466] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.0898241400718689] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.1132640391588211] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.14439520239830017] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.1366981565952301] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.1405194103717804] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.15242457389831543] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.16405773162841797] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.10548143088817596] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.1289048045873642] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.10548338294029236] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.07808388769626617] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.10945957899093628] | Lr[8.000000000000003e-08]
Step[28500] | Loss[0.1328350007534027] | Lr[8.000000000000003e-08]
Step[28500] | Loss[0.16405606269836426] | Lr[8.000000000000003e-08]
Step[28500] | Loss[0.15632027387619019] | Lr[8.000000000000003e-08]
Step[28500] | Loss[0.10937696695327759] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.11722555011510849] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.1093939021229744] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.11724621802568436] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.10943814367055893] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.14458781480789185] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.13269473612308502] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.11325036734342575] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.15224114060401917] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.15627317130565643] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.1562524437904358] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.10552513599395752] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.07035256177186966] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.12892667949199677] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.0898573100566864] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.1289021372795105] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.12108010053634644] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.09763047099113464] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.06251630187034607] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.10556116700172424] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.10552506148815155] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.14851851761341095] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.17179203033447266] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.10155342519283295] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.06253078579902649] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.10540860891342163] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.13672365248203278] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.13284003734588623] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.12883569300174713] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.13277603685855865] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.10934522747993469] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.1718309372663498] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.09378695487976074] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.11722396314144135] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.13289572298526764] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.10547816753387451] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.11716396361589432] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.15621834993362427] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.1406157910823822] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.14462515711784363] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.12501932680606842] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.16003865003585815] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.11336176842451096] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.14459535479545593] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.17194864153862] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.10157925635576248] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.12506553530693054] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.12108568847179413] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.13269585371017456] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.16012606024742126] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.17968319356441498] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.12881013751029968] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.1016189232468605] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.12506209313869476] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.15243156254291534] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.11715266853570938] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.09773336350917816] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.11333005130290985] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.1444840133190155] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.1679733395576477] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.12114709615707397] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.16014103591442108] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.10545376688241959] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.1016107127070427] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.11326198279857635] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.14444392919540405] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.1328524947166443] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.1367809921503067] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.17986515164375305] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.1172376424074173] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.16394338011741638] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.12506040930747986] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.11327928304672241] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.13661682605743408] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.11713109910488129] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.12885023653507233] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.1367226243019104] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.10935748368501663] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.12111014872789383] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.10160412639379501] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.13276124000549316] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.10941966623067856] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.17175942659378052] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.10146913677453995] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.14056625962257385] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.1210951879620552] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.12483769655227661] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.13282611966133118] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.13674527406692505] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.11335549503564835] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.132884681224823] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.1406128704547882] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.12120603770017624] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.12897248566150665] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.14444245398044586] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.12494738399982452] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.14453265070915222] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.10941680520772934] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.16019079089164734] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.12495426088571548] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.12108767032623291] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.1756066530942917] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.08594594895839691] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.14450350403785706] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.13675962388515472] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.16786162555217743] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.1250479519367218] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.12490347772836685] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.08981408178806305] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.13670088350772858] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.1250862181186676] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.13275417685508728] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.12498188018798828] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.14858153462409973] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.12886148691177368] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.14073893427848816] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.1367146074771881] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.0977320745587349] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.08595278859138489] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.14830204844474792] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.14849597215652466] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.12111005187034607] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.08981602638959885] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.12110245227813721] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.1677735149860382] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.13281023502349854] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.12494999170303345] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.12101094424724579] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.17962217330932617] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.17962242662906647] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.11326739937067032] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.1132446900010109] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.08961459994316101] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.1641552746295929] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.10931971669197083] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.11331965029239655] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.11324866861104965] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.18381661176681519] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.1407088041305542] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.12512193620204926] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.08595902472734451] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.10151312500238419] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.09766485542058945] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.16408248245716095] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.14832931756973267] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.0937773734331131] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.10157535970211029] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.1209629625082016] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.12891274690628052] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.09762995690107346] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.11325670778751373] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.10930357873439789] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.09376083314418793] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.14436075091362] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.13278800249099731] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.12886187434196472] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.1093323677778244] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.17962568998336792] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.15230655670166016] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.15219449996948242] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.14450578391551971] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.08597886562347412] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.08599385619163513] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.11718670278787613] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.1209820955991745] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.12108011543750763] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.13664184510707855] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.14455239474773407] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.1716572493314743] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.10928873717784882] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.14845481514930725] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.14059589803218842] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.15215545892715454] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.14837929606437683] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.12501445412635803] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.07812776416540146] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.1445041000843048] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.10158240795135498] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.15621411800384521] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.09372259676456451] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.12111616134643555] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.1328834593296051] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.0703374445438385] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.12506574392318726] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.12098574638366699] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.1406341791152954] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.07808315753936768] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.12113239616155624] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.06643997132778168] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.09380904585123062] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.10154523700475693] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.132775217294693] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.10940548777580261] | Lr[8.000000000000003e-08]
Step[53000] | Loss[0.14455269277095795] | Lr[8.000000000000003e-08]
Step[53000] | Loss[0.1483665257692337] | Lr[8.000000000000003e-08]
Step[53000] | Loss[0.1328294277191162] | Lr[8.000000000000003e-08]
Step[53000] | Loss[0.13666871190071106] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.12496979534626007] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.11335182934999466] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.13279499113559723] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.1250271499156952] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.1132352352142334] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.12897375226020813] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.10547962784767151] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.14461001753807068] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.1327490508556366] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.15229329466819763] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.11716030538082123] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.07815711945295334] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.10155121237039566] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.0976436585187912] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.12102072685956955] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.13295432925224304] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.10936220735311508] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.12887237966060638] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.10942739248275757] | Lr[8.000000000000003e-08]Step[55500] | Loss[0.11332804709672928] | Lr[8.000000000000003e-08]

Step[56000] | Loss[0.14845916628837585] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.14840860664844513] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.15624290704727173] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.10938769578933716] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.10935701429843903] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.14843085408210754] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.13663136959075928] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.09380733966827393] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.16022571921348572] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.13673008978366852] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.14449915289878845] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.1561436653137207] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.12113527953624725] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.1678592413663864] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.1132509708404541] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.08598751574754715] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.12886455655097961] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.17188850045204163] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.13275805115699768] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.09373034536838531] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.11325038224458694] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.14450478553771973] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.16424712538719177] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.1288701742887497] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.10543228685855865] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.17966711521148682] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.10949397832155228] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.14458796381950378] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.11330904066562653] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.13284185528755188] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.1601049154996872] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.14056959748268127] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.132731631398201] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.062458183616399765] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.13289311528205872] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.15618805587291718] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.14065983891487122] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.11722725629806519] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.09384390711784363] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.12120287120342255] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.07422273606061935] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.16804984211921692] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.13286885619163513] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.09766647219657898] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.1485428661108017] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.11717753112316132] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.10935986042022705] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.11716665327548981] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.12113101780414581] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.13671988248825073] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.11313335597515106] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.12892940640449524] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.15622588992118835] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.08599729835987091] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.08987381309270859] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.13284239172935486] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.11321327835321426] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.13665229082107544] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.17199045419692993] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.09813421964645386] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.11723758280277252] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.12893059849739075] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.12101151049137115] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.11331693083047867] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.09764018654823303] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.09378077834844589] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.10554500669240952] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.1328151822090149] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.1288415491580963] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.07803831249475479] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.10155479609966278] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.14058491587638855] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.0781775563955307] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.1561463326215744] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.10548122972249985] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.09770341962575912] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.1602148860692978] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.1094130426645279] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.1289266049861908] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.09380210936069489] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.11723168194293976] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.11327866464853287] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.12890586256980896] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.12507858872413635] | Lr[8.000000000000003e-08]
Step[66500] | Loss[0.16405683755874634] | Lr[8.000000000000003e-08]
Step[66500] | Loss[0.15225782990455627] | Lr[8.000000000000003e-08]
Step[66500] | Loss[0.1054241731762886] | Lr[8.000000000000003e-08]
Step[66500] | Loss[0.14072349667549133] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.14852522313594818] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.14845490455627441] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.09370620548725128] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.14467671513557434] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.12496918439865112] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.16408106684684753] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.16018734872341156] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.12500037252902985] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.14846768975257874] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.156291663646698] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.07422587275505066] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.0819968432188034] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.140609472990036] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.13672704994678497] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.1640346646308899] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.12893305718898773] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.11715957522392273] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.11716508865356445] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.1639809012413025] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.10545496642589569] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.12499389052391052] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.13676142692565918] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.12102242559194565] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.12505996227264404] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.1250629723072052] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.10928985476493835] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.13278785347938538] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.11333085596561432] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.13669005036354065] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.14470091462135315] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.1328173279762268] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.09376489371061325] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.14827360212802887] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.14846201241016388] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.11715547740459442] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.1756991446018219] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.08980793505907059] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.0937156081199646] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.13279989361763] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.1328241229057312] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.14836449921131134] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.12114682048559189] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.16426339745521545] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.13283249735832214] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.09366896748542786] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.1874784231185913] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.15232357382774353] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.12117679417133331] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.11723346263170242] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.13667622208595276] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.14048823714256287] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.18362072110176086] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.13680166006088257] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.1249445378780365] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.10933900624513626] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.12897232174873352] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.13660496473312378] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.09370367974042892] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.0976279228925705] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.0820499062538147] | Lr[8.000000000000003e-08]
Step[74500] | Loss[0.08988884091377258] | Lr[8.000000000000003e-08]
Step[74500] | Loss[0.15622210502624512] | Lr[8.000000000000003e-08]
Step[74500] | Loss[0.11718042194843292] | Lr[8.000000000000003e-08]
Step[74500] | Loss[0.08973407745361328] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.14442992210388184] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.09375672787427902] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.1406080424785614] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.14451056718826294] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.15238183736801147] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.14463555812835693] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.15236885845661163] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.1445331871509552] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.10155467689037323] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.10945883393287659] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.16408228874206543] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.14447124302387238] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.1444517821073532] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.19907976686954498] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.11719420552253723] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.08599534630775452] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.08976869285106659] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.1485198438167572] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.14844734966754913] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.11328895390033722] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.14063221216201782] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.08990509808063507] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.14848355948925018] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.15222980082035065] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.1133446916937828] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.11727035045623779] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.14460523426532745] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.12895610928535461] | Lr[8.000000000000003e-08]
Labels:  tensor([0.5000, 0.5000, 0.2500, 0.2500, 0.2500, 1.0000, 1.0000, 1.0000, 0.7500,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 0.0000, 0.7500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.0000, 1.0000, 0.5000, 0.7500, 0.0000, 0.2500, 0.5000, 0.5000, 0.7500,
        0.5000, 0.5000, 1.0000, 1.0000, 0.2500, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.7500, 0.2500, 0.2500, 1.0000, 0.0000, 0.5000, 1.0000, 0.2500, 0.7500,
        0.0000, 0.5000, 0.7500, 0.5000, 0.5000, 1.0000, 1.0000],
       device='cuda:0')
Stupid:  Labels:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.7500, 0.0000, 0.0000, 0.2500, 0.0000, 0.7500, 0.0000, 0.0000, 0.7500,
        0.2500, 1.0000, 1.0000, 1.0000, 0.2500, 0.5000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.5000, 1.0000,
        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.2500, 0.0000, 0.0000, 1.0000,
        1.0000, 0.5000, 0.2500, 0.7500, 0.0000, 0.0000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.0000, 0.2500, 1.0000, 0.5000, 0.7500, 0.5000, 0.2500, 0.0000, 0.5000,
        0.5000, 0.5000, 0.0000, 0.5000, 1.0000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.2500, 0.7500, 1.0000, 0.7500, 0.0000, 0.7500,
        0.2500, 0.2500, 0.0000, 0.0000, 0.7500, 0.5000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.7500, 1.0000, 0.0000, 0.2500, 0.7500, 1.0000, 0.7500, 1.0000,
        0.5000, 0.2500, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2500, 0.2500, 0.7500, 0.0000,
        1.0000, 1.0000, 0.7500, 0.0000, 0.5000, 0.7500, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.7500, 0.7500, 0.5000, 0.2500, 0.7500, 0.2500, 0.2500, 0.5000,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 1.0000, 0.5000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.5000, 0.7500, 0.0000, 0.5000, 1.0000, 0.2500,
        0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 1.0000, 1.0000, 0.5000, 0.7500, 0.5000, 1.0000, 0.2500, 0.5000,
        0.2500, 0.2500, 0.5000, 0.0000, 0.2500, 0.0000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.0000, 0.2500, 0.7500, 1.0000, 0.5000, 0.0000, 0.0000, 0.5000,
        0.2500, 0.0000, 0.2500, 0.2500, 0.5000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([1.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.5000, 1.0000, 0.5000,
        1.0000, 1.0000, 0.2500, 0.0000, 0.2500, 0.5000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 0.2500, 0.2500, 1.0000, 0.2500, 1.0000, 1.0000,
        0.2500, 0.7500, 0.5000, 1.0000, 0.7500, 0.7500, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 1.0000, 0.5000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2500, 1.0000,
        0.5000, 0.0000, 0.2500, 0.2500, 0.5000, 0.5000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Mean loss[0.1256157426411725] | Mean r^2[-0.07344284489307246]
Stupid loss[0.12561577293801854] | Stupid r^2[-0.07344347893057246]
Labels:  tensor([1.0000, 0.5000, 0.5000, 0.0000, 0.0000, 0.7500, 0.7500, 0.0000, 0.0000,
        0.0000, 0.0000, 0.2500, 1.0000, 0.5000, 0.0000, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Mean loss[0.12420284598993056] | Mean r^2[-0.08124145413096676]
Stupid loss[0.12420311737432894] | Stupid r^2[-0.08124547332830695]
Labels:  tensor([0.2500, 0.2500, 0.5000, 1.0000, 0.5000, 1.0000, 0.0000, 0.5000, 0.7500,
        0.5000, 0.0000, 0.2500, 0.2500, 0.7500, 0.5000, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Mean loss[0.125559540820913] | Mean r^2[-0.07335906490690829]
Stupid loss[0.1255604868228404] | Stupid r^2[-0.07336748809916849]
Labels:  tensor([0.0000, 1.0000, 0.0000, 0.7500, 0.0000, 0.2500, 0.5000, 0.2500, 1.0000,
        0.5000, 0.2500, 0.2500, 1.0000, 0.7500, 0.2500, 0.7500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Mean loss[0.12501527282480732] | Mean r^2[-0.07732534536563693]
Stupid loss[0.1250152513421181] | Stupid r^2[-0.07732605078908046]
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00044798851013183594 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.5630056858062744 seconds
