Node IP: 10.128.2.152
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 8185
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.152:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 8185
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.152:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_hkpnjtfq/8185_czgcppx7
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_8amjq50r/8185_hwx54a1e
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu002.hpc
  master_port=52353
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu002.hpc
  master_port=52353
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_hkpnjtfq/8185_czgcppx7/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_hkpnjtfq/8185_czgcppx7/attempt_0/1/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_8amjq50r/8185_hwx54a1e/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_8amjq50r/8185_hwx54a1e/attempt_0/1/error.json
PORT:  52353
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  1
My rank is:  3
PORT:  52353
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  1
My rank is:  2
PORT:  52353
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  0
My rank is:  1
PORT:  52353
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  0
My rank is:  0
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
------------------------

------------------------

------------------------

------------------------

Loading checkpoint...
Loading checkpoint...
Loading checkpoint...
Loading checkpoint...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 0 using GPU 0
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 1 using GPU 1
Labels:  tensor([0.5000, 0.5000, 0.2500, 0.2500, 0.2500, 1.0000, 1.0000, 1.0000, 0.7500,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 0.0000, 0.7500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([0.0000, 1.0000, 0.5000, 0.7500, 0.0000, 0.2500, 0.5000, 0.5000, 0.7500,
        0.5000, 0.5000, 1.0000, 1.0000, 0.2500, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 2 using GPU 0
LOADED!
I'm process 3 using GPU 1
Labels:  tensor([0.7500, 0.0000, 0.0000, 0.2500, 0.0000, 0.7500, 0.0000, 0.0000, 0.7500,
        0.2500, 1.0000, 1.0000, 1.0000, 0.2500, 0.5000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([0.7500, 0.2500, 0.2500, 1.0000, 0.0000, 0.5000, 1.0000, 0.2500, 0.7500,
        0.0000, 0.5000, 0.7500, 0.5000, 0.5000, 1.0000, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.2500, 0.0000, 0.0000, 1.0000,
        1.0000, 0.5000, 0.2500, 0.7500, 0.0000, 0.0000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([0.5000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.5000, 1.0000,
        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([0.0000, 0.2500, 1.0000, 0.5000, 0.7500, 0.5000, 0.2500, 0.0000, 0.5000,
        0.5000, 0.5000, 0.0000, 0.5000, 1.0000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.2500, 0.7500, 1.0000, 0.7500, 0.0000, 0.7500,
        0.2500, 0.2500, 0.0000, 0.0000, 0.7500, 0.5000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2500, 0.2500, 0.7500, 0.0000,
        1.0000, 1.0000, 0.7500, 0.0000, 0.5000, 0.7500, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([0.5000, 0.7500, 1.0000, 0.0000, 0.2500, 0.7500, 1.0000, 0.7500, 1.0000,
        0.5000, 0.2500, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([0.7500, 0.7500, 0.7500, 0.5000, 0.2500, 0.7500, 0.2500, 0.2500, 0.5000,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 1.0000, 0.5000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.5000, 0.7500, 0.0000, 0.5000, 1.0000, 0.2500,
        0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.0000, 0.2500, 0.7500, 1.0000, 0.5000, 0.0000, 0.0000, 0.5000,
        0.2500, 0.0000, 0.2500, 0.2500, 0.5000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([1.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.5000, 1.0000, 0.5000,
        1.0000, 1.0000, 0.2500, 0.0000, 0.2500, 0.5000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Labels:  tensor([1.0000, 1.0000, 1.0000, 0.5000, 0.7500, 0.5000, 1.0000, 0.2500, 0.5000,
        0.2500, 0.2500, 0.5000, 0.0000, 0.2500, 0.0000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 0.2500, 0.2500, 1.0000, 0.2500, 1.0000, 1.0000,
        0.2500, 0.7500, 0.5000, 1.0000, 0.7500, 0.7500, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.5000, 0.5000, 0.0000, 0.0000, 0.7500, 0.7500, 0.0000, 0.0000,
        0.0000, 0.0000, 0.2500, 1.0000, 0.5000, 0.0000, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Mean loss[0.124203793673602] | Mean r^2[-0.08125492297712915]
Stupid loss[0.12420311737432894] | Stupid r^2[-0.08124547332830695]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 3
--------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 1.0000, 0.5000, 1.0000, 0.0000, 0.5000, 0.7500,
        0.5000, 0.0000, 0.2500, 0.2500, 0.7500, 0.5000, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:0')
------------------------
Mean loss[0.1255626003223841] | Mean r^2[-0.07338629337913993]
Stupid loss[0.1255604868228404] | Stupid r^2[-0.07336748809916849]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 3
--------------
Labels:  tensor([1.0000, 1.0000, 0.5000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2500, 1.0000,
        0.5000, 0.0000, 0.2500, 0.2500, 0.5000, 0.5000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Mean loss[0.12561593572538676] | Mean r^2[-0.07344569800364326]
Stupid loss[0.12561577293801854] | Stupid r^2[-0.07344347893057246]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 3
--------------
Labels:  tensor([0.0000, 1.0000, 0.0000, 0.7500, 0.0000, 0.2500, 0.5000, 0.2500, 1.0000,
        0.5000, 0.2500, 0.2500, 1.0000, 0.7500, 0.2500, 0.7500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002,
        0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002, 0.5002],
       device='cuda:1')
------------------------
Mean loss[0.12501530399616023] | Mean r^2[-0.07732842904058766]
Stupid loss[0.1250152513421181] | Stupid r^2[-0.07732605078908046]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 3
--------------
Step[500] | Loss[0.14843067526817322] | Lr[4.000000000000001e-07]
Step[500] | Loss[0.1208106130361557] | Lr[4.000000000000001e-07]
Step[500] | Loss[0.10568206012248993] | Lr[4.000000000000001e-07]
Step[500] | Loss[0.14425432682037354] | Lr[4.000000000000001e-07]
Step[1000] | Loss[0.117276132106781] | Lr[4.000000000000001e-07]
Step[1000] | Loss[0.12509137392044067] | Lr[4.000000000000001e-07]
Step[1000] | Loss[0.10568253695964813] | Lr[4.000000000000001e-07]
Step[1000] | Loss[0.15997236967086792] | Lr[4.000000000000001e-07]
Step[1500] | Loss[0.16395142674446106] | Lr[4.000000000000001e-07]
Step[1500] | Loss[0.09366730600595474] | Lr[4.000000000000001e-07]
Step[1500] | Loss[0.12858456373214722] | Lr[4.000000000000001e-07]
Step[1500] | Loss[0.12880051136016846] | Lr[4.000000000000001e-07]
Step[2000] | Loss[0.1132826879620552] | Lr[4.000000000000001e-07]
Step[2000] | Loss[0.17174112796783447] | Lr[4.000000000000001e-07]
Step[2000] | Loss[0.12126961350440979] | Lr[4.000000000000001e-07]
Step[2000] | Loss[0.08994542062282562] | Lr[4.000000000000001e-07]
Step[2500] | Loss[0.1289275884628296] | Lr[4.000000000000001e-07]
Step[2500] | Loss[0.15214329957962036] | Lr[4.000000000000001e-07]
Step[2500] | Loss[0.08992724120616913] | Lr[4.000000000000001e-07]
Step[2500] | Loss[0.16405662894248962] | Lr[4.000000000000001e-07]
Step[3000] | Loss[0.14832839369773865] | Lr[4.000000000000001e-07]
Step[3000] | Loss[0.10554715991020203] | Lr[4.000000000000001e-07]
Step[3000] | Loss[0.11708538979291916] | Lr[4.000000000000001e-07]
Step[3000] | Loss[0.15630218386650085] | Lr[4.000000000000001e-07]
Step[3500] | Loss[0.11721112579107285] | Lr[4.000000000000001e-07]
Step[3500] | Loss[0.0857800841331482] | Lr[4.000000000000001e-07]
Step[3500] | Loss[0.11322087049484253] | Lr[4.000000000000001e-07]
Step[3500] | Loss[0.11690130084753036] | Lr[4.000000000000001e-07]
Step[4000] | Loss[0.1214044913649559] | Lr[4.000000000000001e-07]
Step[4000] | Loss[0.14450103044509888] | Lr[4.000000000000001e-07]
Step[4000] | Loss[0.09760726243257523] | Lr[4.000000000000001e-07]
Step[4000] | Loss[0.13275736570358276] | Lr[4.000000000000001e-07]
Step[4500] | Loss[0.08210385590791702] | Lr[4.000000000000001e-07]
Step[4500] | Loss[0.14846271276474] | Lr[4.000000000000001e-07]
Step[4500] | Loss[0.1446894407272339] | Lr[4.000000000000001e-07]
Step[4500] | Loss[0.08989504724740982] | Lr[4.000000000000001e-07]
Step[5000] | Loss[0.1366959810256958] | Lr[4.000000000000001e-07]
Step[5000] | Loss[0.13695071637630463] | Lr[4.000000000000001e-07]
Step[5000] | Loss[0.1132342666387558] | Lr[4.000000000000001e-07]
Step[5000] | Loss[0.15215250849723816] | Lr[4.000000000000001e-07]
Step[5500] | Loss[0.1913740634918213] | Lr[4.000000000000001e-07]
Step[5500] | Loss[0.1015331894159317] | Lr[4.000000000000001e-07]
Step[5500] | Loss[0.10153667628765106] | Lr[4.000000000000001e-07]
Step[5500] | Loss[0.1210467740893364] | Lr[4.000000000000001e-07]
Step[6000] | Loss[0.14074231684207916] | Lr[4.000000000000001e-07]
Step[6000] | Loss[0.14443475008010864] | Lr[4.000000000000001e-07]
Step[6000] | Loss[0.12085017561912537] | Lr[4.000000000000001e-07]
Step[6000] | Loss[0.1366918683052063] | Lr[4.000000000000001e-07]
Step[6500] | Loss[0.06243085116147995] | Lr[4.000000000000001e-07]
Step[6500] | Loss[0.0939769446849823] | Lr[4.000000000000001e-07]
Step[6500] | Loss[0.14056409895420074] | Lr[4.000000000000001e-07]
Step[6500] | Loss[0.13280268013477325] | Lr[4.000000000000001e-07]
Step[7000] | Loss[0.09779585152864456] | Lr[4.000000000000001e-07]
Step[7000] | Loss[0.12109808623790741] | Lr[4.000000000000001e-07]
Step[7000] | Loss[0.09781838953495026] | Lr[4.000000000000001e-07]
Step[7000] | Loss[0.15229707956314087] | Lr[4.000000000000001e-07]
Step[7500] | Loss[0.1092291921377182] | Lr[4.000000000000001e-07]
Step[7500] | Loss[0.0781465470790863] | Lr[4.000000000000001e-07]
Step[7500] | Loss[0.1252453327178955] | Lr[4.000000000000001e-07]
Step[7500] | Loss[0.1481691598892212] | Lr[4.000000000000001e-07]
Step[8000] | Loss[0.09781192243099213] | Lr[4.000000000000001e-07]
Step[8000] | Loss[0.1484849452972412] | Lr[4.000000000000001e-07]
Step[8000] | Loss[0.09380637854337692] | Lr[4.000000000000001e-07]
Step[8000] | Loss[0.09382932633161545] | Lr[4.000000000000001e-07]
Step[8500] | Loss[0.12887302041053772] | Lr[4.000000000000001e-07]
Step[8500] | Loss[0.1131635457277298] | Lr[4.000000000000001e-07]
Step[8500] | Loss[0.1445913016796112] | Lr[4.000000000000001e-07]
Step[8500] | Loss[0.1211046352982521] | Lr[4.000000000000001e-07]
Step[9000] | Loss[0.15609318017959595] | Lr[4.000000000000001e-07]
Step[9000] | Loss[0.12119971215724945] | Lr[4.000000000000001e-07]
Step[9000] | Loss[0.14854732155799866] | Lr[4.000000000000001e-07]
Step[9000] | Loss[0.14063379168510437] | Lr[4.000000000000001e-07]
Step[9500] | Loss[0.09776899218559265] | Lr[4.000000000000001e-07]
Step[9500] | Loss[0.12919537723064423] | Lr[4.000000000000001e-07]
Step[9500] | Loss[0.15623077750205994] | Lr[4.000000000000001e-07]
Step[9500] | Loss[0.13287682831287384] | Lr[4.000000000000001e-07]
Step[10000] | Loss[0.13677774369716644] | Lr[4.000000000000001e-07]
Step[10000] | Loss[0.10551206767559052] | Lr[4.000000000000001e-07]
Step[10000] | Loss[0.15633194148540497] | Lr[4.000000000000001e-07]
Step[10000] | Loss[0.14846515655517578] | Lr[4.000000000000001e-07]
Step[10500] | Loss[0.15627020597457886] | Lr[4.000000000000001e-07]
Step[10500] | Loss[0.10165709257125854] | Lr[4.000000000000001e-07]
Step[10500] | Loss[0.13685384392738342] | Lr[4.000000000000001e-07]
Step[10500] | Loss[0.0976259857416153] | Lr[4.000000000000001e-07]
Step[11000] | Loss[0.11347658932209015] | Lr[4.000000000000001e-07]
Step[11000] | Loss[0.14436927437782288] | Lr[4.000000000000001e-07]
Step[11000] | Loss[0.08986100554466248] | Lr[4.000000000000001e-07]
Step[11000] | Loss[0.13672736287117004] | Lr[4.000000000000001e-07]
Step[11500] | Loss[0.1483992487192154] | Lr[4.000000000000001e-07]
Step[11500] | Loss[0.13650085031986237] | Lr[4.000000000000001e-07]
Step[11500] | Loss[0.12514826655387878] | Lr[4.000000000000001e-07]
Step[11500] | Loss[0.12503895163536072] | Lr[4.000000000000001e-07]
Step[12000] | Loss[0.11704985797405243] | Lr[4.000000000000001e-07]
Step[12000] | Loss[0.14076414704322815] | Lr[4.000000000000001e-07]
Step[12000] | Loss[0.10930542647838593] | Lr[4.000000000000001e-07]
Step[12000] | Loss[0.14868396520614624] | Lr[4.000000000000001e-07]
Step[12500] | Loss[0.07434001564979553] | Lr[4.000000000000001e-07]
Step[12500] | Loss[0.08587776124477386] | Lr[4.000000000000001e-07]
Step[12500] | Loss[0.1599399447441101] | Lr[4.000000000000001e-07]
Step[12500] | Loss[0.15243738889694214] | Lr[4.000000000000001e-07]
Step[13000] | Loss[0.12895621359348297] | Lr[4.000000000000001e-07]
Step[13000] | Loss[0.15635758638381958] | Lr[4.000000000000001e-07]
Step[13000] | Loss[0.13677504658699036] | Lr[4.000000000000001e-07]
Step[13000] | Loss[0.0935150608420372] | Lr[4.000000000000001e-07]
Step[13500] | Loss[0.1367127001285553] | Lr[4.000000000000001e-07]
Step[13500] | Loss[0.16410745680332184] | Lr[4.000000000000001e-07]
Step[13500] | Loss[0.12114609777927399] | Lr[4.000000000000001e-07]
Step[13500] | Loss[0.13668876886367798] | Lr[4.000000000000001e-07]
Step[14000] | Loss[0.13279035687446594] | Lr[4.000000000000001e-07]
Step[14000] | Loss[0.15267720818519592] | Lr[4.000000000000001e-07]
Step[14000] | Loss[0.15234261751174927] | Lr[4.000000000000001e-07]
Step[14000] | Loss[0.09384603053331375] | Lr[4.000000000000001e-07]
Step[14500] | Loss[0.10935914516448975] | Lr[4.000000000000001e-07]
Step[14500] | Loss[0.12487993389368057] | Lr[4.000000000000001e-07]
Step[14500] | Loss[0.14453984797000885] | Lr[4.000000000000001e-07]
Step[14500] | Loss[0.16399610042572021] | Lr[4.000000000000001e-07]
Step[15000] | Loss[0.14048102498054504] | Lr[4.000000000000001e-07]
Step[15000] | Loss[0.13651683926582336] | Lr[4.000000000000001e-07]
Step[15000] | Loss[0.13284854590892792] | Lr[4.000000000000001e-07]
Step[15000] | Loss[0.10925459861755371] | Lr[4.000000000000001e-07]
Step[15500] | Loss[0.12108500301837921] | Lr[4.000000000000001e-07]
Step[15500] | Loss[0.13678136467933655] | Lr[4.000000000000001e-07]
Step[15500] | Loss[0.12500400841236115] | Lr[4.000000000000001e-07]
Step[15500] | Loss[0.1289343386888504] | Lr[4.000000000000001e-07]
Step[16000] | Loss[0.09763190150260925] | Lr[4.000000000000001e-07]
Step[16000] | Loss[0.13280484080314636] | Lr[4.000000000000001e-07]
Step[16000] | Loss[0.1016140729188919] | Lr[4.000000000000001e-07]
Step[16000] | Loss[0.08603468537330627] | Lr[4.000000000000001e-07]
Step[16500] | Loss[0.1172228530049324] | Lr[4.000000000000001e-07]
Step[16500] | Loss[0.17184612154960632] | Lr[4.000000000000001e-07]
Step[16500] | Loss[0.15617582201957703] | Lr[4.000000000000001e-07]
Step[16500] | Loss[0.1328052282333374] | Lr[4.000000000000001e-07]
Step[17000] | Loss[0.1720207929611206] | Lr[4.000000000000001e-07]
Step[17000] | Loss[0.15623347461223602] | Lr[4.000000000000001e-07]
Step[17000] | Loss[0.13294458389282227] | Lr[4.000000000000001e-07]
Step[17000] | Loss[0.10927151143550873] | Lr[4.000000000000001e-07]
Step[17500] | Loss[0.1329602748155594] | Lr[4.000000000000001e-07]
Step[17500] | Loss[0.10551487654447556] | Lr[4.000000000000001e-07]
Step[17500] | Loss[0.08209764957427979] | Lr[4.000000000000001e-07]
Step[17500] | Loss[0.152364581823349] | Lr[4.000000000000001e-07]
Step[18000] | Loss[0.1054835319519043] | Lr[4.000000000000001e-07]
Step[18000] | Loss[0.10155180096626282] | Lr[4.000000000000001e-07]
Step[18000] | Loss[0.10551504790782928] | Lr[4.000000000000001e-07]
Step[18000] | Loss[0.15224885940551758] | Lr[4.000000000000001e-07]
Step[18500] | Loss[0.12497014552354813] | Lr[4.000000000000001e-07]
Step[18500] | Loss[0.14058613777160645] | Lr[4.000000000000001e-07]
Step[18500] | Loss[0.1290546953678131] | Lr[4.000000000000001e-07]
Step[18500] | Loss[0.1014586091041565] | Lr[4.000000000000001e-07]
Step[19000] | Loss[0.1836288720369339] | Lr[4.000000000000001e-07]
Step[19000] | Loss[0.11704656481742859] | Lr[4.000000000000001e-07]
Step[19000] | Loss[0.12895028293132782] | Lr[4.000000000000001e-07]
Step[19000] | Loss[0.11724917590618134] | Lr[4.000000000000001e-07]
Step[19500] | Loss[0.1249937042593956] | Lr[4.000000000000001e-07]
Step[19500] | Loss[0.10948240756988525] | Lr[4.000000000000001e-07]
Step[19500] | Loss[0.16014529764652252] | Lr[4.000000000000001e-07]
Step[19500] | Loss[0.12104184925556183] | Lr[4.000000000000001e-07]
Step[20000] | Loss[0.13274021446704865] | Lr[4.000000000000001e-07]
Step[20000] | Loss[0.0976664125919342] | Lr[4.000000000000001e-07]
Step[20000] | Loss[0.15236985683441162] | Lr[4.000000000000001e-07]
Step[20000] | Loss[0.11711537837982178] | Lr[4.000000000000001e-07]
Step[20500] | Loss[0.10153532028198242] | Lr[4.000000000000001e-07]
Step[20500] | Loss[0.16405539214611053] | Lr[4.000000000000001e-07]
Step[20500] | Loss[0.0899185910820961] | Lr[4.000000000000001e-07]
Step[20500] | Loss[0.14840896427631378] | Lr[4.000000000000001e-07]
Step[21000] | Loss[0.1483476758003235] | Lr[4.000000000000001e-07]
Step[21000] | Loss[0.11338287591934204] | Lr[4.000000000000001e-07]
Step[21000] | Loss[0.12869518995285034] | Lr[4.000000000000001e-07]
Step[21000] | Loss[0.12891122698783875] | Lr[4.000000000000001e-07]
Step[21500] | Loss[0.17195935547351837] | Lr[4.000000000000001e-07]
Step[21500] | Loss[0.13283441960811615] | Lr[4.000000000000001e-07]
Step[21500] | Loss[0.10148745775222778] | Lr[4.000000000000001e-07]
Step[21500] | Loss[0.15250864624977112] | Lr[4.000000000000001e-07]
Step[22000] | Loss[0.1171831414103508] | Lr[4.000000000000001e-07]
Step[22000] | Loss[0.10548897087574005] | Lr[4.000000000000001e-07]
Step[22000] | Loss[0.148454487323761] | Lr[4.000000000000001e-07]
Step[22000] | Loss[0.1211487352848053] | Lr[4.000000000000001e-07]
Step[22500] | Loss[0.1133219450712204] | Lr[4.000000000000001e-07]
Step[22500] | Loss[0.10943968594074249] | Lr[4.000000000000001e-07]
Step[22500] | Loss[0.08594170212745667] | Lr[4.000000000000001e-07]
Step[22500] | Loss[0.13668173551559448] | Lr[4.000000000000001e-07]
Step[23000] | Loss[0.12496762722730637] | Lr[4.000000000000001e-07]
Step[23000] | Loss[0.07805591821670532] | Lr[4.000000000000001e-07]
Step[23000] | Loss[0.1210937350988388] | Lr[4.000000000000001e-07]
Step[23000] | Loss[0.13280048966407776] | Lr[4.000000000000001e-07]
Step[23500] | Loss[0.08602503687143326] | Lr[4.000000000000001e-07]
Step[23500] | Loss[0.113241046667099] | Lr[4.000000000000001e-07]
Step[23500] | Loss[0.08215512335300446] | Lr[4.000000000000001e-07]
Step[23500] | Loss[0.08595813065767288] | Lr[4.000000000000001e-07]
Step[24000] | Loss[0.13670513033866882] | Lr[4.000000000000001e-07]
Step[24000] | Loss[0.10550050437450409] | Lr[4.000000000000001e-07]
Step[24000] | Loss[0.07421208918094635] | Lr[4.000000000000001e-07]
Step[24000] | Loss[0.1172705888748169] | Lr[4.000000000000001e-07]
Step[24500] | Loss[0.1093839555978775] | Lr[4.000000000000001e-07]
Step[24500] | Loss[0.10162802040576935] | Lr[4.000000000000001e-07]
Step[24500] | Loss[0.1563308984041214] | Lr[4.000000000000001e-07]
Step[24500] | Loss[0.14055036008358002] | Lr[4.000000000000001e-07]
Step[25000] | Loss[0.11715871840715408] | Lr[4.000000000000001e-07]
Step[25000] | Loss[0.1640288531780243] | Lr[4.000000000000001e-07]
Step[25000] | Loss[0.10161649435758591] | Lr[4.000000000000001e-07]
Step[25000] | Loss[0.113433837890625] | Lr[4.000000000000001e-07]
Step[25500] | Loss[0.12887893617153168] | Lr[4.000000000000001e-07]
Step[25500] | Loss[0.10543172806501389] | Lr[4.000000000000001e-07]
Step[25500] | Loss[0.14453929662704468] | Lr[4.000000000000001e-07]
Step[25500] | Loss[0.17574362456798553] | Lr[4.000000000000001e-07]
Step[26000] | Loss[0.11319469660520554] | Lr[4.000000000000001e-07]
Step[26000] | Loss[0.11326517164707184] | Lr[4.000000000000001e-07]
Step[26000] | Loss[0.12109526991844177] | Lr[4.000000000000001e-07]
Step[26000] | Loss[0.12900695204734802] | Lr[4.000000000000001e-07]
Step[26500] | Loss[0.0741526409983635] | Lr[4.000000000000001e-07]
Step[26500] | Loss[0.11317788064479828] | Lr[4.000000000000001e-07]
Step[26500] | Loss[0.1483951210975647] | Lr[4.000000000000001e-07]
Step[26500] | Loss[0.0938999131321907] | Lr[4.000000000000001e-07]
Step[27000] | Loss[0.1328565776348114] | Lr[4.000000000000001e-07]
Step[27000] | Loss[0.10557004809379578] | Lr[4.000000000000001e-07]
Step[27000] | Loss[0.1328074038028717] | Lr[4.000000000000001e-07]
Step[27000] | Loss[0.12105058878660202] | Lr[4.000000000000001e-07]
Step[27500] | Loss[0.16393309831619263] | Lr[4.000000000000001e-07]
Step[27500] | Loss[0.16006125509738922] | Lr[4.000000000000001e-07]
Step[27500] | Loss[0.12492194771766663] | Lr[4.000000000000001e-07]
Step[27500] | Loss[0.12503866851329803] | Lr[4.000000000000001e-07]
Step[28000] | Loss[0.09383438527584076] | Lr[4.000000000000001e-07]
Step[28000] | Loss[0.1173168420791626] | Lr[4.000000000000001e-07]
Step[28000] | Loss[0.12494287639856339] | Lr[4.000000000000001e-07]
Step[28000] | Loss[0.10548768937587738] | Lr[4.000000000000001e-07]
Step[28500] | Loss[0.11322426050901413] | Lr[4.000000000000001e-07]
Step[28500] | Loss[0.1133030354976654] | Lr[4.000000000000001e-07]
Step[28500] | Loss[0.14456689357757568] | Lr[4.000000000000001e-07]
Step[28500] | Loss[0.06634151935577393] | Lr[4.000000000000001e-07]
Step[29000] | Loss[0.17950066924095154] | Lr[4.000000000000001e-07]
Step[29000] | Loss[0.10548603534698486] | Lr[4.000000000000001e-07]
Step[29000] | Loss[0.15228185057640076] | Lr[4.000000000000001e-07]
Step[29000] | Loss[0.113199882209301] | Lr[4.000000000000001e-07]
Step[29500] | Loss[0.15231740474700928] | Lr[4.000000000000001e-07]
Step[29500] | Loss[0.08582644164562225] | Lr[4.000000000000001e-07]
Step[29500] | Loss[0.15624216198921204] | Lr[4.000000000000001e-07]
Step[29500] | Loss[0.12113871425390244] | Lr[4.000000000000001e-07]
Step[30000] | Loss[0.1055479645729065] | Lr[4.000000000000001e-07]
Step[30000] | Loss[0.12521493434906006] | Lr[4.000000000000001e-07]
Step[30000] | Loss[0.10936250537633896] | Lr[4.000000000000001e-07]
Step[30000] | Loss[0.12494741380214691] | Lr[4.000000000000001e-07]
Step[30500] | Loss[0.12500548362731934] | Lr[4.000000000000001e-07]
Step[30500] | Loss[0.12503264844417572] | Lr[4.000000000000001e-07]
Step[30500] | Loss[0.16397994756698608] | Lr[4.000000000000001e-07]
Step[30500] | Loss[0.1289370357990265] | Lr[4.000000000000001e-07]
Step[31000] | Loss[0.1251591444015503] | Lr[4.000000000000001e-07]
Step[31000] | Loss[0.14466936886310577] | Lr[4.000000000000001e-07]
Step[31000] | Loss[0.11334190517663956] | Lr[4.000000000000001e-07]
Step[31000] | Loss[0.14840421080589294] | Lr[4.000000000000001e-07]
Step[31500] | Loss[0.1170525997877121] | Lr[4.000000000000001e-07]
Step[31500] | Loss[0.14454473555088043] | Lr[4.000000000000001e-07]
Step[31500] | Loss[0.1562420129776001] | Lr[4.000000000000001e-07]
Step[31500] | Loss[0.1719561517238617] | Lr[4.000000000000001e-07]
Step[32000] | Loss[0.1289770007133484] | Lr[4.000000000000001e-07]
Step[32000] | Loss[0.14851750433444977] | Lr[4.000000000000001e-07]
Step[32000] | Loss[0.14843280613422394] | Lr[4.000000000000001e-07]
Step[32000] | Loss[0.12497740238904953] | Lr[4.000000000000001e-07]
Step[32500] | Loss[0.1641952395439148] | Lr[4.000000000000001e-07]
Step[32500] | Loss[0.09759119153022766] | Lr[4.000000000000001e-07]
Step[32500] | Loss[0.17571663856506348] | Lr[4.000000000000001e-07]
Step[32500] | Loss[0.13674946129322052] | Lr[4.000000000000001e-07]
Step[33000] | Loss[0.1287943720817566] | Lr[4.000000000000001e-07]
Step[33000] | Loss[0.13677796721458435] | Lr[4.000000000000001e-07]
Step[33000] | Loss[0.1602194905281067] | Lr[4.000000000000001e-07]
Step[33000] | Loss[0.0976661965250969] | Lr[4.000000000000001e-07]
Step[33500] | Loss[0.12881936132907867] | Lr[4.000000000000001e-07]
Step[33500] | Loss[0.1054440587759018] | Lr[4.000000000000001e-07]
Step[33500] | Loss[0.12486033886671066] | Lr[4.000000000000001e-07]
Step[33500] | Loss[0.1093129813671112] | Lr[4.000000000000001e-07]
Step[34000] | Loss[0.1131661981344223] | Lr[4.000000000000001e-07]
Step[34000] | Loss[0.12117031216621399] | Lr[4.000000000000001e-07]
Step[34000] | Loss[0.1523183286190033] | Lr[4.000000000000001e-07]
Step[34000] | Loss[0.14449156820774078] | Lr[4.000000000000001e-07]
Step[34500] | Loss[0.10929970443248749] | Lr[4.000000000000001e-07]
Step[34500] | Loss[0.1407366544008255] | Lr[4.000000000000001e-07]
Step[34500] | Loss[0.10924813151359558] | Lr[4.000000000000001e-07]
Step[34500] | Loss[0.11717015504837036] | Lr[4.000000000000001e-07]
Step[35000] | Loss[0.1524265855550766] | Lr[4.000000000000001e-07]
Step[35000] | Loss[0.15242210030555725] | Lr[4.000000000000001e-07]
Step[35000] | Loss[0.1290176659822464] | Lr[4.000000000000001e-07]
Step[35000] | Loss[0.12106101214885712] | Lr[4.000000000000001e-07]
Step[35500] | Loss[0.15621615946292877] | Lr[4.000000000000001e-07]
Step[35500] | Loss[0.0663904994726181] | Lr[4.000000000000001e-07]
Step[35500] | Loss[0.11728914082050323] | Lr[4.000000000000001e-07]
Step[35500] | Loss[0.11714591085910797] | Lr[4.000000000000001e-07]
Step[36000] | Loss[0.10558098554611206] | Lr[4.000000000000001e-07]
Step[36000] | Loss[0.08990327268838882] | Lr[4.000000000000001e-07]
Step[36000] | Loss[0.1368546187877655] | Lr[4.000000000000001e-07]
Step[36000] | Loss[0.12110583484172821] | Lr[4.000000000000001e-07]
Step[36500] | Loss[0.1015872210264206] | Lr[4.000000000000001e-07]
Step[36500] | Loss[0.11329956352710724] | Lr[4.000000000000001e-07]
Step[36500] | Loss[0.13280907273292542] | Lr[4.000000000000001e-07]
Step[36500] | Loss[0.14060650765895844] | Lr[4.000000000000001e-07]
Step[37000] | Loss[0.12109493464231491] | Lr[4.000000000000001e-07]
Step[37000] | Loss[0.10930837690830231] | Lr[4.000000000000001e-07]
Step[37000] | Loss[0.12105870246887207] | Lr[4.000000000000001e-07]
Step[37000] | Loss[0.13282263278961182] | Lr[4.000000000000001e-07]
Step[37500] | Loss[0.11709664016962051] | Lr[4.000000000000001e-07]
Step[37500] | Loss[0.1719607412815094] | Lr[4.000000000000001e-07]
Step[37500] | Loss[0.1680147647857666] | Lr[4.000000000000001e-07]
Step[37500] | Loss[0.13282141089439392] | Lr[4.000000000000001e-07]
Step[38000] | Loss[0.10934552550315857] | Lr[4.000000000000001e-07]
Step[38000] | Loss[0.1172291487455368] | Lr[4.000000000000001e-07]
Step[38000] | Loss[0.08593099564313889] | Lr[4.000000000000001e-07]
Step[38000] | Loss[0.11713097989559174] | Lr[4.000000000000001e-07]
Step[38500] | Loss[0.12493264675140381] | Lr[4.000000000000001e-07]
Step[38500] | Loss[0.10945464670658112] | Lr[4.000000000000001e-07]
Step[38500] | Loss[0.1367420256137848] | Lr[4.000000000000001e-07]
Step[38500] | Loss[0.05855713412165642] | Lr[4.000000000000001e-07]
Step[39000] | Loss[0.12499555945396423] | Lr[4.000000000000001e-07]
Step[39000] | Loss[0.13673141598701477] | Lr[4.000000000000001e-07]
Step[39000] | Loss[0.1211080327630043] | Lr[4.000000000000001e-07]
Step[39000] | Loss[0.13684934377670288] | Lr[4.000000000000001e-07]
Step[39500] | Loss[0.10939456522464752] | Lr[4.000000000000001e-07]
Step[39500] | Loss[0.17576681077480316] | Lr[4.000000000000001e-07]
Step[39500] | Loss[0.08992718905210495] | Lr[4.000000000000001e-07]
Step[39500] | Loss[0.14848074316978455] | Lr[4.000000000000001e-07]
Step[40000] | Loss[0.10941502451896667] | Lr[4.000000000000001e-07]
Step[40000] | Loss[0.10156156122684479] | Lr[4.000000000000001e-07]Step[40000] | Loss[0.1406218558549881] | Lr[4.000000000000001e-07]

Step[40000] | Loss[0.1484161615371704] | Lr[4.000000000000001e-07]
Step[40500] | Loss[0.11336556077003479] | Lr[4.000000000000001e-07]
Step[40500] | Loss[0.11714005470275879] | Lr[4.000000000000001e-07]
Step[40500] | Loss[0.10940604656934738] | Lr[4.000000000000001e-07]
Step[40500] | Loss[0.10555991530418396] | Lr[4.000000000000001e-07]
Step[41000] | Loss[0.0937981903553009] | Lr[4.000000000000001e-07]
Step[41000] | Loss[0.10551266372203827] | Lr[4.000000000000001e-07]
Step[41000] | Loss[0.15620699524879456] | Lr[4.000000000000001e-07]
Step[41000] | Loss[0.10940851271152496] | Lr[4.000000000000001e-07]
Step[41500] | Loss[0.11325433850288391] | Lr[4.000000000000001e-07]
Step[41500] | Loss[0.15234990417957306] | Lr[4.000000000000001e-07]
Step[41500] | Loss[0.10550637543201447] | Lr[4.000000000000001e-07]
Step[41500] | Loss[0.1251426339149475] | Lr[4.000000000000001e-07]
Step[42000] | Loss[0.1291002333164215] | Lr[4.000000000000001e-07]
Step[42000] | Loss[0.1170923262834549] | Lr[4.000000000000001e-07]
Step[42000] | Loss[0.11323191225528717] | Lr[4.000000000000001e-07]
Step[42000] | Loss[0.10543721914291382] | Lr[4.000000000000001e-07]
Step[42500] | Loss[0.12490665912628174] | Lr[4.000000000000001e-07]
Step[42500] | Loss[0.1367112696170807] | Lr[4.000000000000001e-07]
Step[42500] | Loss[0.14845632016658783] | Lr[4.000000000000001e-07]
Step[42500] | Loss[0.11329107731580734] | Lr[4.000000000000001e-07]
Step[43000] | Loss[0.09379248321056366] | Lr[4.000000000000001e-07]
Step[43000] | Loss[0.1289592683315277] | Lr[4.000000000000001e-07]
Step[43000] | Loss[0.12097180634737015] | Lr[4.000000000000001e-07]
Step[43000] | Loss[0.10938610136508942] | Lr[4.000000000000001e-07]
Step[43500] | Loss[0.10931821167469025] | Lr[4.000000000000001e-07]
Step[43500] | Loss[0.11711475253105164] | Lr[4.000000000000001e-07]
Step[43500] | Loss[0.10543323308229446] | Lr[4.000000000000001e-07]
Step[43500] | Loss[0.1288161277770996] | Lr[4.000000000000001e-07]
Step[44000] | Loss[0.12894612550735474] | Lr[4.000000000000001e-07]
Step[44000] | Loss[0.08985929936170578] | Lr[4.000000000000001e-07]
Step[44000] | Loss[0.14051979780197144] | Lr[4.000000000000001e-07]
Step[44000] | Loss[0.10929881781339645] | Lr[4.000000000000001e-07]
Step[44500] | Loss[0.12891338765621185] | Lr[4.000000000000001e-07]
Step[44500] | Loss[0.1251901090145111] | Lr[4.000000000000001e-07]
Step[44500] | Loss[0.14073213934898376] | Lr[4.000000000000001e-07]
Step[44500] | Loss[0.1367511749267578] | Lr[4.000000000000001e-07]
Step[45000] | Loss[0.175761878490448] | Lr[4.000000000000001e-07]
Step[45000] | Loss[0.10554530471563339] | Lr[4.000000000000001e-07]
Step[45000] | Loss[0.14482572674751282] | Lr[4.000000000000001e-07]
Step[45000] | Loss[0.13273760676383972] | Lr[4.000000000000001e-07]
Step[45500] | Loss[0.13669142127037048] | Lr[4.000000000000001e-07]
Step[45500] | Loss[0.0937027633190155] | Lr[4.000000000000001e-07]
Step[45500] | Loss[0.1210627555847168] | Lr[4.000000000000001e-07]
Step[45500] | Loss[0.1443985253572464] | Lr[4.000000000000001e-07]
Step[46000] | Loss[0.13662828505039215] | Lr[4.000000000000001e-07]
Step[46000] | Loss[0.14072498679161072] | Lr[4.000000000000001e-07]
Step[46000] | Loss[0.12117229402065277] | Lr[4.000000000000001e-07]
Step[46000] | Loss[0.09366057813167572] | Lr[4.000000000000001e-07]
Step[46500] | Loss[0.12495574355125427] | Lr[4.000000000000001e-07]
Step[46500] | Loss[0.08974950760602951] | Lr[4.000000000000001e-07]
Step[46500] | Loss[0.14061805605888367] | Lr[4.000000000000001e-07]
Step[46500] | Loss[0.09779049456119537] | Lr[4.000000000000001e-07]
Step[47000] | Loss[0.11317891627550125] | Lr[4.000000000000001e-07]
Step[47000] | Loss[0.10549259185791016] | Lr[4.000000000000001e-07]
Step[47000] | Loss[0.07804656028747559] | Lr[4.000000000000001e-07]
Step[47000] | Loss[0.10546752065420151] | Lr[4.000000000000001e-07]
Step[47500] | Loss[0.09381645917892456] | Lr[4.000000000000001e-07]
Step[47500] | Loss[0.08983585238456726] | Lr[4.000000000000001e-07]
Step[47500] | Loss[0.10930398106575012] | Lr[4.000000000000001e-07]
Step[47500] | Loss[0.1328633427619934] | Lr[4.000000000000001e-07]
Step[48000] | Loss[0.09764422476291656] | Lr[4.000000000000001e-07]
Step[48000] | Loss[0.14840787649154663] | Lr[4.000000000000001e-07]
Step[48000] | Loss[0.09373465180397034] | Lr[4.000000000000001e-07]
Step[48000] | Loss[0.14471493661403656] | Lr[4.000000000000001e-07]
Step[48500] | Loss[0.10943485796451569] | Lr[4.000000000000001e-07]
Step[48500] | Loss[0.15627482533454895] | Lr[4.000000000000001e-07]
Step[48500] | Loss[0.13668417930603027] | Lr[4.000000000000001e-07]
Step[48500] | Loss[0.1131996139883995] | Lr[4.000000000000001e-07]
Step[49000] | Loss[0.15632450580596924] | Lr[4.000000000000001e-07]
Step[49000] | Loss[0.15249107778072357] | Lr[4.000000000000001e-07]
Step[49000] | Loss[0.12484399974346161] | Lr[4.000000000000001e-07]
Step[49000] | Loss[0.12074099481105804] | Lr[4.000000000000001e-07]
Step[49500] | Loss[0.1289021074771881] | Lr[4.000000000000001e-07]
Step[49500] | Loss[0.12507647275924683] | Lr[4.000000000000001e-07]
Step[49500] | Loss[0.16011838614940643] | Lr[4.000000000000001e-07]
Step[49500] | Loss[0.10150317847728729] | Lr[4.000000000000001e-07]
Step[50000] | Loss[0.13280797004699707] | Lr[4.000000000000001e-07]
Step[50000] | Loss[0.14442327618598938] | Lr[4.000000000000001e-07]
Step[50000] | Loss[0.16410937905311584] | Lr[4.000000000000001e-07]
Step[50000] | Loss[0.15247005224227905] | Lr[4.000000000000001e-07]
Step[50500] | Loss[0.08587217330932617] | Lr[4.000000000000001e-07]
Step[50500] | Loss[0.10154083371162415] | Lr[4.000000000000001e-07]
Step[50500] | Loss[0.14067675173282623] | Lr[4.000000000000001e-07]
Step[50500] | Loss[0.16034629940986633] | Lr[4.000000000000001e-07]
Step[51000] | Loss[0.12888626754283905] | Lr[4.000000000000001e-07]
Step[51000] | Loss[0.16404283046722412] | Lr[4.000000000000001e-07]
Step[51000] | Loss[0.1444241851568222] | Lr[4.000000000000001e-07]
Step[51000] | Loss[0.10153891146183014] | Lr[4.000000000000001e-07]
Step[51500] | Loss[0.1287759393453598] | Lr[4.000000000000001e-07]
Step[51500] | Loss[0.10162846744060516] | Lr[4.000000000000001e-07]
Step[51500] | Loss[0.10540247708559036] | Lr[4.000000000000001e-07]
Step[51500] | Loss[0.10948169231414795] | Lr[4.000000000000001e-07]
Step[52000] | Loss[0.1172504872083664] | Lr[4.000000000000001e-07]
Step[52000] | Loss[0.07037496566772461] | Lr[4.000000000000001e-07]
Step[52000] | Loss[0.11721067130565643] | Lr[4.000000000000001e-07]
Step[52000] | Loss[0.1290343552827835] | Lr[4.000000000000001e-07]
Step[52500] | Loss[0.10544659942388535] | Lr[4.000000000000001e-07]
Step[52500] | Loss[0.15617792308330536] | Lr[4.000000000000001e-07]
Step[52500] | Loss[0.12886002659797668] | Lr[4.000000000000001e-07]
Step[52500] | Loss[0.14844977855682373] | Lr[4.000000000000001e-07]
Step[53000] | Loss[0.11723126471042633] | Lr[4.000000000000001e-07]
Step[53000] | Loss[0.1327708661556244] | Lr[4.000000000000001e-07]
Step[53000] | Loss[0.10145188868045807] | Lr[4.000000000000001e-07]
Step[53000] | Loss[0.08591781556606293] | Lr[4.000000000000001e-07]
Step[53500] | Loss[0.10153824836015701] | Lr[4.000000000000001e-07]
Step[53500] | Loss[0.17584604024887085] | Lr[4.000000000000001e-07]
Step[53500] | Loss[0.13678187131881714] | Lr[4.000000000000001e-07]
Step[53500] | Loss[0.1483170986175537] | Lr[4.000000000000001e-07]
Step[54000] | Loss[0.14854103326797485] | Lr[4.000000000000001e-07]
Step[54000] | Loss[0.10937555134296417] | Lr[4.000000000000001e-07]
Step[54000] | Loss[0.1367463767528534] | Lr[4.000000000000001e-07]
Step[54000] | Loss[0.1444505900144577] | Lr[4.000000000000001e-07]
Step[54500] | Loss[0.11326552927494049] | Lr[4.000000000000001e-07]
Step[54500] | Loss[0.10156621038913727] | Lr[4.000000000000001e-07]
Step[54500] | Loss[0.15617798268795013] | Lr[4.000000000000001e-07]
Step[54500] | Loss[0.1326930820941925] | Lr[4.000000000000001e-07]
Step[55000] | Loss[0.1250430941581726] | Lr[4.000000000000001e-07]
Step[55000] | Loss[0.12101074308156967] | Lr[4.000000000000001e-07]
Step[55000] | Loss[0.17190232872962952] | Lr[4.000000000000001e-07]
Step[55000] | Loss[0.1366863250732422] | Lr[4.000000000000001e-07]
Step[55500] | Loss[0.0976751297712326] | Lr[4.000000000000001e-07]
Step[55500] | Loss[0.09379714727401733] | Lr[4.000000000000001e-07]Step[55500] | Loss[0.13280101120471954] | Lr[4.000000000000001e-07]

Step[55500] | Loss[0.10159379243850708] | Lr[4.000000000000001e-07]
Step[56000] | Loss[0.07796283066272736] | Lr[4.000000000000001e-07]
Step[56000] | Loss[0.0938154011964798] | Lr[4.000000000000001e-07]
Step[56000] | Loss[0.12888604402542114] | Lr[4.000000000000001e-07]
Step[56000] | Loss[0.16014009714126587] | Lr[4.000000000000001e-07]
Step[56500] | Loss[0.13669565320014954] | Lr[4.000000000000001e-07]
Step[56500] | Loss[0.09372898191213608] | Lr[4.000000000000001e-07]
Step[56500] | Loss[0.12117889523506165] | Lr[4.000000000000001e-07]
Step[56500] | Loss[0.1054053008556366] | Lr[4.000000000000001e-07]
Step[57000] | Loss[0.13663235306739807] | Lr[4.000000000000001e-07]
Step[57000] | Loss[0.12496189773082733] | Lr[4.000000000000001e-07]
Step[57000] | Loss[0.14072185754776] | Lr[4.000000000000001e-07]
Step[57000] | Loss[0.1368570476770401] | Lr[4.000000000000001e-07]
Step[57500] | Loss[0.13276991248130798] | Lr[4.000000000000001e-07]
Step[57500] | Loss[0.10930071771144867] | Lr[4.000000000000001e-07]
Step[57500] | Loss[0.14056341350078583] | Lr[4.000000000000001e-07]
Step[57500] | Loss[0.14833694696426392] | Lr[4.000000000000001e-07]
Step[58000] | Loss[0.10546334087848663] | Lr[4.000000000000001e-07]
Step[58000] | Loss[0.11309857666492462] | Lr[4.000000000000001e-07]
Step[58000] | Loss[0.11321363598108292] | Lr[4.000000000000001e-07]
Step[58000] | Loss[0.09772968292236328] | Lr[4.000000000000001e-07]
Step[58500] | Loss[0.13674984872341156] | Lr[4.000000000000001e-07]
Step[58500] | Loss[0.12105528265237808] | Lr[4.000000000000001e-07]
Step[58500] | Loss[0.0975525975227356] | Lr[4.000000000000001e-07]
Step[58500] | Loss[0.15224304795265198] | Lr[4.000000000000001e-07]
Step[59000] | Loss[0.12900717556476593] | Lr[4.000000000000001e-07]
Step[59000] | Loss[0.12509101629257202] | Lr[4.000000000000001e-07]
Step[59000] | Loss[0.18372482061386108] | Lr[4.000000000000001e-07]
Step[59000] | Loss[0.14853331446647644] | Lr[4.000000000000001e-07]
Step[59500] | Loss[0.0702924132347107] | Lr[4.000000000000001e-07]
Step[59500] | Loss[0.1287466585636139] | Lr[4.000000000000001e-07]
Step[59500] | Loss[0.09378011524677277] | Lr[4.000000000000001e-07]
Step[59500] | Loss[0.163859024643898] | Lr[4.000000000000001e-07]
Step[60000] | Loss[0.1289806365966797] | Lr[4.000000000000001e-07]
Step[60000] | Loss[0.13274523615837097] | Lr[4.000000000000001e-07]
Step[60000] | Loss[0.14064115285873413] | Lr[4.000000000000001e-07]
Step[60000] | Loss[0.14048771560192108] | Lr[4.000000000000001e-07]
Step[60500] | Loss[0.1210656389594078] | Lr[4.000000000000001e-07]
Step[60500] | Loss[0.07024481892585754] | Lr[4.000000000000001e-07]
Step[60500] | Loss[0.09770762920379639] | Lr[4.000000000000001e-07]
Step[60500] | Loss[0.11320602148771286] | Lr[4.000000000000001e-07]
Step[61000] | Loss[0.10555508732795715] | Lr[4.000000000000001e-07]
Step[61000] | Loss[0.1562807857990265] | Lr[4.000000000000001e-07]
Step[61000] | Loss[0.09773065149784088] | Lr[4.000000000000001e-07]
Step[61000] | Loss[0.10935647040605545] | Lr[4.000000000000001e-07]
Step[61500] | Loss[0.11710244417190552] | Lr[4.000000000000001e-07]
Step[61500] | Loss[0.06657785177230835] | Lr[4.000000000000001e-07]
Step[61500] | Loss[0.1524202823638916] | Lr[4.000000000000001e-07]
Step[61500] | Loss[0.11317487061023712] | Lr[4.000000000000001e-07]
Step[62000] | Loss[0.08213402330875397] | Lr[4.000000000000001e-07]
Step[62000] | Loss[0.11700891703367233] | Lr[4.000000000000001e-07]
Step[62000] | Loss[0.1328166425228119] | Lr[4.000000000000001e-07]
Step[62000] | Loss[0.07818616926670074] | Lr[4.000000000000001e-07]
Step[62500] | Loss[0.121085986495018] | Lr[4.000000000000001e-07]
Step[62500] | Loss[0.13679027557373047] | Lr[4.000000000000001e-07]
Step[62500] | Loss[0.13277886807918549] | Lr[4.000000000000001e-07]
Step[62500] | Loss[0.12109756469726562] | Lr[4.000000000000001e-07]
Step[63000] | Loss[0.1014549508690834] | Lr[4.000000000000001e-07]
Step[63000] | Loss[0.148375004529953] | Lr[4.000000000000001e-07]
Step[63000] | Loss[0.13296130299568176] | Lr[4.000000000000001e-07]
Step[63000] | Loss[0.08982186019420624] | Lr[4.000000000000001e-07]
Step[63500] | Loss[0.136756032705307] | Lr[4.000000000000001e-07]
Step[63500] | Loss[0.1368250995874405] | Lr[4.000000000000001e-07]
Step[63500] | Loss[0.1014861911535263] | Lr[4.000000000000001e-07]
Step[63500] | Loss[0.13661286234855652] | Lr[4.000000000000001e-07]
Step[64000] | Loss[0.1210884153842926] | Lr[4.000000000000001e-07]
Step[64000] | Loss[0.1560666263103485] | Lr[4.000000000000001e-07]
Step[64000] | Loss[0.16016405820846558] | Lr[4.000000000000001e-07]
Step[64000] | Loss[0.1369020938873291] | Lr[4.000000000000001e-07]
Step[64500] | Loss[0.09383630752563477] | Lr[4.000000000000001e-07]
Step[64500] | Loss[0.17193718254566193] | Lr[4.000000000000001e-07]
Step[64500] | Loss[0.10148017108440399] | Lr[4.000000000000001e-07]
Step[64500] | Loss[0.1717883050441742] | Lr[4.000000000000001e-07]
Step[65000] | Loss[0.12884977459907532] | Lr[4.000000000000001e-07]
Step[65000] | Loss[0.09378855675458908] | Lr[4.000000000000001e-07]
Step[65000] | Loss[0.12503761053085327] | Lr[4.000000000000001e-07]
Step[65000] | Loss[0.11327389627695084] | Lr[4.000000000000001e-07]
Step[65500] | Loss[0.10544934868812561] | Lr[4.000000000000001e-07]
Step[65500] | Loss[0.1483599841594696] | Lr[4.000000000000001e-07]
Step[65500] | Loss[0.1483648419380188] | Lr[4.000000000000001e-07]
Step[65500] | Loss[0.12494849413633347] | Lr[4.000000000000001e-07]
Step[66000] | Loss[0.10154753923416138] | Lr[4.000000000000001e-07]
Step[66000] | Loss[0.1678088754415512] | Lr[4.000000000000001e-07]
Step[66000] | Loss[0.17566776275634766] | Lr[4.000000000000001e-07]
Step[66000] | Loss[0.13274890184402466] | Lr[4.000000000000001e-07]
Step[66500] | Loss[0.12890131771564484] | Lr[4.000000000000001e-07]
Step[66500] | Loss[0.0936916396021843] | Lr[4.000000000000001e-07]
Step[66500] | Loss[0.08202249556779861] | Lr[4.000000000000001e-07]
Step[66500] | Loss[0.11329206079244614] | Lr[4.000000000000001e-07]
Step[67000] | Loss[0.14053064584732056] | Lr[4.000000000000001e-07]
Step[67000] | Loss[0.13279399275779724] | Lr[4.000000000000001e-07]
Step[67000] | Loss[0.14059284329414368] | Lr[4.000000000000001e-07]
Step[67000] | Loss[0.1015978455543518] | Lr[4.000000000000001e-07]
Step[67500] | Loss[0.10557933151721954] | Lr[4.000000000000001e-07]
Step[67500] | Loss[0.11336786299943924] | Lr[4.000000000000001e-07]
Step[67500] | Loss[0.12108718603849411] | Lr[4.000000000000001e-07]
Step[67500] | Loss[0.18750788271427155] | Lr[4.000000000000001e-07]
Step[68000] | Loss[0.12104440480470657] | Lr[4.000000000000001e-07]
Step[68000] | Loss[0.1405898928642273] | Lr[4.000000000000001e-07]
Step[68000] | Loss[0.13671952486038208] | Lr[4.000000000000001e-07]
Step[68000] | Loss[0.08199476450681686] | Lr[4.000000000000001e-07]
Step[68500] | Loss[0.08987410366535187] | Lr[4.000000000000001e-07]
Step[68500] | Loss[0.14460599422454834] | Lr[4.000000000000001e-07]
Step[68500] | Loss[0.11326602101325989] | Lr[4.000000000000001e-07]
Step[68500] | Loss[0.11330273747444153] | Lr[4.000000000000001e-07]
Step[69000] | Loss[0.09371034055948257] | Lr[4.000000000000001e-07]
Step[69000] | Loss[0.05855347216129303] | Lr[4.000000000000001e-07]
Step[69000] | Loss[0.15234318375587463] | Lr[4.000000000000001e-07]
Step[69000] | Loss[0.17958517372608185] | Lr[4.000000000000001e-07]
Step[69500] | Loss[0.12888334691524506] | Lr[4.000000000000001e-07]
Step[69500] | Loss[0.12885837256908417] | Lr[4.000000000000001e-07]
Step[69500] | Loss[0.12506669759750366] | Lr[4.000000000000001e-07]
Step[69500] | Loss[0.12886905670166016] | Lr[4.000000000000001e-07]
Step[70000] | Loss[0.125014528632164] | Lr[4.000000000000001e-07]
Step[70000] | Loss[0.10152019560337067] | Lr[4.000000000000001e-07]
Step[70000] | Loss[0.1601843237876892] | Lr[4.000000000000001e-07]
Step[70000] | Loss[0.08977015316486359] | Lr[4.000000000000001e-07]
Step[70500] | Loss[0.14061051607131958] | Lr[4.000000000000001e-07]
Step[70500] | Loss[0.15225845575332642] | Lr[4.000000000000001e-07]
Step[70500] | Loss[0.1133827269077301] | Lr[4.000000000000001e-07]
Step[70500] | Loss[0.1640583872795105] | Lr[4.000000000000001e-07]
Step[71000] | Loss[0.1093778908252716] | Lr[4.000000000000001e-07]
Step[71000] | Loss[0.14458253979682922] | Lr[4.000000000000001e-07]
Step[71000] | Loss[0.1524111032485962] | Lr[4.000000000000001e-07]
Step[71000] | Loss[0.13276493549346924] | Lr[4.000000000000001e-07]
Step[71500] | Loss[0.0741136223077774] | Lr[4.000000000000001e-07]
Step[71500] | Loss[0.14061211049556732] | Lr[4.000000000000001e-07]
Step[71500] | Loss[0.14054235816001892] | Lr[4.000000000000001e-07]
Step[71500] | Loss[0.10539522767066956] | Lr[4.000000000000001e-07]
Step[72000] | Loss[0.10932177305221558] | Lr[4.000000000000001e-07]
Step[72000] | Loss[0.12497417628765106] | Lr[4.000000000000001e-07]
Step[72000] | Loss[0.17201387882232666] | Lr[4.000000000000001e-07]
Step[72000] | Loss[0.14059698581695557] | Lr[4.000000000000001e-07]
Step[72500] | Loss[0.136577308177948] | Lr[4.000000000000001e-07]
Step[72500] | Loss[0.14460670948028564] | Lr[4.000000000000001e-07]
Step[72500] | Loss[0.11328037828207016] | Lr[4.000000000000001e-07]
Step[72500] | Loss[0.12111842632293701] | Lr[4.000000000000001e-07]
Step[73000] | Loss[0.08202424645423889] | Lr[4.000000000000001e-07]
Step[73000] | Loss[0.15228524804115295] | Lr[4.000000000000001e-07]
Step[73000] | Loss[0.07807434350252151] | Lr[4.000000000000001e-07]
Step[73000] | Loss[0.12497095763683319] | Lr[4.000000000000001e-07]
Step[73500] | Loss[0.10946731269359589] | Lr[4.000000000000001e-07]
Step[73500] | Loss[0.14060315489768982] | Lr[4.000000000000001e-07]
Step[73500] | Loss[0.12885470688343048] | Lr[4.000000000000001e-07]
Step[73500] | Loss[0.1250225007534027] | Lr[4.000000000000001e-07]
Step[74000] | Loss[0.12112249433994293] | Lr[4.000000000000001e-07]
Step[74000] | Loss[0.10149747133255005] | Lr[4.000000000000001e-07]
Step[74000] | Loss[0.12514960765838623] | Lr[4.000000000000001e-07]
Step[74000] | Loss[0.066490538418293] | Lr[4.000000000000001e-07]
Step[74500] | Loss[0.1132495179772377] | Lr[4.000000000000001e-07]
Step[74500] | Loss[0.12882578372955322] | Lr[4.000000000000001e-07]
Step[74500] | Loss[0.10548604279756546] | Lr[4.000000000000001e-07]
Step[74500] | Loss[0.1484142392873764] | Lr[4.000000000000001e-07]
Step[75000] | Loss[0.15625497698783875] | Lr[4.000000000000001e-07]
Step[75000] | Loss[0.1054837629199028] | Lr[4.000000000000001e-07]
Step[75000] | Loss[0.13677667081356049] | Lr[4.000000000000001e-07]
Step[75000] | Loss[0.17580704391002655] | Lr[4.000000000000001e-07]
Step[75500] | Loss[0.06642204523086548] | Lr[4.000000000000001e-07]
Step[75500] | Loss[0.12879696488380432] | Lr[4.000000000000001e-07]
Step[75500] | Loss[0.11327061802148819] | Lr[4.000000000000001e-07]
Step[75500] | Loss[0.1484142243862152] | Lr[4.000000000000001e-07]
Step[76000] | Loss[0.17168831825256348] | Lr[4.000000000000001e-07]
Step[76000] | Loss[0.14846733212471008] | Lr[4.000000000000001e-07]
Step[76000] | Loss[0.10938729345798492] | Lr[4.000000000000001e-07]
Step[76000] | Loss[0.15229783952236176] | Lr[4.000000000000001e-07]
Step[76500] | Loss[0.1014975756406784] | Lr[4.000000000000001e-07]
Step[76500] | Loss[0.1680850088596344] | Lr[4.000000000000001e-07]
Step[76500] | Loss[0.14836636185646057] | Lr[4.000000000000001e-07]
Step[76500] | Loss[0.07816094160079956] | Lr[4.000000000000001e-07]
Step[77000] | Loss[0.09763443470001221] | Lr[4.000000000000001e-07]
Step[77000] | Loss[0.14064161479473114] | Lr[4.000000000000001e-07]
Step[77000] | Loss[0.14844724535942078] | Lr[4.000000000000001e-07]
Step[77000] | Loss[0.10933676362037659] | Lr[4.000000000000001e-07]
Step[77500] | Loss[0.10155455768108368] | Lr[4.000000000000001e-07]
Step[77500] | Loss[0.06634187698364258] | Lr[4.000000000000001e-07]
Step[77500] | Loss[0.13662958145141602] | Lr[4.000000000000001e-07]
Step[77500] | Loss[0.13284115493297577] | Lr[4.000000000000001e-07]
Step[78000] | Loss[0.12885209918022156] | Lr[4.000000000000001e-07]
Step[78000] | Loss[0.1483878195285797] | Lr[4.000000000000001e-07]
Step[78000] | Loss[0.1132332906126976] | Lr[4.000000000000001e-07]
Step[78000] | Loss[0.15616124868392944] | Lr[4.000000000000001e-07]
Labels:  tensor([0.5000, 0.5000, 0.2500, 0.2500, 0.2500, 1.0000, 1.0000, 1.0000, 0.7500,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 0.0000, 0.7500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Labels:  Labels:  tensor([0.7500, 0.0000, 0.0000, 0.2500, 0.0000, 0.7500, 0.0000, 0.0000, 0.7500,
        0.2500, 1.0000, 1.0000, 1.0000, 0.2500, 0.5000, 0.0000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
------------------------
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
Labels:  tensor([0.0000, 1.0000, 0.5000, 0.7500, 0.0000, 0.2500, 0.5000, 0.5000, 0.7500,
        0.5000, 0.5000, 1.0000, 1.0000, 0.2500, 0.2500, 0.5000],
------------------------
       device='cuda:1')
tensor([0.7500, 0.2500, 0.2500, 1.0000, 0.0000, 0.5000, 1.0000, 0.2500, 0.7500,
        0.0000, 0.5000, 0.7500, 0.5000, 0.5000, 1.0000, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
------------------------
Labels:  tensor([0.0000, 0.2500, 1.0000, 0.5000, 0.7500, 0.5000, 0.2500, 0.0000, 0.5000,
        0.5000, 0.5000, 0.0000, 0.5000, 1.0000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.2500, 0.7500, 1.0000, 0.7500, 0.0000, 0.7500,
        0.2500, 0.2500, 0.0000, 0.0000, 0.7500, 0.5000, 0.0000],
       device='cuda:1')
Labels:  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.2500, 0.0000, 0.0000, 1.0000,
        1.0000, 0.5000, 0.2500, 0.7500, 0.0000, 0.0000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
------------------------
Labels:  tensor([0.5000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.5000, 1.0000,
        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.7500, 0.7500, 0.7500, 0.5000, 0.2500, 0.7500, 0.2500, 0.2500, 0.5000,
        0.5000, 0.0000, 0.7500, 0.7500, 1.0000, 1.0000, 0.5000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.7500, 0.5000, 0.7500, 0.5000, 0.7500, 0.0000, 0.5000, 1.0000, 0.2500,
        0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
       device='cuda:1')
Labels:  tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2500, 0.2500, 0.7500, 0.0000,
        1.0000, 1.0000, 0.7500, 0.0000, 0.5000, 0.7500, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
------------------------
Labels:  tensor([0.5000, 0.7500, 1.0000, 0.0000, 0.2500, 0.7500, 1.0000, 0.7500, 1.0000,
        0.5000, 0.2500, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.5000, 1.0000, 0.5000,
        1.0000, 1.0000, 0.2500, 0.0000, 0.2500, 0.5000, 0.2500],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 0.2500, 0.2500, 1.0000, 0.2500, 1.0000, 1.0000,
        0.2500, 0.7500, 0.5000, 1.0000, 0.7500, 0.7500, 1.0000],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.5000, 0.0000, 0.2500, 0.7500, 1.0000, 0.5000, 0.0000, 0.0000, 0.5000,
        0.2500, 0.0000, 0.2500, 0.2500, 0.5000, 0.7500, 1.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Labels:  tensor([1.0000, 1.0000, 1.0000, 0.5000, 0.7500, 0.5000, 1.0000, 0.2500, 0.5000,
        0.2500, 0.2500, 0.5000, 0.0000, 0.2500, 0.0000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([0.2500, 0.2500, 0.5000, 1.0000, 0.5000, 1.0000, 0.0000, 0.5000, 0.7500,
        0.5000, 0.0000, 0.2500, 0.2500, 0.7500, 0.5000, 0.0000],
       device='cuda:0')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Mean loss[0.1255596315732754] | Mean r^2[-0.07335987482029034]
Stupid loss[0.1255604868228404] | Stupid r^2[-0.07336748809916849]
EPOCH 4
--------------
Labels:  tensor([0.0000, 1.0000, 0.0000, 0.7500, 0.0000, 0.2500, 0.5000, 0.2500, 1.0000,
        0.5000, 0.2500, 0.2500, 1.0000, 0.7500, 0.2500, 0.7500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Labels:  tensor([1.0000, 0.5000, 0.5000, 0.0000, 0.0000, 0.7500, 0.7500, 0.0000, 0.0000,
        0.0000, 0.0000, 0.2500, 1.0000, 0.5000, 0.0000, 1.0000],
Mean loss[0.12501527175394542] | Mean r^2[-0.077325422627685]
       device='cuda:0')
Stupid loss[0.1250152513421181] | Stupid r^2[-0.07732605078908046]
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
EPOCH 4
--------------
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:0')
------------------------
Mean loss[0.12420287277965851] | Mean r^2[-0.08124184148843373]
Stupid loss[0.12420311737432894] | Stupid r^2[-0.08124547332830695]
EPOCH 4
--------------
Labels:  tensor([1.0000, 1.0000, 0.5000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2500, 1.0000,
        0.5000, 0.0000, 0.2500, 0.2500, 0.5000, 0.5000, 0.2500],
       device='cuda:1')
Stupid:  tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:1')
Outputs:  tensor([0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,
        0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999],
       device='cuda:1')
------------------------
Mean loss[0.1256157465791807] | Mean r^2[-0.0734429138936666]
Stupid loss[0.12561577293801854] | Stupid r^2[-0.07344347893057246]
EPOCH 4
--------------
Step[500] | Loss[0.12500935792922974] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.1249685287475586] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.14444661140441895] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.1367330104112625] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.12505820393562317] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.1250232756137848] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.15618541836738586] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.12892544269561768] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.08595875650644302] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.1602008044719696] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.16796180605888367] | Lr[8.000000000000003e-08]Step[1500] | Loss[0.08205892145633698] | Lr[8.000000000000003e-08]

Step[2000] | Loss[0.07815861701965332] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.0898270457983017] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.10544732213020325] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.16022026538848877] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.13666382431983948] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.14851823449134827] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.14451952278614044] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.12511307001113892] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.14060688018798828] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.15626998245716095] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.12504811584949493] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.128998801112175] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.1445261836051941] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.15580664575099945] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.10935650765895844] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.16416187584400177] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.16011467576026917] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.12103457003831863] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.09375829249620438] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.1524307131767273] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.1172303855419159] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.15236896276474] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.16408579051494598] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.18359041213989258] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.11724229902029037] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.14842598140239716] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.08202384412288666] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.1484515368938446] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.1523461490869522] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.11323726177215576] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.11332106590270996] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.16012199223041534] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.14056119322776794] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.10161106288433075] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.13273334503173828] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.07428990304470062] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.13276180624961853] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.09771465510129929] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.1718544065952301] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.16395486891269684] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.17196297645568848] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.1641397923231125] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.11327895522117615] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.11335951089859009] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.1290278136730194] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.13668033480644226] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.14055825769901276] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.15232455730438232] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.15629786252975464] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.11732014268636703] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.14453470706939697] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.14456278085708618] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.13277190923690796] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.09762944281101227] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.1326766163110733] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.14057934284210205] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.1171664297580719] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.09368896484375] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.16408398747444153] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.12119533121585846] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.17190805077552795] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.11719319969415665] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.09765201807022095] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.1757510006427765] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.08986997604370117] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.16793963313102722] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.09377443790435791] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.14458268880844116] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.17183423042297363] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.11334428936243057] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.10164057463407516] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.14052826166152954] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.10164514183998108] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.10156874358654022] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.11721348762512207] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.093777596950531] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.15623576939105988] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.1132323294878006] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.11327410489320755] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.12495782971382141] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.17193734645843506] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.050804540514945984] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.11716091632843018] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.07804787158966064] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.10154008120298386] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.0977177619934082] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.12099932134151459] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.08993566036224365] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.12118510901927948] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.12495911121368408] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.17589305341243744] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.1524263173341751] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.10930962860584259] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.10543359071016312] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.1601828634738922] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.0780986100435257] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.14456456899642944] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.09377548098564148] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.12504330277442932] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.1366865038871765] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.15226411819458008] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.11726266145706177] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.16792502999305725] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.08987186849117279] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.11731725931167603] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.128919780254364] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.08988862484693527] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.16015902161598206] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.08975382894277573] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.11724499613046646] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.10152250528335571] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.19137300550937653] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.1367553323507309] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.12888070940971375] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.15247002243995667] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.1288154572248459] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.09372072666883469] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.1328815370798111] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.16401034593582153] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.13668659329414368] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.09763532131910324] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.17581358551979065] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.12106244266033173] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.10935991257429123] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.10154187679290771] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.1602020561695099] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.1171707883477211] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.11336345970630646] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.15225443243980408] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.17192208766937256] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.08984372019767761] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.15241947770118713] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.1914770007133484] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.16801203787326813] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.05858991667628288] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.11336696892976761] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.10933427512645721] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.12114622443914413] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.0975777804851532] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.10157434642314911] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.12884804606437683] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.09373687207698822] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.16410048305988312] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.13276726007461548] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.16406171023845673] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.14847005903720856] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.12098340690135956] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.10160262882709503] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.14451880753040314] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.08987075090408325] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.1327734887599945] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.09766273945569992] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.04298700392246246] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.12497563660144806] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.15235750377178192] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.1367407888174057] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.08982101082801819] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.10933482646942139] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.15230830013751984] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.1640733927488327] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.13663478195667267] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.14843028783798218] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.13282956182956696] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.12113895267248154] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.18364427983760834] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.14058023691177368] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.14067894220352173] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.11718957126140594] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.10926714539527893] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.13278824090957642] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.1288936734199524] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.12110161781311035] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.16792111098766327] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.12499383091926575] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.08199745416641235] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.10942177474498749] | Lr[8.000000000000003e-08]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1633673 closing signal SIGTERM
slurmstepd: error: *** JOB 49925 ON gpu002 CANCELLED AT 2023-10-21T04:18:21 ***
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1633674 closing signal SIGTERM
slurmstepd: error: *** STEP 49925.1 ON gpu002 CANCELLED AT 2023-10-21T04:18:21 ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 236248 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 236249 closing signal SIGTERM
