Node IP: 10.128.2.152
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 7062
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.152:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 7062
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.152:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_3ot12t9q/7062_2p8qohxb
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_pdb0zj11/7062_blz6ds_5
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu002.hpc
  master_port=36375
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu002.hpc
  master_port=36375
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_3ot12t9q/7062_2p8qohxb/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_3ot12t9q/7062_2p8qohxb/attempt_0/1/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_pdb0zj11/7062_blz6ds_5/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_pdb0zj11/7062_blz6ds_5/attempt_0/1/error.json
/u/dssc/msanna00/.conda/envs/deeplearning3/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/u/dssc/msanna00/.conda/envs/deeplearning3/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
PORT:  36375
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  1
My rank is:  3
PORT:  36375
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  1
My rank is:  2
PORT:  36375
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  0
My rank is:  1
PORT:  36375
WORLD SIZE:  4
MASTER NODE:  gpu002.hpc
My slurm id is:  0
My rank is:  0
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
------------------------

------------------------

------------------------

------------------------

Loading checkpoint...
Loading checkpoint...
Loading checkpoint...
Loading checkpoint...
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading scheduler state...Loading model state...

Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 0 using GPU 0
LOADED!
I'm process 2 using GPU 0
LOADED!
I'm process 3 using GPU 1
LOADED!
I'm process 1 using GPU 1
Labels:  tensor([3, 0, 0, 1, 0, 3, 0, 0, 3, 1, 4, 4, 4, 1, 2, 0], device='cuda:1')
Preds:  tensor([4, 1, 2, 1, 0, 2, 0, 0, 3, 1, 4, 2, 4, 1, 2, 0], device='cuda:1')
Outputs:  tensor([[    0.0004,     0.0005,     0.0126,     0.3458,     0.6407],
        [    0.3993,     0.5017,     0.0973,     0.0013,     0.0004],
        [    0.1580,     0.3803,     0.4316,     0.0267,     0.0034],
        [    0.4246,     0.4652,     0.1074,     0.0023,     0.0004],
        [    0.8380,     0.1468,     0.0149,     0.0002,     0.0000],
        [    0.0024,     0.0307,     0.7900,     0.1726,     0.0043],
        [    0.9472,     0.0507,     0.0019,     0.0001,     0.0000],
        [    0.5402,     0.4353,     0.0235,     0.0008,     0.0002],
        [    0.0002,     0.0039,     0.3722,     0.5524,     0.0713],
        [    0.2256,     0.4504,     0.3024,     0.0185,     0.0031],
        [    0.1352,     0.1258,     0.2600,     0.1785,     0.3004],
        [    0.0744,     0.0683,     0.4273,     0.3457,     0.0843],
        [    0.0003,     0.0003,     0.0025,     0.0616,     0.9354],
        [    0.1955,     0.6190,     0.1595,     0.0232,     0.0029],
        [    0.0023,     0.1053,     0.8850,     0.0072,     0.0002],
        [    0.9723,     0.0271,     0.0004,     0.0001,     0.0001]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([2, 2, 1, 1, 1, 4, 4, 4, 3, 2, 0, 3, 3, 4, 0, 3], device='cuda:0')
Preds:  tensor([4, 3, 3, 1, 0, 4, 4, 4, 2, 2, 1, 2, 3, 4, 0, 3], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0003,     0.0147,     0.4502,     0.5347],
        [    0.0006,     0.0071,     0.3310,     0.6543,     0.0070],
        [    0.0095,     0.0575,     0.4123,     0.4229,     0.0979],
        [    0.1188,     0.4651,     0.3927,     0.0225,     0.0009],
        [    0.6671,     0.3169,     0.0157,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0014,     0.0693,     0.9290],
        [    0.0006,     0.0003,     0.0013,     0.0385,     0.9593],
        [    0.0003,     0.0004,     0.0040,     0.0887,     0.9067],
        [    0.0021,     0.0275,     0.8544,     0.1052,     0.0108],
        [    0.0048,     0.0527,     0.5615,     0.3730,     0.0079],
        [    0.0372,     0.4635,     0.4530,     0.0420,     0.0042],
        [    0.0059,     0.0763,     0.6955,     0.1889,     0.0334],
        [    0.0005,     0.0043,     0.1431,     0.6145,     0.2376],
        [    0.0002,     0.0017,     0.0038,     0.2130,     0.7812],
        [    0.9334,     0.0648,     0.0017,     0.0000,     0.0000],
        [    0.0001,     0.0002,     0.0102,     0.9008,     0.0888]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 1, 1, 4, 0, 2, 4, 1, 3, 0, 2, 3, 2, 2, 4, 4], device='cuda:0')
Preds:  tensor([3, 1, 0, 4, 0, 2, 4, 0, 3, 0, 1, 4, 1, 2, 3, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0006,     0.0219,     0.9768,     0.0006],
        [    0.4283,     0.4659,     0.1044,     0.0012,     0.0003],
        [    0.6890,     0.2604,     0.0470,     0.0029,     0.0007],
        [    0.0022,     0.0008,     0.0041,     0.0506,     0.9424],
        [    0.8665,     0.0815,     0.0340,     0.0099,     0.0081],
        [    0.0023,     0.1059,     0.7207,     0.1665,     0.0046],
        [    0.0028,     0.0017,     0.0064,     0.0628,     0.9263],
        [    0.5185,     0.4471,     0.0341,     0.0002,     0.0001],
        [    0.0001,     0.0020,     0.3577,     0.6265,     0.0137],
        [    0.6478,     0.2491,     0.0990,     0.0039,     0.0001],
        [    0.3426,     0.5186,     0.1361,     0.0023,     0.0005],
        [    0.0007,     0.0007,     0.0151,     0.4000,     0.5835],
        [    0.0321,     0.6225,     0.3429,     0.0025,     0.0000],
        [    0.0098,     0.1032,     0.6334,     0.2378,     0.0159],
        [    0.0001,     0.0003,     0.0148,     0.6624,     0.3224],
        [    0.0012,     0.0006,     0.0061,     0.2094,     0.7827]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([0, 4, 2, 3, 0, 1, 2, 2, 3, 2, 2, 4, 4, 1, 1, 2], device='cuda:1')
Preds:  tensor([0, 4, 1, 4, 0, 1, 2, 3, 2, 2, 0, 4, 4, 2, 1, 4], device='cuda:1')
Outputs:  tensor([[    0.7216,     0.2738,     0.0046,     0.0001,     0.0000],
        [    0.0002,     0.0002,     0.0012,     0.0905,     0.9079],
        [    0.0464,     0.6978,     0.2153,     0.0341,     0.0063],
        [    0.0001,     0.0000,     0.0001,     0.0109,     0.9888],
        [    0.9835,     0.0148,     0.0013,     0.0002,     0.0002],
        [    0.0692,     0.4362,     0.4086,     0.0813,     0.0047],
        [    0.0159,     0.1418,     0.3478,     0.3316,     0.1629],
        [    0.0004,     0.0053,     0.3879,     0.5791,     0.0273],
        [    0.0967,     0.3906,     0.4750,     0.0371,     0.0006],
        [    0.0060,     0.0800,     0.6931,     0.2150,     0.0059],
        [    0.7532,     0.2155,     0.0294,     0.0013,     0.0006],
        [    0.0001,     0.0001,     0.0007,     0.0352,     0.9639],
        [    0.0006,     0.0004,     0.0010,     0.0052,     0.9927],
        [    0.0215,     0.3999,     0.4731,     0.0960,     0.0095],
        [    0.0052,     0.9897,     0.0045,     0.0004,     0.0002],
        [    0.0123,     0.0049,     0.0296,     0.1749,     0.7783]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([0, 1, 4, 2, 3, 2, 1, 0, 2, 2, 2, 0, 2, 4, 3, 4], device='cuda:0')
Preds:  tensor([0, 1, 4, 1, 2, 2, 1, 0, 0, 3, 3, 0, 3, 4, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.5519,     0.4252,     0.0228,     0.0001,     0.0000],
        [    0.2034,     0.7851,     0.0112,     0.0002,     0.0001],
        [    0.0013,     0.0033,     0.0416,     0.2472,     0.7066],
        [    0.3129,     0.5312,     0.1532,     0.0024,     0.0003],
        [    0.2613,     0.3239,     0.3665,     0.0373,     0.0109],
        [    0.0233,     0.0310,     0.9309,     0.0123,     0.0025],
        [    0.3346,     0.5347,     0.1300,     0.0006,     0.0001],
        [    0.9481,     0.0446,     0.0055,     0.0009,     0.0009],
        [    0.4227,     0.4052,     0.1687,     0.0031,     0.0004],
        [    0.0004,     0.0038,     0.2068,     0.6482,     0.1407],
        [    0.0013,     0.0147,     0.3937,     0.4851,     0.1052],
        [    0.9447,     0.0532,     0.0019,     0.0002,     0.0001],
        [    0.0006,     0.0129,     0.2395,     0.7261,     0.0208],
        [    0.0020,     0.0014,     0.0180,     0.1932,     0.7855],
        [    0.6246,     0.3030,     0.0711,     0.0011,     0.0001],
        [    0.0002,     0.0003,     0.0003,     0.0105,     0.9886]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 1, 3, 4, 3, 0, 3, 1, 1, 0, 0, 3, 2, 0], device='cuda:1')
Preds:  tensor([3, 2, 3, 1, 2, 4, 2, 1, 4, 1, 1, 0, 0, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0025,     0.0134,     0.3684,     0.6029,     0.0129],
        [    0.0022,     0.0537,     0.6999,     0.2387,     0.0055],
        [    0.0024,     0.0052,     0.0701,     0.9185,     0.0038],
        [    0.3781,     0.4558,     0.1616,     0.0042,     0.0003],
        [    0.0011,     0.0325,     0.9326,     0.0332,     0.0005],
        [    0.0008,     0.0009,     0.0113,     0.1468,     0.8402],
        [    0.0248,     0.1519,     0.5699,     0.2187,     0.0346],
        [    0.2098,     0.5231,     0.2645,     0.0024,     0.0001],
        [    0.0001,     0.0002,     0.0052,     0.3301,     0.6644],
        [    0.2151,     0.4554,     0.2961,     0.0293,     0.0042],
        [    0.1672,     0.4979,     0.3260,     0.0084,     0.0005],
        [    0.7977,     0.1886,     0.0132,     0.0004,     0.0002],
        [    0.6127,     0.2798,     0.0965,     0.0090,     0.0021],
        [    0.0146,     0.0570,     0.2304,     0.4839,     0.2141],
        [    0.0574,     0.2853,     0.5645,     0.0872,     0.0057],
        [    0.0177,     0.1592,     0.6373,     0.1837,     0.0021]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([1, 1, 1, 1, 2, 1, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Preds:  tensor([3, 0, 1, 4, 1, 2, 0, 0, 4, 4, 2, 1, 2, 0, 0, 1], device='cuda:0')
Outputs:  tensor([[    0.0074,     0.0497,     0.3632,     0.4301,     0.1497],
        [    0.3745,     0.3694,     0.2501,     0.0055,     0.0005],
        [    0.0657,     0.5117,     0.4139,     0.0083,     0.0003],
        [    0.0558,     0.0442,     0.0980,     0.2002,     0.6018],
        [    0.0341,     0.5228,     0.3770,     0.0590,     0.0071],
        [    0.0166,     0.2146,     0.7134,     0.0531,     0.0024],
        [    0.9170,     0.0750,     0.0057,     0.0009,     0.0015],
        [    0.6501,     0.3413,     0.0085,     0.0001,     0.0001],
        [    0.0003,     0.0002,     0.0018,     0.0715,     0.9262],
        [    0.0006,     0.0003,     0.0024,     0.0461,     0.9505],
        [    0.0093,     0.1779,     0.7395,     0.0723,     0.0010],
        [    0.3565,     0.4813,     0.1565,     0.0044,     0.0013],
        [    0.0008,     0.0142,     0.6170,     0.3676,     0.0003],
        [    0.6268,     0.3196,     0.0506,     0.0028,     0.0002],
        [    0.5749,     0.2857,     0.1069,     0.0208,     0.0118],
        [    0.3116,     0.5062,     0.1783,     0.0037,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 0, 4, 0, 0, 0, 1, 2], device='cuda:1')
Preds:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 1, 3, 0, 1, 0, 2, 4], device='cuda:1')
Outputs:  tensor([[    0.0008,     0.0048,     0.9434,     0.0421,     0.0089],
        [    0.5725,     0.3638,     0.0581,     0.0040,     0.0016],
        [    0.0005,     0.0025,     0.0018,     0.0268,     0.9684],
        [    0.0001,     0.0001,     0.0002,     0.0125,     0.9871],
        [    0.0003,     0.0003,     0.0068,     0.1813,     0.8113],
        [    0.6090,     0.3489,     0.0395,     0.0018,     0.0008],
        [    0.8342,     0.1544,     0.0108,     0.0004,     0.0001],
        [    0.0449,     0.2906,     0.6480,     0.0159,     0.0005],
        [    0.0003,     0.0003,     0.0072,     0.1776,     0.8146],
        [    0.0824,     0.4972,     0.4100,     0.0099,     0.0005],
        [    0.0037,     0.0040,     0.0721,     0.5138,     0.4064],
        [    0.4790,     0.2689,     0.1854,     0.0367,     0.0299],
        [    0.4693,     0.4817,     0.0483,     0.0006,     0.0002],
        [    0.8915,     0.1075,     0.0008,     0.0000,     0.0001],
        [    0.0160,     0.2591,     0.6924,     0.0322,     0.0002],
        [    0.0426,     0.0513,     0.1164,     0.1316,     0.6581]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([3, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 3, 3, 4, 4, 2], device='cuda:0')
Preds:  tensor([3, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 4, 2, 4, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0004,     0.0207,     0.6583,     0.3205],
        [    0.0022,     0.0279,     0.7386,     0.2114,     0.0199],
        [    0.0021,     0.0382,     0.6646,     0.2871,     0.0079],
        [    0.0239,     0.2026,     0.6874,     0.0847,     0.0014],
        [    0.4321,     0.5168,     0.0503,     0.0006,     0.0001],
        [    0.0844,     0.1571,     0.4586,     0.2278,     0.0722],
        [    0.1958,     0.6083,     0.1917,     0.0041,     0.0002],
        [    0.9760,     0.0229,     0.0010,     0.0001,     0.0001],
        [    0.0008,     0.0165,     0.6610,     0.3138,     0.0079],
        [    0.0507,     0.3017,     0.4175,     0.2044,     0.0257],
        [    0.6501,     0.3087,     0.0408,     0.0003,     0.0001],
        [    0.0001,     0.0001,     0.0003,     0.0093,     0.9903],
        [    0.0005,     0.0137,     0.6617,     0.3220,     0.0021],
        [    0.0003,     0.0005,     0.0092,     0.2480,     0.7421],
        [    0.0096,     0.0140,     0.1023,     0.1624,     0.7118],
        [    0.6278,     0.3264,     0.0445,     0.0011,     0.0003]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 2, 3, 0, 2, 4, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:1')
Preds:  tensor([4, 2, 2, 2, 4, 0, 1, 4, 2, 0, 1, 2, 0, 0, 0, 0], device='cuda:1')
Outputs:  tensor([[    0.0004,     0.0007,     0.0105,     0.2127,     0.7758],
        [    0.0025,     0.0462,     0.8219,     0.1276,     0.0018],
        [    0.0015,     0.0373,     0.9053,     0.0554,     0.0006],
        [    0.0042,     0.0350,     0.9588,     0.0017,     0.0002],
        [    0.0002,     0.0002,     0.0027,     0.0884,     0.9086],
        [    0.7391,     0.2497,     0.0109,     0.0002,     0.0001],
        [    0.2185,     0.4331,     0.2876,     0.0543,     0.0065],
        [    0.0017,     0.0024,     0.0130,     0.1222,     0.8607],
        [    0.0143,     0.1629,     0.7504,     0.0715,     0.0010],
        [    0.7524,     0.1694,     0.0471,     0.0114,     0.0197],
        [    0.2339,     0.5735,     0.1907,     0.0018,     0.0001],
        [    0.2908,     0.3184,     0.3446,     0.0364,     0.0097],
        [    0.9913,     0.0061,     0.0015,     0.0004,     0.0007],
        [    0.8211,     0.1455,     0.0239,     0.0040,     0.0054],
        [    0.9898,     0.0099,     0.0002,     0.0000,     0.0001],
        [    0.9968,     0.0024,     0.0003,     0.0001,     0.0004]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([4, 0, 4, 0, 4, 1, 1, 3, 0, 4, 4, 3, 0, 2, 3, 0], device='cuda:0')
Preds:  tensor([4, 0, 4, 0, 4, 1, 1, 2, 0, 3, 4, 3, 0, 1, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0003,     0.0002,     0.0018,     0.0738,     0.9239],
        [    0.6157,     0.3183,     0.0643,     0.0016,     0.0002],
        [    0.0003,     0.0003,     0.0071,     0.2737,     0.7186],
        [    0.8348,     0.1583,     0.0067,     0.0002,     0.0001],
        [    0.0560,     0.0274,     0.0470,     0.0833,     0.7863],
        [    0.0057,     0.9892,     0.0047,     0.0003,     0.0001],
        [    0.1085,     0.4552,     0.4102,     0.0246,     0.0015],
        [    0.0440,     0.2060,     0.2970,     0.2203,     0.2327],
        [    0.9682,     0.0309,     0.0007,     0.0001,     0.0001],
        [    0.0014,     0.0066,     0.1661,     0.4398,     0.3861],
        [    0.0015,     0.0205,     0.0104,     0.1844,     0.7833],
        [    0.0011,     0.0050,     0.1105,     0.4909,     0.3924],
        [    0.7140,     0.2748,     0.0111,     0.0001,     0.0001],
        [    0.0615,     0.5815,     0.3184,     0.0375,     0.0011],
        [    0.0001,     0.0002,     0.0052,     0.3183,     0.6762],
        [    0.8643,     0.1287,     0.0066,     0.0002,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Labels:  tensor([2, 3, 4, 0, 1, 3, 4, 3, 4, 2, 1, 0, 4, 4, 4, 4], device='cuda:1')
Preds:  tensor([2, 3, 4, 1, 1, 4, 4, 2, 4, 3, 1, 0, 4, 4, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0394,     0.3791,     0.5632,     0.0178,     0.0006],
        [    0.0003,     0.0017,     0.1306,     0.7527,     0.1147],
        [    0.0013,     0.0007,     0.0017,     0.0113,     0.9850],
        [    0.3044,     0.6541,     0.0409,     0.0004,     0.0002],
        [    0.2896,     0.5124,     0.1798,     0.0145,     0.0037],
        [    0.0010,     0.0019,     0.0422,     0.4077,     0.5473],
        [    0.0014,     0.0006,     0.0021,     0.0092,     0.9868],
        [    0.0095,     0.3680,     0.6101,     0.0120,     0.0003],
        [    0.0004,     0.0004,     0.0001,     0.0025,     0.9965],
        [    0.0002,     0.0052,     0.4389,     0.5460,     0.0097],
        [    0.3345,     0.5380,     0.1252,     0.0021,     0.0003],
        [    0.8832,     0.1120,     0.0044,     0.0002,     0.0001],
        [    0.0001,     0.0002,     0.0020,     0.0890,     0.9086],
        [    0.0006,     0.0005,     0.0025,     0.0689,     0.9275],
        [    0.0020,     0.0083,     0.2807,     0.5493,     0.1598],
        [    0.0002,     0.0001,     0.0012,     0.0858,     0.9127]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([4, 1, 1, 0, 0, 1, 2, 4, 2, 4, 4, 1, 0, 1, 2, 1], device='cuda:0')
Preds:  tensor([4, 1, 0, 0, 0, 1, 2, 4, 2, 4, 4, 4, 0, 0, 3, 0], device='cuda:0')
Outputs:  tensor([[    0.0011,     0.0009,     0.0041,     0.0474,     0.9465],
        [    0.3727,     0.5503,     0.0761,     0.0007,     0.0002],
        [    0.9535,     0.0452,     0.0012,     0.0001,     0.0000],
        [    0.6625,     0.3061,     0.0292,     0.0012,     0.0009],
        [    0.7414,     0.2396,     0.0178,     0.0007,     0.0005],
        [    0.1608,     0.6360,     0.2016,     0.0015,     0.0001],
        [    0.0671,     0.1948,     0.4763,     0.2318,     0.0300],
        [    0.0001,     0.0002,     0.0005,     0.0885,     0.9107],
        [    0.0352,     0.3793,     0.5642,     0.0212,     0.0001],
        [    0.0006,     0.0005,     0.0053,     0.1801,     0.8135],
        [    0.0127,     0.0148,     0.0383,     0.0956,     0.8387],
        [    0.2551,     0.0566,     0.0742,     0.1020,     0.5121],
        [    0.9516,     0.0475,     0.0008,     0.0001,     0.0000],
        [    0.5744,     0.3398,     0.0817,     0.0037,     0.0004],
        [    0.0718,     0.1162,     0.3403,     0.4067,     0.0651],
        [    0.5950,     0.2950,     0.0908,     0.0138,     0.0053]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 2, 1, 1, 4, 1, 4, 4, 1, 3, 2, 4, 3, 3, 4], device='cuda:1')
Preds:  tensor([3, 1, 2, 0, 1, 4, 0, 4, 4, 1, 2, 2, 4, 2, 4, 3], device='cuda:1')
Outputs:  tensor([[    0.0141,     0.0506,     0.3843,     0.4233,     0.1276],
        [    0.1076,     0.4725,     0.3993,     0.0200,     0.0006],
        [    0.1475,     0.3851,     0.4021,     0.0645,     0.0008],
        [    0.5997,     0.3193,     0.0796,     0.0013,     0.0002],
        [    0.3634,     0.5840,     0.0523,     0.0003,     0.0000],
        [    0.0016,     0.0007,     0.0020,     0.0426,     0.9531],
        [    0.6794,     0.2725,     0.0458,     0.0016,     0.0007],
        [    0.0002,     0.0004,     0.0156,     0.2691,     0.7146],
        [    0.0020,     0.0008,     0.0025,     0.0707,     0.9239],
        [    0.4594,     0.4883,     0.0512,     0.0009,     0.0001],
        [    0.0026,     0.0622,     0.8234,     0.1114,     0.0004],
        [    0.0018,     0.0416,     0.5497,     0.3993,     0.0076],
        [    0.0007,     0.0011,     0.0135,     0.2570,     0.7277],
        [    0.0004,     0.0122,     0.7036,     0.2800,     0.0038],
        [    0.0007,     0.0008,     0.0086,     0.1130,     0.8769],
        [    0.0003,     0.0007,     0.0301,     0.5205,     0.4484]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([2, 0, 1, 3, 4, 2, 0, 0, 2, 1, 0, 1, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([4, 0, 2, 2, 4, 2, 0, 0, 1, 0, 0, 0, 0, 1, 2, 4], device='cuda:0')
Outputs:  tensor([[    0.0006,     0.0016,     0.0222,     0.2307,     0.7449],
        [    0.7434,     0.2342,     0.0212,     0.0009,     0.0003],
        [    0.0048,     0.1128,     0.8116,     0.0704,     0.0003],
        [    0.0022,     0.0670,     0.8779,     0.0525,     0.0004],
        [    0.0058,     0.0014,     0.0028,     0.0148,     0.9752],
        [    0.0085,     0.1583,     0.6344,     0.1950,     0.0039],
        [    0.5194,     0.4329,     0.0465,     0.0010,     0.0001],
        [    0.7787,     0.1988,     0.0222,     0.0002,     0.0000],
        [    0.0434,     0.8500,     0.0895,     0.0122,     0.0049],
        [    0.6485,     0.2345,     0.1010,     0.0108,     0.0052],
        [    0.9911,     0.0086,     0.0002,     0.0000,     0.0001],
        [    0.6210,     0.2851,     0.0916,     0.0022,     0.0001],
        [    0.7298,     0.1918,     0.0669,     0.0084,     0.0030],
        [    0.0111,     0.6302,     0.3024,     0.0535,     0.0028],
        [    0.0887,     0.1654,     0.3442,     0.2435,     0.1582],
        [    0.0003,     0.0005,     0.0004,     0.0247,     0.9742]],
       device='cuda:0')
Metric:  tensor(0.4375, device='cuda:0')
------------------------
Labels:  tensor([4, 4, 4, 2, 3, 2, 4, 1, 2, 1, 1, 2, 0, 1, 0, 1], device='cuda:1')
Preds:  tensor([4, 4, 4, 1, 2, 3, 4, 2, 2, 2, 0, 2, 0, 0, 0, 2], device='cuda:1')
Outputs:  tensor([[    0.0005,     0.0006,     0.0046,     0.0790,     0.9153],
        [    0.0003,     0.0003,     0.0035,     0.1638,     0.8322],
        [    0.0025,     0.0008,     0.0056,     0.1608,     0.8302],
        [    0.2083,     0.4234,     0.3626,     0.0053,     0.0005],
        [    0.0040,     0.0311,     0.5741,     0.3466,     0.0441],
        [    0.0001,     0.0009,     0.0762,     0.8168,     0.1060],
        [    0.0014,     0.0009,     0.0023,     0.0088,     0.9866],
        [    0.0870,     0.4219,     0.4266,     0.0605,     0.0039],
        [    0.0018,     0.0674,     0.8313,     0.0991,     0.0003],
        [    0.0328,     0.3006,     0.6158,     0.0497,     0.0011],
        [    0.6646,     0.2672,     0.0596,     0.0079,     0.0007],
        [    0.0178,     0.3246,     0.6403,     0.0167,     0.0006],
        [    0.9830,     0.0165,     0.0003,     0.0001,     0.0001],
        [    0.7344,     0.2453,     0.0196,     0.0005,     0.0001],
        [    0.7639,     0.2237,     0.0119,     0.0004,     0.0001],
        [    0.0114,     0.1123,     0.6440,     0.2254,     0.0069]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([1, 1, 2, 4, 2, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 0], device='cuda:0')
Preds:  tensor([1, 2, 2, 4, 0, 4, 0, 4, 4, 2, 0, 2, 0, 4, 4, 1], device='cuda:0')
Outputs:  tensor([[    0.2280,     0.5366,     0.2341,     0.0012,     0.0001],
        [    0.0562,     0.3407,     0.5561,     0.0451,     0.0019],
        [    0.0004,     0.0059,     0.9886,     0.0048,     0.0003],
        [    0.0002,     0.0002,     0.0002,     0.0386,     0.9608],
        [    0.8398,     0.1334,     0.0253,     0.0011,     0.0005],
        [    0.0007,     0.0002,     0.0006,     0.0144,     0.9842],
        [    0.9693,     0.0301,     0.0004,     0.0001,     0.0002],
        [    0.0001,     0.0001,     0.0005,     0.0264,     0.9729],
        [    0.0001,     0.0001,     0.0045,     0.2671,     0.7282],
        [    0.0489,     0.4317,     0.5112,     0.0082,     0.0001],
        [    0.6073,     0.3166,     0.0731,     0.0028,     0.0002],
        [    0.0079,     0.1090,     0.7241,     0.1542,     0.0048],
        [    0.7642,     0.1872,     0.0382,     0.0058,     0.0047],
        [    0.0007,     0.0008,     0.0162,     0.4884,     0.4938],
        [    0.0267,     0.0143,     0.0293,     0.0516,     0.8781],
        [    0.0737,     0.4970,     0.3986,     0.0303,     0.0004]],
       device='cuda:0')
Metric:  tensor(0.4375, device='cuda:0')
------------------------
Mean loss[0.9213146580725894] | Mean metric[0.6085895558809176]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 2
--------------
Labels:  tensor([0, 4, 0, 3, 0, 1, 2, 1, 4, 2, 1, 1, 4, 3, 1, 3], device='cuda:1')
Preds:  tensor([0, 4, 0, 3, 0, 0, 1, 2, 2, 3, 1, 2, 4, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.6751,     0.3066,     0.0177,     0.0002,     0.0003],
        [    0.0005,     0.0004,     0.0019,     0.0590,     0.9383],
        [    0.8525,     0.1379,     0.0091,     0.0004,     0.0001],
        [    0.0000,     0.0003,     0.0358,     0.8686,     0.0953],
        [    0.7950,     0.2016,     0.0033,     0.0001,     0.0000],
        [    0.6925,     0.2724,     0.0334,     0.0015,     0.0003],
        [    0.1765,     0.4181,     0.3674,     0.0347,     0.0032],
        [    0.0866,     0.3604,     0.5214,     0.0284,     0.0032],
        [    0.1668,     0.1964,     0.3908,     0.2108,     0.0352],
        [    0.0007,     0.0046,     0.1810,     0.6233,     0.1905],
        [    0.0828,     0.5491,     0.3640,     0.0039,     0.0002],
        [    0.0168,     0.0918,     0.6168,     0.2693,     0.0053],
        [    0.0007,     0.0006,     0.0118,     0.4109,     0.5761],
        [    0.0001,     0.0003,     0.0435,     0.8551,     0.1011],
        [    0.0060,     0.1033,     0.7404,     0.1470,     0.0033],
        [    0.0168,     0.3177,     0.6169,     0.0415,     0.0071]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Mean loss[0.9213408221262964] | Mean metric[0.6051427525622255]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 2
--------------
Labels:  tensor([4, 2, 2, 0, 0, 3, 3, 0, 0, 0, 0, 1, 4, 2, 0, 4], device='cuda:0')
Preds:  tensor([4, 4, 2, 0, 0, 3, 3, 0, 0, 0, 1, 0, 3, 2, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0002,     0.0004,     0.0511,     0.9482],
        [    0.0002,     0.0004,     0.0055,     0.1223,     0.8716],
        [    0.1283,     0.2989,     0.4362,     0.1328,     0.0037],
        [    0.9413,     0.0559,     0.0024,     0.0002,     0.0002],
        [    0.7838,     0.1851,     0.0287,     0.0014,     0.0010],
        [    0.0001,     0.0001,     0.0028,     0.9906,     0.0064],
        [    0.0001,     0.0014,     0.1734,     0.7806,     0.0444],
        [    0.4369,     0.4083,     0.0936,     0.0200,     0.0412],
        [    0.9835,     0.0161,     0.0002,     0.0000,     0.0001],
        [    0.7317,     0.2540,     0.0132,     0.0005,     0.0006],
        [    0.2258,     0.6765,     0.0290,     0.0212,     0.0475],
        [    0.5018,     0.4387,     0.0541,     0.0026,     0.0027],
        [    0.0003,     0.0006,     0.0098,     0.8864,     0.1029],
        [    0.2020,     0.2948,     0.3759,     0.0713,     0.0560],
        [    0.7750,     0.1957,     0.0262,     0.0019,     0.0012],
        [    0.0001,     0.0002,     0.0015,     0.1225,     0.8758]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Mean loss[0.9261883869876275] | Mean metric[0.604227672035139]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 2
--------------
Labels:  tensor([4, 4, 2, 0, 0, 4, 4, 1, 4, 2, 0, 1, 1, 2, 2, 1], device='cuda:1')
Preds:  tensor([4, 4, 1, 2, 0, 4, 4, 1, 4, 2, 3, 1, 2, 1, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0004,     0.0003,     0.0052,     0.1307,     0.8634],
        [    0.0002,     0.0003,     0.0027,     0.0897,     0.9071],
        [    0.2456,     0.6796,     0.0665,     0.0069,     0.0015],
        [    0.0625,     0.2112,     0.4845,     0.2025,     0.0393],
        [    0.4618,     0.1528,     0.1872,     0.0830,     0.1153],
        [    0.0019,     0.0007,     0.0013,     0.0146,     0.9816],
        [    0.0003,     0.0004,     0.0026,     0.0631,     0.9337],
        [    0.3163,     0.5334,     0.1497,     0.0006,     0.0001],
        [    0.0001,     0.0002,     0.0041,     0.1095,     0.8860],
        [    0.0663,     0.2237,     0.5927,     0.1125,     0.0047],
        [    0.0030,     0.0268,     0.2317,     0.4829,     0.2555],
        [    0.3067,     0.4441,     0.2291,     0.0146,     0.0054],
        [    0.0009,     0.0253,     0.9231,     0.0496,     0.0010],
        [    0.1246,     0.5176,     0.3507,     0.0069,     0.0002],
        [    0.0132,     0.2835,     0.5135,     0.1746,     0.0152],
        [    0.0100,     0.2085,     0.7055,     0.0746,     0.0014]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Mean loss[0.9197063925005623] | Mean metric[0.6053867740361152]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 2
--------------
Step[500] | Loss[0.9093506932258606] | Lr[2.0000000000000003e-06]
Step[500] | Loss[0.6922423243522644] | Lr[2.0000000000000003e-06]
Step[500] | Loss[0.2988356351852417] | Lr[2.0000000000000003e-06]
Step[500] | Loss[0.7422816753387451] | Lr[2.0000000000000003e-06]
Step[1000] | Loss[0.7323082685470581] | Lr[2.0000000000000003e-06]
Step[1000] | Loss[0.6486176252365112] | Lr[2.0000000000000003e-06]
Step[1000] | Loss[0.8576053977012634] | Lr[2.0000000000000003e-06]
Step[1000] | Loss[0.7067684531211853] | Lr[2.0000000000000003e-06]
Step[1500] | Loss[0.3792825937271118] | Lr[2.0000000000000003e-06]
Step[1500] | Loss[0.5260927081108093] | Lr[2.0000000000000003e-06]
Step[1500] | Loss[0.48830485343933105] | Lr[2.0000000000000003e-06]
Step[1500] | Loss[0.6123939156532288] | Lr[2.0000000000000003e-06]
Step[2000] | Loss[0.4801461398601532] | Lr[2.0000000000000003e-06]
Step[2000] | Loss[0.4562346339225769] | Lr[2.0000000000000003e-06]
Step[2000] | Loss[0.9579204320907593] | Lr[2.0000000000000003e-06]
Step[2000] | Loss[0.5164467096328735] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.6560676693916321] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.4702967405319214] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.7747235298156738] | Lr[2.0000000000000003e-06]
Step[2500] | Loss[0.47346216440200806] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.7312417030334473] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.4439597427845001] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.6915960907936096] | Lr[2.0000000000000003e-06]
Step[3000] | Loss[0.8320368528366089] | Lr[2.0000000000000003e-06]
Step[3500] | Loss[1.0526586771011353] | Lr[2.0000000000000003e-06]
Step[3500] | Loss[0.5564146041870117] | Lr[2.0000000000000003e-06]
Step[3500] | Loss[0.39703819155693054] | Lr[2.0000000000000003e-06]
Step[3500] | Loss[0.4628260135650635] | Lr[2.0000000000000003e-06]
Step[4000] | Loss[0.9274491667747498] | Lr[2.0000000000000003e-06]
Step[4000] | Loss[0.938049852848053] | Lr[2.0000000000000003e-06]
Step[4000] | Loss[0.7778792381286621] | Lr[2.0000000000000003e-06]
Step[4000] | Loss[0.6938336491584778] | Lr[2.0000000000000003e-06]
Step[4500] | Loss[0.798418402671814] | Lr[2.0000000000000003e-06]
Step[4500] | Loss[0.7314133048057556] | Lr[2.0000000000000003e-06]
Step[4500] | Loss[0.5426463484764099] | Lr[2.0000000000000003e-06]
Step[4500] | Loss[0.7770564556121826] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.5888372659683228] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.4556654393672943] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.7201281189918518] | Lr[2.0000000000000003e-06]
Step[5000] | Loss[0.4679960608482361] | Lr[2.0000000000000003e-06]
Step[5500] | Loss[0.8889694213867188] | Lr[2.0000000000000003e-06]
Step[5500] | Loss[0.5287781953811646] | Lr[2.0000000000000003e-06]
Step[5500] | Loss[0.3571496307849884] | Lr[2.0000000000000003e-06]
Step[5500] | Loss[0.5653945207595825] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.5329039692878723] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.43552732467651367] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.8623369932174683] | Lr[2.0000000000000003e-06]
Step[6000] | Loss[0.4312261939048767] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.6324262619018555] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.5858808159828186] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.6486604809761047] | Lr[2.0000000000000003e-06]
Step[6500] | Loss[0.7758874893188477] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[0.7244840264320374] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[1.0276885032653809] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[0.5585612654685974] | Lr[2.0000000000000003e-06]
Step[7000] | Loss[0.635575532913208] | Lr[2.0000000000000003e-06]
Step[7500] | Loss[0.5995653867721558] | Lr[2.0000000000000003e-06]
Step[7500] | Loss[0.5700372457504272] | Lr[2.0000000000000003e-06]
Step[7500] | Loss[0.7043746113777161] | Lr[2.0000000000000003e-06]
Step[7500] | Loss[0.5958277583122253] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.5799693465232849] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.45755019783973694] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.6511294841766357] | Lr[2.0000000000000003e-06]
Step[8000] | Loss[0.30871179699897766] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.6519774794578552] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.9147594571113586] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.5286151766777039] | Lr[2.0000000000000003e-06]
Step[8500] | Loss[0.8321276307106018] | Lr[2.0000000000000003e-06]
Step[9000] | Loss[0.6880864500999451] | Lr[2.0000000000000003e-06]
Step[9000] | Loss[0.5084171295166016] | Lr[2.0000000000000003e-06]
Step[9000] | Loss[0.778257429599762] | Lr[2.0000000000000003e-06]
Step[9000] | Loss[0.9078392386436462] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.8168419003486633] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.35709068179130554] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.8280553817749023] | Lr[2.0000000000000003e-06]
Step[9500] | Loss[0.44276541471481323] | Lr[2.0000000000000003e-06]
Step[10000] | Loss[0.8784365653991699] | Lr[2.0000000000000003e-06]
Step[10000] | Loss[0.22897584736347198] | Lr[2.0000000000000003e-06]
Step[10000] | Loss[0.6025183796882629] | Lr[2.0000000000000003e-06]
Step[10000] | Loss[0.7630234360694885] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.8380850553512573] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.6588330268859863] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.924372136592865] | Lr[2.0000000000000003e-06]
Step[10500] | Loss[0.44230180978775024] | Lr[2.0000000000000003e-06]
Step[11000] | Loss[0.6467550992965698] | Lr[2.0000000000000003e-06]
Step[11000] | Loss[0.7265690565109253] | Lr[2.0000000000000003e-06]
Step[11000] | Loss[0.44182154536247253] | Lr[2.0000000000000003e-06]
Step[11000] | Loss[0.9080907702445984] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.7886126041412354] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.6637997031211853] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.6171785593032837] | Lr[2.0000000000000003e-06]
Step[11500] | Loss[0.5891730785369873] | Lr[2.0000000000000003e-06]
Step[12000] | Loss[0.6138023138046265] | Lr[2.0000000000000003e-06]
Step[12000] | Loss[0.5082261562347412] | Lr[2.0000000000000003e-06]
Step[12000] | Loss[0.549018144607544] | Lr[2.0000000000000003e-06]
Step[12000] | Loss[0.4565393030643463] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.758329451084137] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.4412440359592438] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.5168349742889404] | Lr[2.0000000000000003e-06]
Step[12500] | Loss[0.49265947937965393] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.3427776098251343] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.2554565370082855] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.4971659481525421] | Lr[2.0000000000000003e-06]
Step[13000] | Loss[0.348367840051651] | Lr[2.0000000000000003e-06]
Step[13500] | Loss[0.2989042401313782] | Lr[2.0000000000000003e-06]
Step[13500] | Loss[0.6966493725776672] | Lr[2.0000000000000003e-06]
Step[13500] | Loss[0.5085588693618774] | Lr[2.0000000000000003e-06]
Step[13500] | Loss[0.6212265491485596] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[0.948732852935791] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[0.8812538385391235] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[0.5657622814178467] | Lr[2.0000000000000003e-06]
Step[14000] | Loss[0.5426163673400879] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.5261967182159424] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.7269893884658813] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.8880505561828613] | Lr[2.0000000000000003e-06]
Step[14500] | Loss[0.48594987392425537] | Lr[2.0000000000000003e-06]
Step[15000] | Loss[0.8184720873832703] | Lr[2.0000000000000003e-06]
Step[15000] | Loss[0.6704729199409485] | Lr[2.0000000000000003e-06]
Step[15000] | Loss[0.8087832927703857] | Lr[2.0000000000000003e-06]
Step[15000] | Loss[0.650242030620575] | Lr[2.0000000000000003e-06]
Step[15500] | Loss[0.47547683119773865] | Lr[2.0000000000000003e-06]
Step[15500] | Loss[0.7038342356681824] | Lr[2.0000000000000003e-06]
Step[15500] | Loss[0.5698163509368896] | Lr[2.0000000000000003e-06]
Step[15500] | Loss[0.7292708158493042] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.7622038125991821] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.7767571806907654] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.8023914098739624] | Lr[2.0000000000000003e-06]
Step[16000] | Loss[0.4884481132030487] | Lr[2.0000000000000003e-06]
Step[16500] | Loss[0.4957078695297241] | Lr[2.0000000000000003e-06]
Step[16500] | Loss[0.5887048244476318] | Lr[2.0000000000000003e-06]
Step[16500] | Loss[0.67405104637146] | Lr[2.0000000000000003e-06]
Step[16500] | Loss[0.5686371326446533] | Lr[2.0000000000000003e-06]
Step[17000] | Loss[0.9786500334739685] | Lr[2.0000000000000003e-06]
Step[17000] | Loss[0.3602333068847656] | Lr[2.0000000000000003e-06]
Step[17000] | Loss[0.7472655177116394] | Lr[2.0000000000000003e-06]
Step[17000] | Loss[0.3284011483192444] | Lr[2.0000000000000003e-06]
Step[17500] | Loss[0.649804949760437] | Lr[2.0000000000000003e-06]
Step[17500] | Loss[0.6206589341163635] | Lr[2.0000000000000003e-06]
Step[17500] | Loss[0.9480880498886108] | Lr[2.0000000000000003e-06]
Step[17500] | Loss[0.722804069519043] | Lr[2.0000000000000003e-06]
Step[18000] | Loss[0.7272555232048035] | Lr[2.0000000000000003e-06]
Step[18000] | Loss[0.5003334879875183] | Lr[2.0000000000000003e-06]
Step[18000] | Loss[0.5354784727096558] | Lr[2.0000000000000003e-06]
Step[18000] | Loss[0.8232194185256958] | Lr[2.0000000000000003e-06]
Step[18500] | Loss[0.5991353988647461] | Lr[2.0000000000000003e-06]
Step[18500] | Loss[0.7222306728363037] | Lr[2.0000000000000003e-06]
Step[18500] | Loss[0.7185510396957397] | Lr[2.0000000000000003e-06]
Step[18500] | Loss[0.37641605734825134] | Lr[2.0000000000000003e-06]
Step[19000] | Loss[0.6995201706886292] | Lr[2.0000000000000003e-06]
Step[19000] | Loss[0.7651670575141907] | Lr[2.0000000000000003e-06]
Step[19000] | Loss[0.5076879858970642] | Lr[2.0000000000000003e-06]
Step[19000] | Loss[0.6092721819877625] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.4980264902114868] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.450093150138855] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.3722579777240753] | Lr[2.0000000000000003e-06]
Step[19500] | Loss[0.3760916292667389] | Lr[2.0000000000000003e-06]
Step[20000] | Loss[0.6406070590019226] | Lr[2.0000000000000003e-06]
Step[20000] | Loss[0.7877103090286255] | Lr[2.0000000000000003e-06]
Step[20000] | Loss[1.0682510137557983] | Lr[2.0000000000000003e-06]
Step[20000] | Loss[0.6716046929359436] | Lr[2.0000000000000003e-06]
Step[20500] | Loss[1.2006278038024902] | Lr[2.0000000000000003e-06]
Step[20500] | Loss[0.6204831004142761] | Lr[2.0000000000000003e-06]
Step[20500] | Loss[0.6529207229614258] | Lr[2.0000000000000003e-06]
Step[20500] | Loss[0.5300787091255188] | Lr[2.0000000000000003e-06]
Step[21000] | Loss[0.2535265386104584] | Lr[2.0000000000000003e-06]
Step[21000] | Loss[0.5304851531982422] | Lr[2.0000000000000003e-06]
Step[21000] | Loss[0.5559345483779907] | Lr[2.0000000000000003e-06]
Step[21000] | Loss[0.8447580337524414] | Lr[2.0000000000000003e-06]
Step[21500] | Loss[0.7435142993927002] | Lr[2.0000000000000003e-06]
Step[21500] | Loss[0.5378705263137817] | Lr[2.0000000000000003e-06]
Step[21500] | Loss[0.8136639595031738] | Lr[2.0000000000000003e-06]
Step[21500] | Loss[0.5080389380455017] | Lr[2.0000000000000003e-06]
Step[22000] | Loss[0.18682532012462616] | Lr[2.0000000000000003e-06]
Step[22000] | Loss[0.6876901984214783] | Lr[2.0000000000000003e-06]
Step[22000] | Loss[0.807722806930542] | Lr[2.0000000000000003e-06]
Step[22000] | Loss[0.36750444769859314] | Lr[2.0000000000000003e-06]
Step[22500] | Loss[0.5241641998291016] | Lr[2.0000000000000003e-06]
Step[22500] | Loss[0.751401960849762] | Lr[2.0000000000000003e-06]
Step[22500] | Loss[0.8783284425735474] | Lr[2.0000000000000003e-06]
Step[22500] | Loss[0.5264787673950195] | Lr[2.0000000000000003e-06]
Step[23000] | Loss[0.5056143999099731] | Lr[2.0000000000000003e-06]
Step[23000] | Loss[0.9265148639678955] | Lr[2.0000000000000003e-06]
Step[23000] | Loss[0.7412084341049194] | Lr[2.0000000000000003e-06]
Step[23000] | Loss[0.721829354763031] | Lr[2.0000000000000003e-06]
Step[23500] | Loss[0.3917982876300812] | Lr[2.0000000000000003e-06]
Step[23500] | Loss[0.4204980432987213] | Lr[2.0000000000000003e-06]
Step[23500] | Loss[0.582534909248352] | Lr[2.0000000000000003e-06]
Step[23500] | Loss[1.2618756294250488] | Lr[2.0000000000000003e-06]
Step[24000] | Loss[0.43878471851348877] | Lr[2.0000000000000003e-06]
Step[24000] | Loss[0.49345770478248596] | Lr[2.0000000000000003e-06]Step[24000] | Loss[0.7552427649497986] | Lr[2.0000000000000003e-06]

Step[24000] | Loss[0.5454190969467163] | Lr[2.0000000000000003e-06]
Step[24500] | Loss[0.4978788495063782] | Lr[2.0000000000000003e-06]
Step[24500] | Loss[0.48770415782928467] | Lr[2.0000000000000003e-06]
Step[24500] | Loss[0.6661777496337891] | Lr[2.0000000000000003e-06]
Step[24500] | Loss[0.41491174697875977] | Lr[2.0000000000000003e-06]
Step[25000] | Loss[0.6370302438735962] | Lr[2.0000000000000003e-06]
Step[25000] | Loss[0.4127151370048523] | Lr[2.0000000000000003e-06]
Step[25000] | Loss[1.1200029850006104] | Lr[2.0000000000000003e-06]
Step[25000] | Loss[0.6799280643463135] | Lr[2.0000000000000003e-06]
Step[25500] | Loss[0.49614274501800537] | Lr[2.0000000000000003e-06]
Step[25500] | Loss[0.44960296154022217] | Lr[2.0000000000000003e-06]
Step[25500] | Loss[0.6166111826896667] | Lr[2.0000000000000003e-06]
Step[25500] | Loss[0.5198063850402832] | Lr[2.0000000000000003e-06]
Step[26000] | Loss[0.39669176936149597] | Lr[2.0000000000000003e-06]
Step[26000] | Loss[0.7441165447235107] | Lr[2.0000000000000003e-06]
Step[26000] | Loss[0.6707662343978882] | Lr[2.0000000000000003e-06]
Step[26000] | Loss[0.5517347455024719] | Lr[2.0000000000000003e-06]
Step[26500] | Loss[0.5472708344459534] | Lr[2.0000000000000003e-06]
Step[26500] | Loss[0.7737631797790527] | Lr[2.0000000000000003e-06]
Step[26500] | Loss[0.9051772952079773] | Lr[2.0000000000000003e-06]
Step[26500] | Loss[0.7069898843765259] | Lr[2.0000000000000003e-06]
Step[27000] | Loss[0.6689132452011108] | Lr[2.0000000000000003e-06]
Step[27000] | Loss[0.8245179653167725] | Lr[2.0000000000000003e-06]
Step[27000] | Loss[0.7881605625152588] | Lr[2.0000000000000003e-06]
Step[27000] | Loss[0.7245471477508545] | Lr[2.0000000000000003e-06]
Step[27500] | Loss[0.48879939317703247] | Lr[2.0000000000000003e-06]
Step[27500] | Loss[0.2761400640010834] | Lr[2.0000000000000003e-06]
Step[27500] | Loss[0.5961093306541443] | Lr[2.0000000000000003e-06]
Step[27500] | Loss[0.9007872343063354] | Lr[2.0000000000000003e-06]
Step[28000] | Loss[0.5714831352233887] | Lr[2.0000000000000003e-06]
Step[28000] | Loss[0.5822426080703735] | Lr[2.0000000000000003e-06]
Step[28000] | Loss[0.5987538695335388] | Lr[2.0000000000000003e-06]
Step[28000] | Loss[0.2761691212654114] | Lr[2.0000000000000003e-06]
Step[28500] | Loss[0.6281827092170715] | Lr[2.0000000000000003e-06]
Step[28500] | Loss[0.5928871035575867] | Lr[2.0000000000000003e-06]
Step[28500] | Loss[0.6109921336174011] | Lr[2.0000000000000003e-06]
Step[28500] | Loss[0.508806049823761] | Lr[2.0000000000000003e-06]
Step[29000] | Loss[0.5086347460746765] | Lr[2.0000000000000003e-06]
Step[29000] | Loss[0.5883843302726746] | Lr[2.0000000000000003e-06]
Step[29000] | Loss[0.4365087151527405] | Lr[2.0000000000000003e-06]
Step[29000] | Loss[0.5911477208137512] | Lr[2.0000000000000003e-06]
Step[29500] | Loss[0.49302420020103455] | Lr[2.0000000000000003e-06]
Step[29500] | Loss[0.6313854455947876] | Lr[2.0000000000000003e-06]
Step[29500] | Loss[0.4709879755973816] | Lr[2.0000000000000003e-06]
Step[29500] | Loss[0.8229038119316101] | Lr[2.0000000000000003e-06]
Step[30000] | Loss[0.871120274066925] | Lr[2.0000000000000003e-06]
Step[30000] | Loss[0.3510120213031769] | Lr[2.0000000000000003e-06]
Step[30000] | Loss[0.5905607342720032] | Lr[2.0000000000000003e-06]
Step[30000] | Loss[0.6592654585838318] | Lr[2.0000000000000003e-06]
Step[30500] | Loss[0.8495681881904602] | Lr[2.0000000000000003e-06]
Step[30500] | Loss[0.6821324825286865] | Lr[2.0000000000000003e-06]
Step[30500] | Loss[0.5334166288375854] | Lr[2.0000000000000003e-06]
Step[30500] | Loss[0.45045092701911926] | Lr[2.0000000000000003e-06]
Step[31000] | Loss[0.5276076793670654] | Lr[2.0000000000000003e-06]
Step[31000] | Loss[0.6481058597564697] | Lr[2.0000000000000003e-06]
Step[31000] | Loss[0.6464341282844543] | Lr[2.0000000000000003e-06]
Step[31000] | Loss[0.7456842660903931] | Lr[2.0000000000000003e-06]
Step[31500] | Loss[0.6564960479736328] | Lr[2.0000000000000003e-06]
Step[31500] | Loss[0.5932961702346802] | Lr[2.0000000000000003e-06]
Step[31500] | Loss[0.51059490442276] | Lr[2.0000000000000003e-06]
Step[31500] | Loss[0.8457891345024109] | Lr[2.0000000000000003e-06]
Step[32000] | Loss[0.553631067276001] | Lr[2.0000000000000003e-06]
Step[32000] | Loss[0.5185226202011108] | Lr[2.0000000000000003e-06]
Step[32000] | Loss[1.0698035955429077] | Lr[2.0000000000000003e-06]
Step[32000] | Loss[0.6240336894989014] | Lr[2.0000000000000003e-06]
Step[32500] | Loss[0.5181258320808411] | Lr[2.0000000000000003e-06]
Step[32500] | Loss[0.5825481414794922] | Lr[2.0000000000000003e-06]
Step[32500] | Loss[0.4860527813434601] | Lr[2.0000000000000003e-06]
Step[32500] | Loss[0.9112902879714966] | Lr[2.0000000000000003e-06]
Step[33000] | Loss[0.6060397624969482] | Lr[2.0000000000000003e-06]
Step[33000] | Loss[0.751264750957489] | Lr[2.0000000000000003e-06]
Step[33000] | Loss[0.6611329913139343] | Lr[2.0000000000000003e-06]
Step[33000] | Loss[0.5854431986808777] | Lr[2.0000000000000003e-06]
Step[33500] | Loss[0.9148067235946655] | Lr[2.0000000000000003e-06]
Step[33500] | Loss[1.0049870014190674] | Lr[2.0000000000000003e-06]
Step[33500] | Loss[0.6059520244598389] | Lr[2.0000000000000003e-06]
Step[33500] | Loss[0.44410228729248047] | Lr[2.0000000000000003e-06]
Step[34000] | Loss[0.6658847332000732] | Lr[2.0000000000000003e-06]
Step[34000] | Loss[0.6054935455322266] | Lr[2.0000000000000003e-06]
Step[34000] | Loss[0.5731685757637024] | Lr[2.0000000000000003e-06]
Step[34000] | Loss[0.533284604549408] | Lr[2.0000000000000003e-06]
Step[34500] | Loss[0.42615535855293274] | Lr[2.0000000000000003e-06]
Step[34500] | Loss[1.0710928440093994] | Lr[2.0000000000000003e-06]
Step[34500] | Loss[0.6028790473937988] | Lr[2.0000000000000003e-06]
Step[34500] | Loss[0.7333821058273315] | Lr[2.0000000000000003e-06]
Step[35000] | Loss[0.5268186926841736] | Lr[2.0000000000000003e-06]
Step[35000] | Loss[0.24588030576705933] | Lr[2.0000000000000003e-06]
Step[35000] | Loss[0.5991806983947754] | Lr[2.0000000000000003e-06]
Step[35000] | Loss[0.5191951990127563] | Lr[2.0000000000000003e-06]
Step[35500] | Loss[0.4858456552028656] | Lr[2.0000000000000003e-06]
Step[35500] | Loss[0.6410769820213318] | Lr[2.0000000000000003e-06]
Step[35500] | Loss[0.7828865647315979] | Lr[2.0000000000000003e-06]
Step[35500] | Loss[0.4689159393310547] | Lr[2.0000000000000003e-06]
Step[36000] | Loss[0.23864184319972992] | Lr[2.0000000000000003e-06]
Step[36000] | Loss[0.6326526999473572] | Lr[2.0000000000000003e-06]
Step[36000] | Loss[0.5681538581848145] | Lr[2.0000000000000003e-06]
Step[36000] | Loss[0.647598147392273] | Lr[2.0000000000000003e-06]
Step[36500] | Loss[0.7011512517929077] | Lr[2.0000000000000003e-06]
Step[36500] | Loss[0.5658528208732605] | Lr[2.0000000000000003e-06]
Step[36500] | Loss[0.884990930557251] | Lr[2.0000000000000003e-06]
Step[36500] | Loss[1.0332903861999512] | Lr[2.0000000000000003e-06]
Step[37000] | Loss[0.4956406354904175] | Lr[2.0000000000000003e-06]
Step[37000] | Loss[0.3736552298069] | Lr[2.0000000000000003e-06]
Step[37000] | Loss[0.6845583915710449] | Lr[2.0000000000000003e-06]
Step[37000] | Loss[0.5082764029502869] | Lr[2.0000000000000003e-06]
Step[37500] | Loss[0.8727145791053772] | Lr[2.0000000000000003e-06]
Step[37500] | Loss[0.5562068223953247] | Lr[2.0000000000000003e-06]
Step[37500] | Loss[0.5332024097442627] | Lr[2.0000000000000003e-06]
Step[37500] | Loss[0.6295706629753113] | Lr[2.0000000000000003e-06]
Step[38000] | Loss[0.48047980666160583] | Lr[2.0000000000000003e-06]
Step[38000] | Loss[1.066672682762146] | Lr[2.0000000000000003e-06]
Step[38000] | Loss[0.36551523208618164] | Lr[2.0000000000000003e-06]
Step[38000] | Loss[0.4415320158004761] | Lr[2.0000000000000003e-06]
Step[38500] | Loss[0.5624648332595825] | Lr[2.0000000000000003e-06]
Step[38500] | Loss[0.9165053963661194] | Lr[2.0000000000000003e-06]
Step[38500] | Loss[0.722419023513794] | Lr[2.0000000000000003e-06]
Step[38500] | Loss[0.6917007565498352] | Lr[2.0000000000000003e-06]
Step[39000] | Loss[0.671593964099884] | Lr[2.0000000000000003e-06]
Step[39000] | Loss[0.7463030815124512] | Lr[2.0000000000000003e-06]
Step[39000] | Loss[0.4856109619140625] | Lr[2.0000000000000003e-06]
Step[39000] | Loss[0.6938259601593018] | Lr[2.0000000000000003e-06]
Step[39500] | Loss[0.8237496614456177] | Lr[2.0000000000000003e-06]
Step[39500] | Loss[1.1372038125991821] | Lr[2.0000000000000003e-06]
Step[39500] | Loss[0.4908384084701538] | Lr[2.0000000000000003e-06]
Step[39500] | Loss[0.5691235065460205] | Lr[2.0000000000000003e-06]
Step[40000] | Loss[0.7655104994773865] | Lr[2.0000000000000003e-06]
Step[40000] | Loss[0.4849366545677185] | Lr[2.0000000000000003e-06]
Step[40000] | Loss[0.5883800387382507] | Lr[2.0000000000000003e-06]
Step[40000] | Loss[0.7531506419181824] | Lr[2.0000000000000003e-06]
Step[40500] | Loss[0.5236375331878662] | Lr[2.0000000000000003e-06]
Step[40500] | Loss[0.7976970672607422] | Lr[2.0000000000000003e-06]
Step[40500] | Loss[0.42911839485168457] | Lr[2.0000000000000003e-06]
Step[40500] | Loss[0.7256731390953064] | Lr[2.0000000000000003e-06]
Step[41000] | Loss[0.5712034702301025] | Lr[2.0000000000000003e-06]
Step[41000] | Loss[0.7448678612709045] | Lr[2.0000000000000003e-06]
Step[41000] | Loss[0.7042555809020996] | Lr[2.0000000000000003e-06]
Step[41000] | Loss[0.5417572855949402] | Lr[2.0000000000000003e-06]
Step[41500] | Loss[1.0852346420288086] | Lr[2.0000000000000003e-06]
Step[41500] | Loss[0.6384364366531372] | Lr[2.0000000000000003e-06]
Step[41500] | Loss[0.7150088548660278] | Lr[2.0000000000000003e-06]
Step[41500] | Loss[1.0632387399673462] | Lr[2.0000000000000003e-06]
Step[42000] | Loss[0.785344123840332] | Lr[2.0000000000000003e-06]
Step[42000] | Loss[0.5746189951896667] | Lr[2.0000000000000003e-06]
Step[42000] | Loss[0.8142324090003967] | Lr[2.0000000000000003e-06]
Step[42000] | Loss[0.7365750074386597] | Lr[2.0000000000000003e-06]
Step[42500] | Loss[0.5832064151763916] | Lr[2.0000000000000003e-06]
Step[42500] | Loss[0.4786284565925598] | Lr[2.0000000000000003e-06]
Step[42500] | Loss[0.4710153639316559] | Lr[2.0000000000000003e-06]
Step[42500] | Loss[0.4016306400299072] | Lr[2.0000000000000003e-06]
Step[43000] | Loss[0.5163211822509766] | Lr[2.0000000000000003e-06]
Step[43000] | Loss[0.7936291098594666] | Lr[2.0000000000000003e-06]
Step[43000] | Loss[0.6436488032341003] | Lr[2.0000000000000003e-06]
Step[43000] | Loss[0.37007319927215576] | Lr[2.0000000000000003e-06]
Step[43500] | Loss[0.23406817018985748] | Lr[2.0000000000000003e-06]
Step[43500] | Loss[0.7395839691162109] | Lr[2.0000000000000003e-06]
Step[43500] | Loss[0.8508052825927734] | Lr[2.0000000000000003e-06]
Step[43500] | Loss[0.46305015683174133] | Lr[2.0000000000000003e-06]
Step[44000] | Loss[0.39786309003829956] | Lr[2.0000000000000003e-06]
Step[44000] | Loss[0.8577724695205688] | Lr[2.0000000000000003e-06]
Step[44000] | Loss[0.426067590713501] | Lr[2.0000000000000003e-06]
Step[44000] | Loss[0.7244324088096619] | Lr[2.0000000000000003e-06]
Step[44500] | Loss[0.576816737651825] | Lr[2.0000000000000003e-06]
Step[44500] | Loss[0.5534917712211609] | Lr[2.0000000000000003e-06]
Step[44500] | Loss[0.4369790852069855] | Lr[2.0000000000000003e-06]
Step[44500] | Loss[0.7442662715911865] | Lr[2.0000000000000003e-06]
Step[45000] | Loss[0.6642537713050842] | Lr[2.0000000000000003e-06]
Step[45000] | Loss[0.6185912489891052] | Lr[2.0000000000000003e-06]
Step[45000] | Loss[0.5414115190505981] | Lr[2.0000000000000003e-06]
Step[45000] | Loss[0.8824862241744995] | Lr[2.0000000000000003e-06]
Step[45500] | Loss[0.7640684843063354] | Lr[2.0000000000000003e-06]
Step[45500] | Loss[0.4417475461959839] | Lr[2.0000000000000003e-06]
Step[45500] | Loss[0.3815041184425354] | Lr[2.0000000000000003e-06]
Step[45500] | Loss[0.548376739025116] | Lr[2.0000000000000003e-06]
Step[46000] | Loss[0.6246971487998962] | Lr[2.0000000000000003e-06]
Step[46000] | Loss[1.062083125114441] | Lr[2.0000000000000003e-06]
Step[46000] | Loss[0.8509766459465027] | Lr[2.0000000000000003e-06]
Step[46000] | Loss[0.4287766218185425] | Lr[2.0000000000000003e-06]
Step[46500] | Loss[0.5169411301612854] | Lr[2.0000000000000003e-06]
Step[46500] | Loss[0.7218720316886902] | Lr[2.0000000000000003e-06]
Step[46500] | Loss[0.49370795488357544] | Lr[2.0000000000000003e-06]
Step[46500] | Loss[0.4145090878009796] | Lr[2.0000000000000003e-06]
Step[47000] | Loss[0.7628934383392334] | Lr[2.0000000000000003e-06]
Step[47000] | Loss[0.55461585521698] | Lr[2.0000000000000003e-06]
Step[47000] | Loss[0.7955804467201233] | Lr[2.0000000000000003e-06]
Step[47000] | Loss[0.5257983803749084] | Lr[2.0000000000000003e-06]
Step[47500] | Loss[0.5452322959899902] | Lr[2.0000000000000003e-06]
Step[47500] | Loss[0.7445462942123413] | Lr[2.0000000000000003e-06]
Step[47500] | Loss[0.5118976831436157] | Lr[2.0000000000000003e-06]
Step[47500] | Loss[0.8087496757507324] | Lr[2.0000000000000003e-06]
Step[48000] | Loss[0.6176226139068604] | Lr[2.0000000000000003e-06]
Step[48000] | Loss[0.8765885829925537] | Lr[2.0000000000000003e-06]
Step[48000] | Loss[0.5871573090553284] | Lr[2.0000000000000003e-06]
Step[48000] | Loss[0.7941994667053223] | Lr[2.0000000000000003e-06]
Step[48500] | Loss[0.2783096432685852] | Lr[2.0000000000000003e-06]
Step[48500] | Loss[0.6680894494056702] | Lr[2.0000000000000003e-06]
Step[48500] | Loss[1.1175376176834106] | Lr[2.0000000000000003e-06]
Step[48500] | Loss[0.46717533469200134] | Lr[2.0000000000000003e-06]
Step[49000] | Loss[0.8597520589828491] | Lr[2.0000000000000003e-06]
Step[49000] | Loss[0.45277905464172363] | Lr[2.0000000000000003e-06]
Step[49000] | Loss[0.7642521858215332] | Lr[2.0000000000000003e-06]
Step[49000] | Loss[0.9236699938774109] | Lr[2.0000000000000003e-06]
Step[49500] | Loss[0.40706437826156616] | Lr[2.0000000000000003e-06]
Step[49500] | Loss[0.4398270845413208] | Lr[2.0000000000000003e-06]
Step[49500] | Loss[0.8619036674499512] | Lr[2.0000000000000003e-06]
Step[49500] | Loss[0.4929106831550598] | Lr[2.0000000000000003e-06]
Step[50000] | Loss[0.7613096237182617] | Lr[2.0000000000000003e-06]
Step[50000] | Loss[0.6991377472877502] | Lr[2.0000000000000003e-06]
Step[50000] | Loss[0.8189089894294739] | Lr[2.0000000000000003e-06]
Step[50000] | Loss[0.5690525770187378] | Lr[2.0000000000000003e-06]
Step[50500] | Loss[0.5371968150138855] | Lr[2.0000000000000003e-06]
Step[50500] | Loss[0.7368228435516357] | Lr[2.0000000000000003e-06]
Step[50500] | Loss[0.4034787714481354] | Lr[2.0000000000000003e-06]
Step[50500] | Loss[0.4389721751213074] | Lr[2.0000000000000003e-06]
Step[51000] | Loss[0.649409830570221] | Lr[2.0000000000000003e-06]
Step[51000] | Loss[0.5956292152404785] | Lr[2.0000000000000003e-06]
Step[51000] | Loss[0.8938899040222168] | Lr[2.0000000000000003e-06]
Step[51000] | Loss[0.8957366943359375] | Lr[2.0000000000000003e-06]
Step[51500] | Loss[0.4767801761627197] | Lr[2.0000000000000003e-06]
Step[51500] | Loss[0.4485553801059723] | Lr[2.0000000000000003e-06]
Step[51500] | Loss[0.3528590202331543] | Lr[2.0000000000000003e-06]
Step[51500] | Loss[0.894875168800354] | Lr[2.0000000000000003e-06]
Step[52000] | Loss[0.8581035137176514] | Lr[2.0000000000000003e-06]
Step[52000] | Loss[0.6364285349845886] | Lr[2.0000000000000003e-06]
Step[52000] | Loss[0.5033098459243774] | Lr[2.0000000000000003e-06]
Step[52000] | Loss[0.3565067946910858] | Lr[2.0000000000000003e-06]
Step[52500] | Loss[0.553175151348114] | Lr[2.0000000000000003e-06]
Step[52500] | Loss[0.30334800481796265] | Lr[2.0000000000000003e-06]
Step[52500] | Loss[0.8103091716766357] | Lr[2.0000000000000003e-06]
Step[52500] | Loss[0.4163767397403717] | Lr[2.0000000000000003e-06]
Step[53000] | Loss[0.695054292678833] | Lr[2.0000000000000003e-06]
Step[53000] | Loss[0.6910613775253296] | Lr[2.0000000000000003e-06]
Step[53000] | Loss[0.6756348013877869] | Lr[2.0000000000000003e-06]
Step[53000] | Loss[0.5451455116271973] | Lr[2.0000000000000003e-06]
Step[53500] | Loss[0.5831465125083923] | Lr[2.0000000000000003e-06]
Step[53500] | Loss[0.49047285318374634] | Lr[2.0000000000000003e-06]
Step[53500] | Loss[0.8270334005355835] | Lr[2.0000000000000003e-06]
Step[53500] | Loss[0.4084545373916626] | Lr[2.0000000000000003e-06]
Step[54000] | Loss[0.3592973053455353] | Lr[2.0000000000000003e-06]
Step[54000] | Loss[0.599280834197998] | Lr[2.0000000000000003e-06]
Step[54000] | Loss[0.3714582026004791] | Lr[2.0000000000000003e-06]
Step[54000] | Loss[0.6731978058815002] | Lr[2.0000000000000003e-06]
Step[54500] | Loss[0.6304727792739868] | Lr[2.0000000000000003e-06]
Step[54500] | Loss[0.7642749547958374] | Lr[2.0000000000000003e-06]
Step[54500] | Loss[0.35457244515419006] | Lr[2.0000000000000003e-06]
Step[54500] | Loss[0.6453928351402283] | Lr[2.0000000000000003e-06]
Step[55000] | Loss[0.6565629839897156] | Lr[2.0000000000000003e-06]
Step[55000] | Loss[0.4354735314846039] | Lr[2.0000000000000003e-06]
Step[55000] | Loss[0.43892762064933777] | Lr[2.0000000000000003e-06]
Step[55000] | Loss[0.7522329688072205] | Lr[2.0000000000000003e-06]
Step[55500] | Loss[0.7426338791847229] | Lr[2.0000000000000003e-06]
Step[55500] | Loss[0.9572526216506958] | Lr[2.0000000000000003e-06]
Step[55500] | Loss[1.0282442569732666] | Lr[2.0000000000000003e-06]
Step[55500] | Loss[0.9417093992233276] | Lr[2.0000000000000003e-06]
Step[56000] | Loss[0.6782619953155518] | Lr[2.0000000000000003e-06]
Step[56000] | Loss[0.5816811323165894] | Lr[2.0000000000000003e-06]
Step[56000] | Loss[0.642170250415802] | Lr[2.0000000000000003e-06]
Step[56000] | Loss[0.6603296995162964] | Lr[2.0000000000000003e-06]
Step[56500] | Loss[0.5070359706878662] | Lr[2.0000000000000003e-06]
Step[56500] | Loss[0.42251068353652954] | Lr[2.0000000000000003e-06]
Step[56500] | Loss[0.30323490500450134] | Lr[2.0000000000000003e-06]
Step[56500] | Loss[0.37688395380973816] | Lr[2.0000000000000003e-06]
Step[57000] | Loss[0.5497591495513916] | Lr[2.0000000000000003e-06]
Step[57000] | Loss[0.44128891825675964] | Lr[2.0000000000000003e-06]
Step[57000] | Loss[0.5595390796661377] | Lr[2.0000000000000003e-06]
Step[57000] | Loss[0.679803729057312] | Lr[2.0000000000000003e-06]
Step[57500] | Loss[0.5247394442558289] | Lr[2.0000000000000003e-06]
Step[57500] | Loss[0.2614930272102356] | Lr[2.0000000000000003e-06]
Step[57500] | Loss[0.9115977883338928] | Lr[2.0000000000000003e-06]
Step[57500] | Loss[0.5354943871498108] | Lr[2.0000000000000003e-06]
Step[58000] | Loss[0.6934412121772766] | Lr[2.0000000000000003e-06]
Step[58000] | Loss[0.5563917756080627] | Lr[2.0000000000000003e-06]
Step[58000] | Loss[0.43749356269836426] | Lr[2.0000000000000003e-06]
Step[58000] | Loss[0.7302505970001221] | Lr[2.0000000000000003e-06]
Step[58500] | Loss[0.7081043720245361] | Lr[2.0000000000000003e-06]
Step[58500] | Loss[0.7353093028068542] | Lr[2.0000000000000003e-06]
Step[58500] | Loss[0.8078829646110535] | Lr[2.0000000000000003e-06]
Step[58500] | Loss[0.7693116068840027] | Lr[2.0000000000000003e-06]
Step[59000] | Loss[0.6155554056167603] | Lr[2.0000000000000003e-06]
Step[59000] | Loss[0.7162160277366638] | Lr[2.0000000000000003e-06]
Step[59000] | Loss[0.5243338942527771] | Lr[2.0000000000000003e-06]
Step[59000] | Loss[0.7651662826538086] | Lr[2.0000000000000003e-06]
Step[59500] | Loss[0.2707120478153229] | Lr[2.0000000000000003e-06]
Step[59500] | Loss[0.8200604319572449] | Lr[2.0000000000000003e-06]
Step[59500] | Loss[0.681423544883728] | Lr[2.0000000000000003e-06]
Step[59500] | Loss[0.28870102763175964] | Lr[2.0000000000000003e-06]
Step[60000] | Loss[0.550019383430481] | Lr[2.0000000000000003e-06]
Step[60000] | Loss[0.5902169346809387] | Lr[2.0000000000000003e-06]
Step[60000] | Loss[1.0486212968826294] | Lr[2.0000000000000003e-06]
Step[60000] | Loss[0.8192042112350464] | Lr[2.0000000000000003e-06]
Step[60500] | Loss[0.5106020569801331] | Lr[2.0000000000000003e-06]
Step[60500] | Loss[0.5393467545509338] | Lr[2.0000000000000003e-06]
Step[60500] | Loss[1.0003538131713867] | Lr[2.0000000000000003e-06]
Step[60500] | Loss[0.8144688010215759] | Lr[2.0000000000000003e-06]
Step[61000] | Loss[0.9729336500167847] | Lr[2.0000000000000003e-06]
Step[61000] | Loss[0.6331347227096558] | Lr[2.0000000000000003e-06]
Step[61000] | Loss[0.4326005280017853] | Lr[2.0000000000000003e-06]
Step[61000] | Loss[1.0086883306503296] | Lr[2.0000000000000003e-06]
Step[61500] | Loss[0.612632691860199] | Lr[2.0000000000000003e-06]
Step[61500] | Loss[0.6502904295921326] | Lr[2.0000000000000003e-06]
Step[61500] | Loss[0.7927278876304626] | Lr[2.0000000000000003e-06]
Step[61500] | Loss[0.583786129951477] | Lr[2.0000000000000003e-06]
Step[62000] | Loss[0.9591886401176453] | Lr[2.0000000000000003e-06]
Step[62000] | Loss[0.5080153942108154] | Lr[2.0000000000000003e-06]
Step[62000] | Loss[0.46604758501052856] | Lr[2.0000000000000003e-06]
Step[62000] | Loss[0.8445825576782227] | Lr[2.0000000000000003e-06]
Step[62500] | Loss[0.6216395497322083] | Lr[2.0000000000000003e-06]
Step[62500] | Loss[0.4903219938278198] | Lr[2.0000000000000003e-06]
Step[62500] | Loss[0.5700160264968872] | Lr[2.0000000000000003e-06]
Step[62500] | Loss[0.7308359146118164] | Lr[2.0000000000000003e-06]
Step[63000] | Loss[0.6136002540588379] | Lr[2.0000000000000003e-06]
Step[63000] | Loss[0.617648184299469] | Lr[2.0000000000000003e-06]
Step[63000] | Loss[0.7479811906814575] | Lr[2.0000000000000003e-06]
Step[63000] | Loss[0.3087461590766907] | Lr[2.0000000000000003e-06]
Step[63500] | Loss[0.7886953949928284] | Lr[2.0000000000000003e-06]
Step[63500] | Loss[0.23530830442905426] | Lr[2.0000000000000003e-06]
Step[63500] | Loss[0.3723713755607605] | Lr[2.0000000000000003e-06]
Step[63500] | Loss[0.39243125915527344] | Lr[2.0000000000000003e-06]
Step[64000] | Loss[0.4799079895019531] | Lr[2.0000000000000003e-06]
Step[64000] | Loss[0.792602002620697] | Lr[2.0000000000000003e-06]
Step[64000] | Loss[0.463969886302948] | Lr[2.0000000000000003e-06]
Step[64000] | Loss[0.5408937335014343] | Lr[2.0000000000000003e-06]
Step[64500] | Loss[0.5262941718101501] | Lr[2.0000000000000003e-06]
Step[64500] | Loss[0.6454199552536011] | Lr[2.0000000000000003e-06]
Step[64500] | Loss[1.0261582136154175] | Lr[2.0000000000000003e-06]
Step[64500] | Loss[0.45475199818611145] | Lr[2.0000000000000003e-06]
Step[65000] | Loss[0.3883613646030426] | Lr[2.0000000000000003e-06]
Step[65000] | Loss[0.6937079429626465] | Lr[2.0000000000000003e-06]
Step[65000] | Loss[0.448378324508667] | Lr[2.0000000000000003e-06]
Step[65000] | Loss[0.6265149116516113] | Lr[2.0000000000000003e-06]
Step[65500] | Loss[0.5912060737609863] | Lr[2.0000000000000003e-06]
Step[65500] | Loss[0.9264715909957886] | Lr[2.0000000000000003e-06]
Step[65500] | Loss[0.8167271614074707] | Lr[2.0000000000000003e-06]
Step[65500] | Loss[0.49036940932273865] | Lr[2.0000000000000003e-06]
Step[66000] | Loss[0.5280231237411499] | Lr[2.0000000000000003e-06]Step[66000] | Loss[0.7085389494895935] | Lr[2.0000000000000003e-06]

Step[66000] | Loss[0.35245445370674133] | Lr[2.0000000000000003e-06]
Step[66000] | Loss[0.6225136518478394] | Lr[2.0000000000000003e-06]
Step[66500] | Loss[0.658784806728363] | Lr[2.0000000000000003e-06]
Step[66500] | Loss[0.7528160214424133] | Lr[2.0000000000000003e-06]
Step[66500] | Loss[0.4680238664150238] | Lr[2.0000000000000003e-06]
Step[66500] | Loss[0.8345903158187866] | Lr[2.0000000000000003e-06]
Step[67000] | Loss[0.8850274682044983] | Lr[2.0000000000000003e-06]
Step[67000] | Loss[0.5634154081344604] | Lr[2.0000000000000003e-06]
Step[67000] | Loss[0.4568592607975006] | Lr[2.0000000000000003e-06]
Step[67000] | Loss[0.37004825472831726] | Lr[2.0000000000000003e-06]
Step[67500] | Loss[0.3554394245147705] | Lr[2.0000000000000003e-06]
Step[67500] | Loss[0.6534774303436279] | Lr[2.0000000000000003e-06]
Step[67500] | Loss[0.8415579199790955] | Lr[2.0000000000000003e-06]
Step[67500] | Loss[0.5216978192329407] | Lr[2.0000000000000003e-06]
Step[68000] | Loss[0.6157024502754211] | Lr[2.0000000000000003e-06]
Step[68000] | Loss[0.5991103649139404] | Lr[2.0000000000000003e-06]
Step[68000] | Loss[0.7876890897750854] | Lr[2.0000000000000003e-06]
Step[68000] | Loss[0.5857040286064148] | Lr[2.0000000000000003e-06]
Step[68500] | Loss[0.5143972039222717] | Lr[2.0000000000000003e-06]
Step[68500] | Loss[0.8034474849700928] | Lr[2.0000000000000003e-06]
Step[68500] | Loss[0.6460837125778198] | Lr[2.0000000000000003e-06]
Step[68500] | Loss[0.265630841255188] | Lr[2.0000000000000003e-06]
Step[69000] | Loss[0.7387132048606873] | Lr[2.0000000000000003e-06]
Step[69000] | Loss[0.43734198808670044] | Lr[2.0000000000000003e-06]
Step[69000] | Loss[0.5743398666381836] | Lr[2.0000000000000003e-06]
Step[69000] | Loss[0.3447301983833313] | Lr[2.0000000000000003e-06]
Step[69500] | Loss[0.8059068918228149] | Lr[2.0000000000000003e-06]
Step[69500] | Loss[1.2749087810516357] | Lr[2.0000000000000003e-06]
Step[69500] | Loss[0.8015250563621521] | Lr[2.0000000000000003e-06]
Step[69500] | Loss[0.7245673537254333] | Lr[2.0000000000000003e-06]
Step[70000] | Loss[0.49863219261169434] | Lr[2.0000000000000003e-06]
Step[70000] | Loss[0.5510068535804749] | Lr[2.0000000000000003e-06]
Step[70000] | Loss[0.605803370475769] | Lr[2.0000000000000003e-06]
Step[70000] | Loss[0.4415919780731201] | Lr[2.0000000000000003e-06]
Step[70500] | Loss[0.6713136434555054] | Lr[2.0000000000000003e-06]
Step[70500] | Loss[0.7849418520927429] | Lr[2.0000000000000003e-06]
Step[70500] | Loss[0.8028671145439148] | Lr[2.0000000000000003e-06]
Step[70500] | Loss[1.139401912689209] | Lr[2.0000000000000003e-06]
Step[71000] | Loss[1.0238289833068848] | Lr[2.0000000000000003e-06]
Step[71000] | Loss[0.669050395488739] | Lr[2.0000000000000003e-06]
Step[71000] | Loss[0.5018389225006104] | Lr[2.0000000000000003e-06]
Step[71000] | Loss[0.3048997223377228] | Lr[2.0000000000000003e-06]
Step[71500] | Loss[0.5044048428535461] | Lr[2.0000000000000003e-06]
Step[71500] | Loss[0.7374786734580994] | Lr[2.0000000000000003e-06]
Step[71500] | Loss[0.3704323470592499] | Lr[2.0000000000000003e-06]
Step[71500] | Loss[0.46131154894828796] | Lr[2.0000000000000003e-06]
Step[72000] | Loss[0.7088894844055176] | Lr[2.0000000000000003e-06]
Step[72000] | Loss[0.4392501711845398] | Lr[2.0000000000000003e-06]
Step[72000] | Loss[0.47352030873298645] | Lr[2.0000000000000003e-06]Step[72000] | Loss[0.5693589448928833] | Lr[2.0000000000000003e-06]

Step[72500] | Loss[0.5617225170135498] | Lr[2.0000000000000003e-06]
Step[72500] | Loss[0.6254271268844604] | Lr[2.0000000000000003e-06]
Step[72500] | Loss[0.7132077217102051] | Lr[2.0000000000000003e-06]
Step[72500] | Loss[0.5056180953979492] | Lr[2.0000000000000003e-06]
Step[73000] | Loss[0.6800814270973206] | Lr[2.0000000000000003e-06]
Step[73000] | Loss[0.5422036051750183] | Lr[2.0000000000000003e-06]
Step[73000] | Loss[1.0550010204315186] | Lr[2.0000000000000003e-06]
Step[73000] | Loss[0.6844496726989746] | Lr[2.0000000000000003e-06]
Step[73500] | Loss[0.5301172733306885] | Lr[2.0000000000000003e-06]
Step[73500] | Loss[0.6208124160766602] | Lr[2.0000000000000003e-06]
Step[73500] | Loss[0.5694054365158081] | Lr[2.0000000000000003e-06]
Step[73500] | Loss[0.539237380027771] | Lr[2.0000000000000003e-06]
Step[74000] | Loss[0.9100900888442993] | Lr[2.0000000000000003e-06]
Step[74000] | Loss[0.8224611878395081] | Lr[2.0000000000000003e-06]
Step[74000] | Loss[0.7902519106864929] | Lr[2.0000000000000003e-06]
Step[74000] | Loss[0.6058018803596497] | Lr[2.0000000000000003e-06]
Step[74500] | Loss[1.0187045335769653] | Lr[2.0000000000000003e-06]
Step[74500] | Loss[0.5480664372444153] | Lr[2.0000000000000003e-06]
Step[74500] | Loss[0.5085527300834656] | Lr[2.0000000000000003e-06]
Step[74500] | Loss[0.4637884199619293] | Lr[2.0000000000000003e-06]
Step[75000] | Loss[0.6064144372940063] | Lr[2.0000000000000003e-06]
Step[75000] | Loss[0.5527693629264832] | Lr[2.0000000000000003e-06]
Step[75000] | Loss[0.5583059787750244] | Lr[2.0000000000000003e-06]
Step[75000] | Loss[0.793307900428772] | Lr[2.0000000000000003e-06]
Step[75500] | Loss[0.3898647725582123] | Lr[2.0000000000000003e-06]
Step[75500] | Loss[0.5151322484016418] | Lr[2.0000000000000003e-06]
Step[75500] | Loss[0.6444668173789978] | Lr[2.0000000000000003e-06]
Step[75500] | Loss[0.49202412366867065] | Lr[2.0000000000000003e-06]
Step[76000] | Loss[0.8006922602653503] | Lr[2.0000000000000003e-06]
Step[76000] | Loss[0.425049364566803] | Lr[2.0000000000000003e-06]
Step[76000] | Loss[0.5498999953269958] | Lr[2.0000000000000003e-06]
Step[76000] | Loss[0.7762089967727661] | Lr[2.0000000000000003e-06]
Step[76500] | Loss[0.7177577614784241] | Lr[2.0000000000000003e-06]
Step[76500] | Loss[0.8614367246627808] | Lr[2.0000000000000003e-06]
Step[76500] | Loss[0.4698689579963684] | Lr[2.0000000000000003e-06]
Step[76500] | Loss[0.6393794417381287] | Lr[2.0000000000000003e-06]
Step[77000] | Loss[0.6192963719367981] | Lr[2.0000000000000003e-06]
Step[77000] | Loss[0.627411961555481] | Lr[2.0000000000000003e-06]
Step[77000] | Loss[0.4882442057132721] | Lr[2.0000000000000003e-06]
Step[77000] | Loss[0.6272397637367249] | Lr[2.0000000000000003e-06]
Step[77500] | Loss[0.9071159362792969] | Lr[2.0000000000000003e-06]
Step[77500] | Loss[0.4733436703681946] | Lr[2.0000000000000003e-06]
Step[77500] | Loss[0.4151151180267334] | Lr[2.0000000000000003e-06]
Step[77500] | Loss[0.7023534178733826] | Lr[2.0000000000000003e-06]
Step[78000] | Loss[0.4562153220176697] | Lr[2.0000000000000003e-06]
Step[78000] | Loss[0.7941567301750183] | Lr[2.0000000000000003e-06]
Step[78000] | Loss[0.5061304569244385] | Lr[2.0000000000000003e-06]
Step[78000] | Loss[0.8573664426803589] | Lr[2.0000000000000003e-06]
Labels:  tensor([2, 2, 1, 1, 1, 4, 4, 4, 3, 2, 0, 3, 3, 4, 0, 3], device='cuda:0')
Preds:  tensor([4, 3, 3, 1, 0, 4, 4, 4, 2, 2, 2, 2, 3, 4, 0, 3], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0002,     0.0139,     0.4763,     0.5095],
        [    0.0004,     0.0051,     0.3087,     0.6810,     0.0048],
        [    0.0047,     0.0321,     0.3627,     0.5026,     0.0980],
        [    0.1012,     0.4844,     0.3999,     0.0140,     0.0005],
        [    0.6095,     0.3723,     0.0179,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0008,     0.0548,     0.9442],
        [    0.0004,     0.0002,     0.0013,     0.0421,     0.9559],
        [    0.0002,     0.0003,     0.0050,     0.1153,     0.8792],
        [    0.0019,     0.0250,     0.8658,     0.0961,     0.0112],
        [    0.0042,     0.0431,     0.5325,     0.4118,     0.0085],
        [    0.0408,     0.3986,     0.5091,     0.0478,     0.0036],
        [    0.0056,     0.0786,     0.7058,     0.1769,     0.0331],
        [    0.0007,     0.0050,     0.1327,     0.5539,     0.3078],
        [    0.0002,     0.0016,     0.0035,     0.2451,     0.7496],
        [    0.9358,     0.0625,     0.0016,     0.0000,     0.0000],
        [    0.0001,     0.0002,     0.0069,     0.8888,     0.1040]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 1, 1, 4, 0, 2, 4, 1, 3, 0, 2, 3, 2, 2, 4, 4], device='cuda:0')
Labels:  tensor([0, 4, 2, 3, 0, 1, 2, 2, 3, 2, 2, 4, 4, 1, 1, 2], device='cuda:1')
Preds:  tensor([3, 1, 0, 4, 0, 2, 4, 0, 3, 0, 1, 4, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([0, 4, 1, 4, 0, 2, 3, 3, 2, 2, 0, 4, 4, 2, 1, 4], device='cuda:1')
Outputs:  Labels:  tensor([3, 0, 0, 1, 0, 3, 0, 0, 3, 1, 4, 4, 4, 1, 2, 0], device='cuda:1')
Outputs:  tensor([[    0.8679,     0.1276,     0.0044,     0.0001,     0.0000],
        [    0.0002,     0.0002,     0.0013,     0.1010,     0.8974],
        [    0.0347,     0.6961,     0.2285,     0.0335,     0.0073],
        [    0.0001,     0.0000,     0.0001,     0.0139,     0.9859],
        [    0.9849,     0.0131,     0.0015,     0.0002,     0.0002],
        [    0.0538,     0.3873,     0.4355,     0.1155,     0.0079],
        [    0.0154,     0.1366,     0.2550,     0.3148,     0.2783],
        [    0.0003,     0.0035,     0.3240,     0.6374,     0.0348],
        [    0.1058,     0.3883,     0.4749,     0.0304,     0.0005],
        [    0.0035,     0.0534,     0.6814,     0.2561,     0.0056],
        [    0.7360,     0.2277,     0.0344,     0.0013,     0.0007],
        [    0.0001,     0.0001,     0.0005,     0.0385,     0.9609],
        [    0.0004,     0.0002,     0.0004,     0.0024,     0.9966],
        [    0.0130,     0.3808,     0.5064,     0.0939,     0.0060],
Preds:  tensor([4, 1, 2, 1, 0, 2, 0, 1, 3, 1, 4, 3, 4, 1, 2, 0], device='cuda:1')
        [    0.0036,     0.9911,     0.0046,     0.0004,     0.0002],
        [    0.0145,     0.0052,     0.0412,     0.2152,     0.7239]],
       device='cuda:1')
Outputs:  tensor([[    0.0002,     0.0005,     0.0173,     0.9815,     0.0005],
        [    0.4417,     0.4541,     0.1026,     0.0013,     0.0003],
        [    0.7460,     0.2149,     0.0361,     0.0025,     0.0005],
        [    0.0017,     0.0007,     0.0042,     0.0565,     0.9370],
        [    0.8583,     0.0647,     0.0403,     0.0187,     0.0179],
        [    0.0015,     0.0906,     0.6603,     0.2406,     0.0069],
        [    0.0032,     0.0020,     0.0089,     0.0881,     0.8978],
        [    0.5450,     0.4260,     0.0287,     0.0002,     0.0001],
        [    0.0001,     0.0011,     0.2225,     0.7572,     0.0191],
        [    0.6956,     0.2254,     0.0768,     0.0022,     0.0001],
        [    0.3606,     0.5012,     0.1358,     0.0020,     0.0004],
        [    0.0008,     0.0007,     0.0184,     0.4664,     0.5136],
        [    0.0220,     0.6748,     0.3018,     0.0014,     0.0000],
        [    0.0065,     0.0682,     0.6177,     0.2878,     0.0198],
Metric:  tensor(0.4375, device='cuda:1')
        [    0.0001,     0.0002,     0.0069,     0.6011,     0.3918],
        [    0.0024,     0.0009,     0.0077,     0.2235,     0.7655]],
       device='cuda:0')
------------------------
Metric:  tensor(0.6250, device='cuda:0')
------------------------
tensor([[    0.0004,     0.0004,     0.0076,     0.3150,     0.6767],
        [    0.4009,     0.5081,     0.0900,     0.0008,     0.0002],
        [    0.1339,     0.3395,     0.4960,     0.0281,     0.0026],
        [    0.4290,     0.4756,     0.0931,     0.0018,     0.0004],
        [    0.8112,     0.1662,     0.0223,     0.0002,     0.0001],
        [    0.0016,     0.0229,     0.7647,     0.2070,     0.0037],
        [    0.9448,     0.0527,     0.0023,     0.0001,     0.0001],
        [    0.4223,     0.5543,     0.0225,     0.0007,     0.0002],
        [    0.0002,     0.0044,     0.3840,     0.5336,     0.0778],
        [    0.1993,     0.4316,     0.3496,     0.0175,     0.0021],
        [    0.1786,     0.1146,     0.2395,     0.1751,     0.2922],
        [    0.0429,     0.0353,     0.2942,     0.5234,     0.1042],
        [    0.0003,     0.0003,     0.0031,     0.0785,     0.9179],
        [    0.1763,     0.7041,     0.1083,     0.0095,     0.0017],
        [    0.0009,     0.0433,     0.9522,     0.0035,     0.0001],
        [    0.9770,     0.0223,     0.0004,     0.0001,     0.0001]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([0, 1, 4, 2, 3, 2, 1, 0, 2, 2, 2, 0, 2, 4, 3, 4], device='cuda:0')
Preds:  tensor([0, 1, 4, 1, 2, 2, 1, 0, 1, 3, 3, 0, 3, 4, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.4852,     0.4841,     0.0305,     0.0001,     0.0000],
        [    0.1188,     0.8752,     0.0057,     0.0002,     0.0001],
        [    0.0005,     0.0010,     0.0159,     0.1970,     0.7857],
        [    0.3059,     0.5369,     0.1538,     0.0031,     0.0004],
        [    0.1898,     0.3387,     0.4195,     0.0382,     0.0138],
        [    0.0404,     0.0255,     0.9098,     0.0203,     0.0040],
        [    0.3164,     0.5626,     0.1204,     0.0005,     0.0001],
        [    0.9595,     0.0332,     0.0051,     0.0010,     0.0012],
        [    0.3968,     0.4073,     0.1923,     0.0033,     0.0004],
        [    0.0004,     0.0036,     0.2220,     0.6604,     0.1136],
        [    0.0011,     0.0140,     0.4523,     0.4526,     0.0800],
        [    0.9506,     0.0475,     0.0018,     0.0002,     0.0001],
        [    0.0004,     0.0094,     0.1863,     0.7812,     0.0227],
        [    0.0049,     0.0025,     0.0284,     0.2212,     0.7431],
        [    0.6528,     0.2834,     0.0627,     0.0011,     0.0001],
        [    0.0002,     0.0002,     0.0003,     0.0119,     0.9874]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 1, 1, 2, 1, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Preds:  tensor([3, 0, 1, 4, 1, 2, 0, 0, 4, 4, 2, 1, 2, 0, 0, 1], device='cuda:0')
Outputs:  tensor([[    0.0039,     0.0329,     0.3305,     0.4735,     0.1592],
        [    0.3718,     0.3531,     0.2685,     0.0061,     0.0006],
        [    0.0663,     0.5414,     0.3866,     0.0054,     0.0002],
        [    0.0374,     0.0260,     0.0743,     0.1757,     0.6866],
        [    0.0256,     0.6549,     0.2823,     0.0316,     0.0055],
        [    0.0074,     0.1410,     0.8049,     0.0454,     0.0013],
        [    0.9370,     0.0560,     0.0044,     0.0009,     0.0017],
        [    0.5827,     0.4070,     0.0101,     0.0001,     0.0001],
        [    0.0003,     0.0002,     0.0022,     0.0972,     0.9001],
        [    0.0007,     0.0004,     0.0035,     0.0610,     0.9344],
        [    0.0053,     0.1270,     0.7850,     0.0818,     0.0009],
        [    0.3997,     0.4673,     0.1293,     0.0027,     0.0010],
        [    0.0011,     0.0167,     0.7627,     0.2192,     0.0002],
        [    0.6026,     0.3300,     0.0634,     0.0038,     0.0002],
        [    0.5512,     0.2944,     0.1105,     0.0249,     0.0190],
        [    0.2896,     0.5030,     0.2039,     0.0034,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 1, 3, 4, 3, 0, 3, 1, 1, 0, 0, 3, 2, 0], device='cuda:1')
Preds:  tensor([3, 2, 3, 1, 2, 4, 2, 1, 4, 1, 1, 0, 0, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0021,     0.0105,     0.3312,     0.6467,     0.0094],
        [    0.0013,     0.0386,     0.7215,     0.2340,     0.0046],
        [    0.0082,     0.0055,     0.0594,     0.9240,     0.0027],
        [    0.3286,     0.4504,     0.2153,     0.0054,     0.0004],
        [    0.0009,     0.0264,     0.9401,     0.0321,     0.0005],
        [    0.0008,     0.0008,     0.0129,     0.1717,     0.8138],
        [    0.0112,     0.0876,     0.5494,     0.3161,     0.0356],
        [    0.1930,     0.5320,     0.2731,     0.0017,     0.0001],
        [    0.0001,     0.0001,     0.0047,     0.3525,     0.6426],
        [    0.1634,     0.5183,     0.2879,     0.0261,     0.0043],
        [    0.1355,     0.4924,     0.3660,     0.0058,     0.0003],
        [    0.8450,     0.1465,     0.0081,     0.0002,     0.0001],
        [    0.6152,     0.2789,     0.0964,     0.0077,     0.0017],
        [    0.0115,     0.0542,     0.2220,     0.5495,     0.1627],
        [    0.0226,     0.1624,     0.6469,     0.1609,     0.0071],
        [    0.0144,     0.1370,     0.6873,     0.1603,     0.0011]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 0, 4, 0, 0, 0, 1, 2], device='cuda:1')
Preds:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 2, 4, 0, 0, 0, 2, 4], device='cuda:1')
Outputs:  tensor([[    0.0007,     0.0031,     0.9748,     0.0182,     0.0033],
        [    0.6166,     0.3316,     0.0469,     0.0036,     0.0014],
        [    0.0004,     0.0022,     0.0012,     0.0182,     0.9781],
        [    0.0001,     0.0000,     0.0001,     0.0140,     0.9857],
        [    0.0003,     0.0003,     0.0077,     0.2133,     0.7784],
        [    0.6520,     0.3196,     0.0264,     0.0013,     0.0008],
        [    0.8423,     0.1481,     0.0091,     0.0003,     0.0001],
        [    0.0436,     0.2731,     0.6552,     0.0272,     0.0008],
        [    0.0002,     0.0002,     0.0053,     0.1685,     0.8257],
        [    0.0602,     0.4331,     0.4959,     0.0103,     0.0005],
        [    0.0036,     0.0037,     0.0638,     0.4627,     0.4662],
        [    0.5081,     0.2252,     0.1761,     0.0456,     0.0450],
        [    0.5305,     0.4366,     0.0324,     0.0004,     0.0001],
        [    0.8857,     0.1133,     0.0009,     0.0000,     0.0001],
        [    0.0120,     0.2454,     0.7151,     0.0273,     0.0002],
        [    0.0512,     0.0523,     0.0970,     0.1085,     0.6909]],
       device='cuda:1')
Metric:  tensor(0.8125, device='cuda:1')
------------------------
Labels:  tensor([3, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 3, 3, 4, 4, 2], device='cuda:0')
Preds:  tensor([3, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 4, 3, 4, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0003,     0.0129,     0.6258,     0.3610],
        [    0.0014,     0.0196,     0.6940,     0.2574,     0.0276],
        [    0.0013,     0.0261,     0.6617,     0.3032,     0.0077],
        [    0.0225,     0.2019,     0.7039,     0.0707,     0.0011],
        [    0.3644,     0.5745,     0.0606,     0.0005,     0.0001],
        [    0.0539,     0.0891,     0.4032,     0.3345,     0.1192],
        [    0.1810,     0.6361,     0.1792,     0.0035,     0.0002],
        [    0.9775,     0.0214,     0.0009,     0.0001,     0.0001],
        [    0.0006,     0.0153,     0.7367,     0.2431,     0.0043],
        [    0.0371,     0.2415,     0.4003,     0.2771,     0.0440],
        [    0.7164,     0.2543,     0.0291,     0.0002,     0.0001],
        [    0.0000,     0.0000,     0.0002,     0.0128,     0.9869],
        [    0.0003,     0.0075,     0.4560,     0.5331,     0.0031],
        [    0.0002,     0.0004,     0.0078,     0.2469,     0.7448],
        [    0.0073,     0.0120,     0.0951,     0.1634,     0.7222],
        [    0.6931,     0.2702,     0.0356,     0.0009,     0.0002]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([4, 0, 4, 0, 4, 1, 1, 3, 0, 4, 4, 3, 0, 2, 3, 0], device='cuda:0')
Preds:  tensor([4, 0, 4, 0, 4, 1, 2, 4, 0, 3, 4, 3, 0, 1, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0003,     0.0002,     0.0019,     0.0873,     0.9103],
        [    0.6622,     0.2865,     0.0498,     0.0013,     0.0002],
        [    0.0003,     0.0003,     0.0058,     0.2377,     0.7560],
        [    0.8743,     0.1208,     0.0047,     0.0001,     0.0001],
        [    0.0577,     0.0242,     0.0378,     0.0827,     0.7976],
        [    0.0040,     0.9913,     0.0043,     0.0003,     0.0002],
        [    0.0838,     0.3969,     0.4852,     0.0324,     0.0017],
        [    0.0163,     0.0744,     0.1471,     0.2875,     0.4747],
        [    0.9798,     0.0195,     0.0006,     0.0001,     0.0001],
        [    0.0012,     0.0059,     0.1656,     0.5196,     0.3077],
        [    0.0010,     0.0078,     0.0055,     0.1657,     0.8199],
        [    0.0011,     0.0043,     0.1071,     0.5184,     0.3691],
        [    0.7641,     0.2287,     0.0070,     0.0001,     0.0000],
        [    0.0409,     0.4783,     0.4216,     0.0579,     0.0013],
        [    0.0001,     0.0002,     0.0037,     0.2944,     0.7016],
        [    0.8505,     0.1415,     0.0077,     0.0002,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 2, 3, 0, 2, 4, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:1')
Preds:  tensor([4, 2, 2, 2, 4, 0, 1, 4, 2, 0, 1, 2, 0, 0, 0, 0], device='cuda:1')
Outputs:  tensor([[    0.0002,     0.0005,     0.0094,     0.2506,     0.7393],
        [    0.0018,     0.0354,     0.8242,     0.1366,     0.0020],
        [    0.0012,     0.0285,     0.8882,     0.0812,     0.0008],
        [    0.0030,     0.0250,     0.9705,     0.0013,     0.0002],
        [    0.0002,     0.0002,     0.0023,     0.0903,     0.9070],
        [    0.7712,     0.2165,     0.0119,     0.0002,     0.0001],
        [    0.2007,     0.4406,     0.2903,     0.0600,     0.0085],
        [    0.0008,     0.0012,     0.0079,     0.1264,     0.8637],
        [    0.0081,     0.1277,     0.8042,     0.0595,     0.0005],
        [    0.7115,     0.1706,     0.0694,     0.0217,     0.0268],
        [    0.2112,     0.5833,     0.2032,     0.0023,     0.0001],
        [    0.2546,     0.2820,     0.3934,     0.0564,     0.0136],
        [    0.9946,     0.0033,     0.0009,     0.0003,     0.0008],
        [    0.8502,     0.1131,     0.0248,     0.0054,     0.0065],
        [    0.9896,     0.0101,     0.0002,     0.0001,     0.0001],
        [    0.9971,     0.0019,     0.0003,     0.0002,     0.0005]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([2, 3, 4, 0, 1, 3, 4, 3, 4, 2, 1, 0, 4, 4, 4, 4], device='cuda:1')
Preds:  tensor([2, 3, 4, 1, 1, 4, 4, 2, 4, 3, 1, 0, 4, 4, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0341,     0.3595,     0.5922,     0.0137,     0.0005],
        [    0.0002,     0.0013,     0.1247,     0.7507,     0.1230],
        [    0.0006,     0.0003,     0.0011,     0.0138,     0.9841],
        [    0.2413,     0.7145,     0.0434,     0.0005,     0.0003],
        [    0.3340,     0.5226,     0.1319,     0.0089,     0.0027],
        [    0.0010,     0.0013,     0.0260,     0.3563,     0.6154],
        [    0.0006,     0.0003,     0.0015,     0.0116,     0.9859],
        [    0.0069,     0.2879,     0.6926,     0.0124,     0.0002],
        [    0.0004,     0.0004,     0.0001,     0.0021,     0.9970],
        [    0.0002,     0.0037,     0.3892,     0.5959,     0.0109],
        [    0.3454,     0.5281,     0.1244,     0.0018,     0.0002],
        [    0.8803,     0.1144,     0.0050,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0017,     0.0911,     0.9069],
        [    0.0009,     0.0006,     0.0038,     0.0945,     0.9003],
        [    0.0013,     0.0053,     0.2042,     0.6317,     0.1575],
        [    0.0002,     0.0001,     0.0013,     0.1112,     0.8872]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([4, 1, 1, 0, 0, 1, 2, 4, 2, 4, 4, 1, 0, 1, 2, 1], device='cuda:0')
Preds:  tensor([4, 1, 0, 0, 0, 1, 2, 4, 2, 4, 4, 4, 0, 0, 3, 0], device='cuda:0')
Outputs:  tensor([[    0.0008,     0.0007,     0.0029,     0.0406,     0.9550],
        [    0.3566,     0.5751,     0.0677,     0.0005,     0.0001],
        [    0.9619,     0.0371,     0.0010,     0.0001,     0.0001],
        [    0.6499,     0.3232,     0.0250,     0.0010,     0.0008],
        [    0.7324,     0.2451,     0.0210,     0.0009,     0.0006],
        [    0.1262,     0.6147,     0.2573,     0.0017,     0.0001],
        [    0.0637,     0.1623,     0.4617,     0.2790,     0.0333],
        [    0.0001,     0.0003,     0.0006,     0.1302,     0.8688],
        [    0.0158,     0.2804,     0.6811,     0.0225,     0.0001],
        [    0.0006,     0.0005,     0.0046,     0.1590,     0.8353],
        [    0.0073,     0.0085,     0.0246,     0.0855,     0.8740],
        [    0.3072,     0.0615,     0.0904,     0.1132,     0.4276],
        [    0.9547,     0.0445,     0.0007,     0.0001,     0.0000],
        [    0.6293,     0.3050,     0.0632,     0.0022,     0.0003],
        [    0.0314,     0.0528,     0.2380,     0.5773,     0.1006],
        [    0.6486,     0.2649,     0.0725,     0.0103,     0.0037]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([2, 0, 1, 3, 4, 2, 0, 0, 2, 1, 0, 1, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([4, 0, 2, 2, 4, 2, 0, 0, 1, 0, 0, 0, 0, 1, 3, 4], device='cuda:0')
Labels:  tensor([1, 1, 2, 1, 1, 4, 1, 4, 4, 1, 3, 2, 4, 3, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0013,     0.0289,     0.3143,     0.6551],
        [    0.7394,     0.2424,     0.0173,     0.0007,     0.0003],
        [    0.0023,     0.0720,     0.8642,     0.0613,     0.0002],
        [    0.0016,     0.0566,     0.8852,     0.0564,     0.0003],
        [    0.0054,     0.0011,     0.0025,     0.0151,     0.9758],
        [    0.0053,     0.1138,     0.6413,     0.2349,     0.0048],
        [    0.5065,     0.4463,     0.0463,     0.0008,     0.0001],
        [    0.7697,     0.2026,     0.0274,     0.0002,     0.0000],
        [    0.0342,     0.9024,     0.0526,     0.0060,     0.0048],
        [    0.7455,     0.1521,     0.0857,     0.0116,     0.0051],
        [    0.9929,     0.0067,     0.0002,     0.0000,     0.0001],
        [    0.6860,     0.2440,     0.0680,     0.0019,     0.0001],
        [    0.7650,     0.1662,     0.0583,     0.0077,     0.0028],
        [    0.0081,     0.6840,     0.2715,     0.0345,     0.0020],
Preds:  tensor([3, 1, 1, 0, 1, 4, 0, 4, 4, 1, 2, 2, 4, 2, 4, 3], device='cuda:1')
        [    0.0626,     0.1074,     0.2849,     0.3078,     0.2374],
        [    0.0003,     0.0005,     0.0003,     0.0272,     0.9717]],
       device='cuda:0')
Outputs:  tensor([[    0.0158,     0.0557,     0.4183,     0.4192,     0.0910],
        [    0.1354,     0.4968,     0.3550,     0.0124,     0.0004],
        [    0.1719,     0.3853,     0.3797,     0.0623,     0.0008],
        [    0.6445,     0.2948,     0.0598,     0.0008,     0.0001],
        [    0.3006,     0.6511,     0.0481,     0.0002,     0.0000],
        [    0.0018,     0.0007,     0.0023,     0.0632,     0.9320],
        [    0.7654,     0.1999,     0.0325,     0.0015,     0.0008],
        [    0.0002,     0.0004,     0.0127,     0.2541,     0.7326],
        [    0.0023,     0.0009,     0.0032,     0.0839,     0.9097],
        [    0.4421,     0.5044,     0.0527,     0.0008,     0.0001],
        [    0.0028,     0.0560,     0.7915,     0.1493,     0.0004],
        [    0.0009,     0.0294,     0.5438,     0.4175,     0.0084],
        [    0.0005,     0.0007,     0.0076,     0.2055,     0.7857],
        [    0.0003,     0.0080,     0.6465,     0.3417,     0.0035],
Metric:  tensor(0.5000, device='cuda:0')
        [    0.0004,     0.0005,     0.0093,     0.1509,     0.8388],
        [    0.0003,     0.0005,     0.0226,     0.5282,     0.4485]],
       device='cuda:1')
------------------------
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 4, 2, 3, 2, 4, 1, 2, 1, 1, 2, 0, 1, 0, 1], device='cuda:1')
Preds:  tensor([4, 4, 4, 1, 3, 3, 4, 1, 2, 2, 0, 2, 0, 0, 0, 2], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0004,     0.0034,     0.0743,     0.9216],
        [    0.0003,     0.0003,     0.0048,     0.2205,     0.7741],
        [    0.0039,     0.0010,     0.0072,     0.2044,     0.7835],
        [    0.2044,     0.4478,     0.3432,     0.0041,     0.0004],
        [    0.0026,     0.0184,     0.4109,     0.4767,     0.0914],
        [    0.0001,     0.0004,     0.0510,     0.8543,     0.0941],
        [    0.0009,     0.0005,     0.0012,     0.0060,     0.9914],
        [    0.0796,     0.4439,     0.4329,     0.0414,     0.0022],
        [    0.0015,     0.0534,     0.8640,     0.0808,     0.0002],
        [    0.0350,     0.2951,     0.6216,     0.0472,     0.0011],
        [    0.7416,     0.2153,     0.0374,     0.0050,     0.0007],
        [    0.0098,     0.3009,     0.6741,     0.0147,     0.0005],
        [    0.9776,     0.0217,     0.0004,     0.0001,     0.0002],
        [    0.7379,     0.2413,     0.0201,     0.0005,     0.0001],
        [    0.7434,     0.2428,     0.0134,     0.0003,     0.0001],
        [    0.0080,     0.0742,     0.6248,     0.2850,     0.0081]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([1, 1, 2, 4, 2, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 0], device='cuda:0')
Preds:  tensor([1, 2, 2, 4, 0, 4, 0, 4, 4, 2, 0, 2, 0, 3, 4, 1], device='cuda:0')
Outputs:  tensor([[    0.2003,     0.5523,     0.2465,     0.0008,     0.0001],
        [    0.0360,     0.2873,     0.6338,     0.0415,     0.0014],
        [    0.0004,     0.0045,     0.9899,     0.0049,     0.0003],
        [    0.0002,     0.0002,     0.0002,     0.0449,     0.9544],
        [    0.8783,     0.1007,     0.0192,     0.0011,     0.0007],
        [    0.0008,     0.0002,     0.0006,     0.0156,     0.9828],
        [    0.9766,     0.0227,     0.0003,     0.0001,     0.0003],
        [    0.0001,     0.0000,     0.0004,     0.0258,     0.9737],
        [    0.0001,     0.0001,     0.0044,     0.2834,     0.7120],
        [    0.0453,     0.4129,     0.5356,     0.0061,     0.0001],
        [    0.6390,     0.2951,     0.0637,     0.0021,     0.0001],
        [    0.0044,     0.0696,     0.7361,     0.1850,     0.0050],
        [    0.7963,     0.1576,     0.0359,     0.0060,     0.0041],
        [    0.0007,     0.0008,     0.0167,     0.5377,     0.4441],
        [    0.0166,     0.0075,     0.0219,     0.0571,     0.8970],
        [    0.0771,     0.4613,     0.4315,     0.0298,     0.0004]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Mean loss[0.9347489163926778] | Mean metric[0.6069119082479258]
Stupid loss[0.0] | Naive soulution metric[0.2]
EPOCH 3
--------------
Labels:  tensor([0, 4, 0, 3, 0, 1, 2, 1, 4, 2, 1, 1, 4, 3, 1, 3], device='cuda:1')
Preds:  tensor([0, 4, 0, 3, 0, 0, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.6791,     0.3036,     0.0166,     0.0003,     0.0005],
        [    0.0004,     0.0003,     0.0016,     0.0596,     0.9382],
        [    0.8361,     0.1532,     0.0102,     0.0004,     0.0001],
        [    0.0000,     0.0003,     0.0340,     0.8735,     0.0922],
        [    0.7759,     0.2204,     0.0035,     0.0001,     0.0000],
        [    0.6947,     0.2836,     0.0210,     0.0006,     0.0001],
        [    0.1372,     0.3490,     0.4543,     0.0547,     0.0048],
        [    0.0563,     0.2979,     0.6145,     0.0292,     0.0022],
        [    0.1003,     0.1264,     0.3964,     0.3366,     0.0403],
        [    0.0006,     0.0038,     0.1593,     0.6381,     0.1982],
        [    0.0467,     0.5887,     0.3620,     0.0024,     0.0001],
        [    0.0142,     0.0820,     0.6031,     0.2977,     0.0030],
        [    0.0005,     0.0004,     0.0158,     0.5511,     0.4321],
        [    0.0001,     0.0003,     0.0383,     0.8400,     0.1213],
        [    0.0026,     0.0648,     0.7947,     0.1360,     0.0018],
        [    0.0106,     0.2388,     0.7031,     0.0431,     0.0044]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Mean loss[0.9354643704630562] | Mean metric[0.6018484626647145]
Labels:  tensor([4, 2, 2, 0, 0, 3, 3, 0, 0, 0, 0, 1, 4, 2, 0, 4], device='cuda:0')
Stupid loss[0.0] | Naive soulution metric[0.2]
Preds:  tensor([4, 4, 2, 0, 0, 3, 3, 0, 0, 0, 1, 0, 3, 2, 0, 4], device='cuda:0')
EPOCH 3
--------------
Outputs:  tensor([[    0.0001,     0.0001,     0.0002,     0.0387,     0.9608],
        [    0.0002,     0.0003,     0.0053,     0.1477,     0.8465],
        [    0.1063,     0.2287,     0.4462,     0.2137,     0.0051],
        [    0.9518,     0.0451,     0.0025,     0.0002,     0.0004],
        [    0.7935,     0.1760,     0.0280,     0.0013,     0.0013],
        [    0.0001,     0.0001,     0.0022,     0.9946,     0.0030],
        [    0.0001,     0.0010,     0.1249,     0.8159,     0.0581],
        [    0.4368,     0.4146,     0.0935,     0.0175,     0.0376],
        [    0.9913,     0.0083,     0.0002,     0.0001,     0.0002],
        [    0.7018,     0.2838,     0.0135,     0.0004,     0.0005],
        [    0.1418,     0.7583,     0.0143,     0.0153,     0.0702],
        [    0.5191,     0.4307,     0.0475,     0.0015,     0.0012],
        [    0.0005,     0.0006,     0.0089,     0.8689,     0.1211],
        [    0.1839,     0.2293,     0.3791,     0.0967,     0.1110],
        [    0.7912,     0.1841,     0.0222,     0.0016,     0.0009],
        [    0.0001,     0.0002,     0.0016,     0.1325,     0.8655]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Mean loss[0.9399135901891295] | Mean metric[0.600201317715959]
Stupid loss[0.0] | Naive soulution metric[0.2]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Distance of parambert.embeddings.word_embeddings.weight: 0.0
Distance of parambert.embeddings.position_embeddings.weight: 0.0
Distance of parambert.embeddings.token_type_embeddings.weight: 0.0
Distance of parambert.embeddings.LayerNorm.weight: 0.0
Distance of parambert.embeddings.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.0.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.0.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.0.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.0.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.0.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.0.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.0.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.0.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.0.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.0.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.0.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.0.output.dense.weight: 0.0
Distance of parambert.encoder.layer.0.output.dense.bias: 0.0
Distance of parambert.encoder.layer.0.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.0.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.1.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.1.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.1.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.1.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.1.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.1.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.1.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.1.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.1.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.1.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.1.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.1.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.1.output.dense.weight: 0.0
Distance of parambert.encoder.layer.1.output.dense.bias: 0.0
Distance of parambert.encoder.layer.1.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.1.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.2.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.2.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.2.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.2.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.2.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.2.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.2.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.2.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.2.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.2.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.2.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.2.output.dense.weight: 0.0
Distance of parambert.encoder.layer.2.output.dense.bias: 0.0
Distance of parambert.encoder.layer.2.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.2.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.3.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.3.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.3.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.3.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.3.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.3.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.3.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.3.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.3.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.3.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.3.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.3.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.3.output.dense.weight: 0.0
Distance of parambert.encoder.layer.3.output.dense.bias: 0.0
Distance of parambert.encoder.layer.3.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.3.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.4.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.4.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.4.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.4.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.4.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.4.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.4.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.4.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.4.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.4.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.4.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.4.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.4.output.dense.weight: 0.0
Distance of parambert.encoder.layer.4.output.dense.bias: 0.0
Distance of parambert.encoder.layer.4.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.4.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.5.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.5.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.5.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.5.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.5.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.5.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.5.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.5.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.5.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.5.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.5.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.5.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.5.output.dense.weight: 0.0
Distance of parambert.encoder.layer.5.output.dense.bias: 0.0
Distance of parambert.encoder.layer.5.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.5.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.6.attention.self.query.weight: 9.880831718444824
Distance of parambert.encoder.layer.6.attention.self.query.bias: 0.21475069224834442
Distance of parambert.encoder.layer.6.attention.self.key.weight: 9.792707443237305
Distance of parambert.encoder.layer.6.attention.self.key.bias: 0.004994198214262724
Distance of parambert.encoder.layer.6.attention.self.value.weight: 9.280461311340332
Distance of parambert.encoder.layer.6.attention.self.value.bias: 0.08511369675397873
Distance of parambert.encoder.layer.6.attention.output.dense.weight: 9.356110572814941
Distance of parambert.encoder.layer.6.attention.output.dense.bias: 0.3227602243423462
Distance of parambert.encoder.layer.6.attention.output.LayerNorm.weight: 0.8014435172080994
Distance of parambert.encoder.layer.6.attention.output.LayerNorm.bias: 0.3225509226322174
Distance of parambert.encoder.layer.6.intermediate.dense.weight: 21.64536476135254
Distance of parambert.encoder.layer.6.intermediate.dense.bias: 0.3389770984649658
Distance of parambert.encoder.layer.6.output.dense.weight: 19.48859214782715
Distance of parambert.encoder.layer.6.output.dense.bias: 0.12686854600906372
Distance of parambert.encoder.layer.6.output.LayerNorm.weight: 1.373603105545044
Distance of parambert.encoder.layer.6.output.LayerNorm.bias: 0.19113007187843323
Distance of parambert.encoder.layer.7.attention.self.query.weight: 10.090534210205078
Distance of parambert.encoder.layer.7.attention.self.query.bias: 0.19098065793514252
Distance of parambert.encoder.layer.7.attention.self.key.weight: 9.90488338470459
Distance of parambert.encoder.layer.7.attention.self.key.bias: 0.005324058700352907
Distance of parambert.encoder.layer.7.attention.self.value.weight: 9.170663833618164
Distance of parambert.encoder.layer.7.attention.self.value.bias: 0.08950488269329071
Distance of parambert.encoder.layer.7.attention.output.dense.weight: 9.047250747680664
Distance of parambert.encoder.layer.7.attention.output.dense.bias: 0.2606464624404907
Distance of parambert.encoder.layer.7.attention.output.LayerNorm.weight: 0.8664907813072205
Distance of parambert.encoder.layer.7.attention.output.LayerNorm.bias: 0.18552808463573456
Distance of parambert.encoder.layer.7.intermediate.dense.weight: 21.460834503173828
Distance of parambert.encoder.layer.7.intermediate.dense.bias: 0.3520817458629608
Distance of parambert.encoder.layer.7.output.dense.weight: 19.167282104492188
Distance of parambert.encoder.layer.7.output.dense.bias: 0.12133428454399109
Distance of parambert.encoder.layer.7.output.LayerNorm.weight: 1.4843971729278564
Distance of parambert.encoder.layer.7.output.LayerNorm.bias: 0.13722045719623566
Distance of parambert.encoder.layer.8.attention.self.query.weight: 9.915482521057129
Distance of parambert.encoder.layer.8.attention.self.query.bias: 0.2168770432472229
Distance of parambert.encoder.layer.8.attention.self.key.weight: 9.766473770141602
Distance of parambert.encoder.layer.8.attention.self.key.bias: 0.006650346796959639
Distance of parambert.encoder.layer.8.attention.self.value.weight: 8.85840892791748
Distance of parambert.encoder.layer.8.attention.self.value.bias: 0.08661101758480072
Distance of parambert.encoder.layer.8.attention.output.dense.weight: 8.725502967834473
Distance of parambert.encoder.layer.8.attention.output.dense.bias: 0.2802784740924835
Distance of parambert.encoder.layer.8.attention.output.LayerNorm.weight: 0.9010505676269531
Distance of parambert.encoder.layer.8.attention.output.LayerNorm.bias: 0.2576044499874115
Distance of parambert.encoder.layer.8.intermediate.dense.weight: 21.27098274230957
Distance of parambert.encoder.layer.8.intermediate.dense.bias: 0.3988751173019409
Distance of parambert.encoder.layer.8.output.dense.weight: 18.866994857788086
Distance of parambert.encoder.layer.8.output.dense.bias: 0.15267996490001678
Distance of parambert.encoder.layer.8.output.LayerNorm.weight: 1.6588221788406372
Distance of parambert.encoder.layer.8.output.LayerNorm.bias: 0.12936021387577057
Distance of parambert.encoder.layer.9.attention.self.query.weight: 9.987009048461914
Distance of parambert.encoder.layer.9.attention.self.query.bias: 0.23874646425247192
Distance of parambert.encoder.layer.9.attention.self.key.weight: 9.858332633972168
Distance of parambert.encoder.layer.9.attention.self.key.bias: 0.005842020269483328
Distance of parambert.encoder.layer.9.attention.self.value.weight: 8.402816772460938
Distance of parambert.encoder.layer.9.attention.self.value.bias: 0.10620149970054626
Distance of parambert.encoder.layer.9.attention.output.dense.weight: 8.240321159362793
Distance of parambert.encoder.layer.9.attention.output.dense.bias: 0.322604775428772
Distance of parambert.encoder.layer.9.attention.output.LayerNorm.weight: 1.0552945137023926
Distance of parambert.encoder.layer.9.attention.output.LayerNorm.bias: 0.3280644118785858
Distance of parambert.encoder.layer.9.intermediate.dense.weight: 20.777624130249023
Distance of parambert.encoder.layer.9.intermediate.dense.bias: 0.4912567436695099
Distance of parambert.encoder.layer.9.output.dense.weight: 18.390317916870117
Distance of parambert.encoder.layer.9.output.dense.bias: 0.21179094910621643
Distance of parambert.encoder.layer.9.output.LayerNorm.weight: 1.845447301864624
Distance of parambert.encoder.layer.9.output.LayerNorm.bias: 0.16574324667453766
Distance of parambert.encoder.layer.10.attention.self.query.weight: 9.656839370727539
Distance of parambert.encoder.layer.10.attention.self.query.bias: 0.23477190732955933
Distance of parambert.encoder.layer.10.attention.self.key.weight: 9.622322082519531
Distance of parambert.encoder.layer.10.attention.self.key.bias: 0.0054479073733091354
Distance of parambert.encoder.layer.10.attention.self.value.weight: 7.89947509765625
Distance of parambert.encoder.layer.10.attention.self.value.bias: 0.12181326746940613
Distance of parambert.encoder.layer.10.attention.output.dense.weight: 7.487300395965576
Distance of parambert.encoder.layer.10.attention.output.dense.bias: 0.30923375487327576
Distance of parambert.encoder.layer.10.attention.output.LayerNorm.weight: 1.368791103363037
Distance of parambert.encoder.layer.10.attention.output.LayerNorm.bias: 0.3600429892539978
Distance of parambert.encoder.layer.10.intermediate.dense.weight: 19.478315353393555
Distance of parambert.encoder.layer.10.intermediate.dense.bias: 0.7323933243751526
Distance of parambert.encoder.layer.10.output.dense.weight: 16.820232391357422
Distance of parambert.encoder.layer.10.output.dense.bias: 0.2986481189727783
Distance of parambert.encoder.layer.10.output.LayerNorm.weight: 2.184925079345703
Distance of parambert.encoder.layer.10.output.LayerNorm.bias: 0.19803902506828308
Distance of parambert.encoder.layer.11.attention.self.query.weight: 9.056818008422852
Distance of parambert.encoder.layer.11.attention.self.query.bias: 0.27503031492233276
Distance of parambert.encoder.layer.11.attention.self.key.weight: 9.159320831298828
Distance of parambert.encoder.layer.11.attention.self.key.bias: 0.005032449495047331
Distance of parambert.encoder.layer.11.attention.self.value.weight: 7.085536479949951
Distance of parambert.encoder.layer.11.attention.self.value.bias: 0.11826924234628677
Distance of parambert.encoder.layer.11.attention.output.dense.weight: 6.644744396209717
Distance of parambert.encoder.layer.11.attention.output.dense.bias: 0.3437488377094269
Distance of parambert.encoder.layer.11.attention.output.LayerNorm.weight: 1.8791282176971436
Distance of parambert.encoder.layer.11.attention.output.LayerNorm.bias: 0.4336561858654022
Distance of parambert.encoder.layer.11.intermediate.dense.weight: 20.51639175415039
Distance of parambert.encoder.layer.11.intermediate.dense.bias: 1.5162261724472046
Distance of parambert.encoder.layer.11.output.dense.weight: 15.252562522888184
Distance of parambert.encoder.layer.11.output.dense.bias: 0.43604812026023865
Distance of parambert.encoder.layer.11.output.LayerNorm.weight: 2.1719932556152344
Distance of parambert.encoder.layer.11.output.LayerNorm.bias: 0.5227877497673035
Distance of parambert.pooler.dense.weight: 7.593677997589111
Distance of parambert.pooler.dense.bias: 0.2571040391921997
EPOCH 3
--------------
Labels:  tensor([4, 4, 2, 0, 0, 4, 4, 1, 4, 2, 0, 1, 1, 2, 2, 1], device='cuda:1')
Preds:  tensor([4, 4, 1, 2, 0, 4, 4, 1, 4, 2, 3, 1, 2, 1, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0006,     0.0004,     0.0077,     0.1649,     0.8264],
        [    0.0003,     0.0004,     0.0052,     0.1394,     0.8547],
        [    0.1614,     0.7896,     0.0428,     0.0049,     0.0014],
        [    0.0901,     0.2860,     0.4686,     0.1358,     0.0195],
        [    0.4370,     0.1114,     0.1794,     0.1009,     0.1712],
        [    0.0013,     0.0005,     0.0009,     0.0160,     0.9812],
        [    0.0004,     0.0004,     0.0033,     0.0782,     0.9177],
        [    0.2601,     0.5591,     0.1802,     0.0005,     0.0001],
        [    0.0001,     0.0001,     0.0029,     0.0994,     0.8975],
        [    0.0550,     0.1727,     0.5828,     0.1831,     0.0064],
        [    0.0022,     0.0209,     0.2223,     0.4716,     0.2831],
        [    0.2490,     0.3962,     0.3144,     0.0290,     0.0115],
        [    0.0007,     0.0210,     0.9257,     0.0516,     0.0010],
        [    0.1307,     0.5072,     0.3570,     0.0049,     0.0002],
        [    0.0057,     0.1799,     0.5359,     0.2561,     0.0224],
        [    0.0074,     0.2078,     0.7227,     0.0610,     0.0011]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Mean loss[0.9333516320203676] | Mean metric[0.6028550512445096]
Stupid loss[0.0] | Naive soulution metric[0.2]
EPOCH 3
--------------
Step[500] | Loss[0.934657633304596] | Lr[4.000000000000001e-07]
Step[500] | Loss[0.5743361711502075] | Lr[4.000000000000001e-07]
Step[500] | Loss[0.6040492057800293] | Lr[4.000000000000001e-07]
Step[500] | Loss[0.4973270893096924] | Lr[4.000000000000001e-07]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
slurmstepd: error: *** JOB 52974 ON gpu002 CANCELLED AT 2023-10-29T19:39:41 ***
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4823 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4824 closing signal SIGTERM
slurmstepd: error: *** STEP 52974.1 ON gpu002 CANCELLED AT 2023-10-29T19:39:41 ***
