Node IP: 10.128.2.151
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 1980
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.151:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 1980
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.128.2.151:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_3qchllg_/1980_4pv0voq3
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_0_ttyrsm/1980_qbvssiub
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu001.hpc
  master_port=60401
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=gpu001.hpc
  master_port=60401
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_3qchllg_/1980_4pv0voq3/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_3qchllg_/1980_4pv0voq3/attempt_0/1/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_0_ttyrsm/1980_qbvssiub/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_0_ttyrsm/1980_qbvssiub/attempt_0/1/error.json
/u/dssc/msanna00/.conda/envs/deeplearning3/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/u/dssc/msanna00/.conda/envs/deeplearning3/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
PORT:  60401
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  0
My rank is:  0
PORT:  60401
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  0
My rank is:  1
PORT:  60401
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  1
My rank is:  3
PORT:  60401
WORLD SIZE:  4
MASTER NODE:  gpu001.hpc
My slurm id is:  1
My rank is:  2
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
------------------------

------------------------

------------------------

------------------------

Loading checkpoint...
Loading checkpoint...
Loading checkpoint...Loading checkpoint...

Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
Retrieving epoch...
Loading model state...
Retrieving epoch...
Loading model state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
Loading scheduler state...
Loading optmizer state...
LOADED!
I'm process 2 using GPU 0
LOADED!
I'm process 0 using GPU 0
LOADED!
I'm process 3 using GPU 1
LOADED!
I'm process 1 using GPU 1
Labels:  tensor([2, 2, 1, 1, 1, 4, 4, 4, 3, 2, 0, 3, 3, 4, 0, 3], device='cuda:0')
Preds:  tensor([4, 3, 3, 1, 0, 4, 4, 4, 2, 2, 2, 2, 3, 4, 0, 3], device='cuda:0')
Outputs:  Labels:  tensor([0, 4, 2, 3, 0, 1, 2, 2, 3, 2, 2, 4, 4, 1, 1, 2], device='cuda:1')
Preds:  tensor([0, 4, 1, 4, 0, 1, 3, 3, 2, 2, 0, 4, 4, 2, 1, 4], device='cuda:1')
Outputs:  tensor([[    0.0001,     0.0002,     0.0136,     0.4900,     0.4961],
        [    0.0005,     0.0059,     0.3511,     0.6386,     0.0039],
        [    0.0042,     0.0319,     0.3806,     0.4985,     0.0848],
        [    0.1124,     0.5022,     0.3729,     0.0120,     0.0005],
        [    0.6149,     0.3690,     0.0159,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0006,     0.0521,     0.9471],
        [    0.0004,     0.0002,     0.0011,     0.0373,     0.9611],
        [    0.0002,     0.0003,     0.0041,     0.1031,     0.8924],
        [    0.0018,     0.0254,     0.8849,     0.0804,     0.0075],
        [    0.0038,     0.0420,     0.5397,     0.4069,     0.0077],
        [    0.0447,     0.4325,     0.4816,     0.0384,     0.0028],
        [    0.0054,     0.0837,     0.7265,     0.1594,     0.0249],
        [    0.0006,     0.0045,     0.1369,     0.5705,     0.2874],
        [    0.0002,     0.0015,     0.0032,     0.2474,     0.7476],
        [    0.9334,     0.0649,     0.0016,     0.0000,     0.0000],
        [    0.0001,     0.0001,     0.0063,     0.8916,     0.1018]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
tensor([[    0.8806,     0.1153,     0.0040,     0.0000,     0.0000],
        [    0.0002,     0.0001,     0.0012,     0.0955,     0.9029],
        [    0.0355,     0.6968,     0.2285,     0.0322,     0.0071],
        [    0.0001,     0.0000,     0.0001,     0.0135,     0.9863],
        [    0.9859,     0.0124,     0.0014,     0.0002,     0.0002],
        [    0.0537,     0.4212,     0.4201,     0.0990,     0.0059],
        [    0.0148,     0.1353,     0.2667,     0.3213,     0.2619],
        [    0.0003,     0.0036,     0.3441,     0.6229,     0.0292],
        [    0.1149,     0.4135,     0.4454,     0.0257,     0.0004],
        [    0.0032,     0.0529,     0.6926,     0.2468,     0.0045],
        [    0.7343,     0.2298,     0.0341,     0.0013,     0.0006],
        [    0.0000,     0.0000,     0.0005,     0.0365,     0.9629],
        [    0.0004,     0.0002,     0.0004,     0.0021,     0.9970],
        [    0.0122,     0.4030,     0.4926,     0.0865,     0.0057],
        [    0.0035,     0.9914,     0.0044,     0.0004,     0.0002],
        [    0.0107,     0.0039,     0.0370,     0.2143,     0.7341]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([3, 0, 0, 1, 0, 3, 0, 0, 3, 1, 4, 4, 4, 1, 2, 0], device='cuda:1')
Preds:  tensor([4, 1, 2, 1, 0, 2, 0, 1, 3, 1, 4, 3, 4, 1, 2, 0], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0003,     0.0073,     0.3238,     0.6682],
        [    0.3822,     0.5266,     0.0903,     0.0007,     0.0002],
        [    0.1206,     0.3281,     0.5193,     0.0296,     0.0024],
        [    0.4172,     0.4848,     0.0958,     0.0018,     0.0004],
        [    0.8203,     0.1589,     0.0205,     0.0002,     0.0000],
        [    0.0013,     0.0199,     0.7577,     0.2183,     0.0028],
        [    0.9491,     0.0488,     0.0020,     0.0001,     0.0000],
        [    0.3867,     0.5921,     0.0204,     0.0006,     0.0002],
        [    0.0002,     0.0039,     0.3816,     0.5385,     0.0758],
        [    0.1952,     0.4357,     0.3507,     0.0166,     0.0018],
        [    0.1581,     0.1096,     0.2537,     0.1880,     0.2907],
        [    0.0373,     0.0325,     0.3016,     0.5469,     0.0817],
        [    0.0002,     0.0002,     0.0031,     0.0800,     0.9165],
        [    0.1816,     0.7022,     0.1040,     0.0104,     0.0019],
        [    0.0008,     0.0416,     0.9543,     0.0031,     0.0001],
        [    0.9776,     0.0219,     0.0004,     0.0001,     0.0001]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([3, 1, 1, 4, 0, 2, 4, 1, 3, 0, 2, 3, 2, 2, 4, 4], device='cuda:0')
Preds:  tensor([3, 1, 0, 4, 0, 2, 4, 0, 3, 0, 1, 4, 1, 2, 3, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0005,     0.0194,     0.9795,     0.0005],
        [    0.4427,     0.4604,     0.0955,     0.0011,     0.0003],
        [    0.7603,     0.2047,     0.0323,     0.0023,     0.0005],
        [    0.0013,     0.0005,     0.0037,     0.0525,     0.9419],
        [    0.8681,     0.0610,     0.0389,     0.0172,     0.0148],
        [    0.0015,     0.0977,     0.6760,     0.2189,     0.0058],
        [    0.0024,     0.0017,     0.0077,     0.0783,     0.9100],
        [    0.5368,     0.4343,     0.0286,     0.0002,     0.0001],
        [    0.0001,     0.0009,     0.2041,     0.7777,     0.0172],
        [    0.6809,     0.2306,     0.0859,     0.0025,     0.0001],
        [    0.3568,     0.5105,     0.1307,     0.0017,     0.0004],
        [    0.0007,     0.0007,     0.0183,     0.4727,     0.5076],
        [    0.0215,     0.7250,     0.2524,     0.0012,     0.0000],
        [    0.0058,     0.0665,     0.6305,     0.2809,     0.0163],
        [    0.0001,     0.0001,     0.0071,     0.6330,     0.3596],
        [    0.0023,     0.0008,     0.0073,     0.2181,     0.7715]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 0, 4, 0, 0, 0, 1, 2], device='cuda:1')
Preds:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 2, 4, 0, 0, 0, 2, 4], device='cuda:1')
Outputs:  tensor([[    0.0006,     0.0030,     0.9767,     0.0171,     0.0026],
        [    0.6172,     0.3347,     0.0435,     0.0033,     0.0013],
        [    0.0003,     0.0014,     0.0008,     0.0150,     0.9825],
        [    0.0001,     0.0000,     0.0001,     0.0130,     0.9867],
        [    0.0002,     0.0003,     0.0074,     0.2235,     0.7685],
        [    0.6573,     0.3183,     0.0229,     0.0010,     0.0006],
        [    0.8497,     0.1419,     0.0080,     0.0003,     0.0001],
        [    0.0459,     0.2843,     0.6437,     0.0254,     0.0007],
        [    0.0002,     0.0002,     0.0057,     0.1751,     0.8188],
        [    0.0548,     0.4367,     0.4982,     0.0099,     0.0004],
        [    0.0032,     0.0031,     0.0601,     0.4609,     0.4726],
        [    0.5144,     0.2295,     0.1759,     0.0420,     0.0382],
        [    0.5350,     0.4362,     0.0284,     0.0003,     0.0001],
        [    0.8842,     0.1150,     0.0008,     0.0000,     0.0001],
        [    0.0122,     0.2565,     0.7056,     0.0256,     0.0002],
        [    0.0623,     0.0620,     0.1070,     0.1021,     0.6666]],
       device='cuda:1')
Metric:  tensor(0.8125, device='cuda:1')
------------------------
Labels:  tensor([1, 1, 1, 1, 2, 1, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Preds:  tensor([3, 0, 1, 4, 1, 2, 0, 0, 4, 4, 2, 1, 2, 0, 0, 1], device='cuda:0')
Outputs:  tensor([[    0.0034,     0.0325,     0.3503,     0.4857,     0.1281],
        [    0.3866,     0.3558,     0.2518,     0.0053,     0.0004],
        [    0.0594,     0.5427,     0.3925,     0.0052,     0.0002],
        [    0.0403,     0.0280,     0.0856,     0.1874,     0.6587],
        [    0.0237,     0.7089,     0.2414,     0.0224,     0.0037],
        [    0.0063,     0.1330,     0.8153,     0.0444,     0.0010],
        [    0.9392,     0.0543,     0.0041,     0.0008,     0.0015],
        [    0.5878,     0.4028,     0.0093,     0.0001,     0.0000],
        [    0.0003,     0.0002,     0.0021,     0.1024,     0.8950],
        [    0.0006,     0.0003,     0.0033,     0.0584,     0.9373],
        [    0.0053,     0.1313,     0.7823,     0.0802,     0.0009],
        [    0.4158,     0.4665,     0.1146,     0.0022,     0.0009],
        [    0.0011,     0.0182,     0.8107,     0.1698,     0.0002],
        [    0.6055,     0.3318,     0.0591,     0.0035,     0.0002],
        [    0.5140,     0.3161,     0.1235,     0.0270,     0.0195],
        [    0.2936,     0.5024,     0.2005,     0.0033,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([0, 1, 4, 2, 3, 2, 1, 0, 2, 2, 2, 0, 2, 4, 3, 4], device='cuda:0')
Preds:  tensor([1, 1, 4, 1, 2, 2, 1, 0, 1, 3, 2, 0, 3, 4, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.4729,     0.4980,     0.0289,     0.0001,     0.0000],
        [    0.1044,     0.8907,     0.0047,     0.0001,     0.0001],
        [    0.0004,     0.0008,     0.0131,     0.1831,     0.8027],
        [    0.3190,     0.5291,     0.1487,     0.0028,     0.0003],
        [    0.1665,     0.3473,     0.4406,     0.0347,     0.0109],
        [    0.0400,     0.0244,     0.9110,     0.0213,     0.0033],
        [    0.3171,     0.5715,     0.1109,     0.0004,     0.0001],
        [    0.9605,     0.0328,     0.0049,     0.0009,     0.0009],
        [    0.4052,     0.4106,     0.1810,     0.0028,     0.0003],
        [    0.0004,     0.0038,     0.2519,     0.6579,     0.0859],
        [    0.0010,     0.0135,     0.4769,     0.4436,     0.0649],
        [    0.9521,     0.0462,     0.0015,     0.0001,     0.0001],
        [    0.0004,     0.0105,     0.2070,     0.7618,     0.0204],
        [    0.0040,     0.0020,     0.0257,     0.2168,     0.7515],
        [    0.6501,     0.2896,     0.0592,     0.0010,     0.0001],
        [    0.0002,     0.0002,     0.0002,     0.0099,     0.9895]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 1, 3, 4, 3, 0, 3, 1, 1, 0, 0, 3, 2, 0], device='cuda:1')
Preds:  tensor([3, 2, 3, 1, 2, 4, 2, 1, 4, 1, 1, 0, 0, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0019,     0.0100,     0.3459,     0.6338,     0.0084],
        [    0.0012,     0.0386,     0.7357,     0.2209,     0.0037],
        [    0.0067,     0.0056,     0.0757,     0.9097,     0.0023],
        [    0.3305,     0.4604,     0.2041,     0.0047,     0.0004],
        [    0.0008,     0.0238,     0.9450,     0.0300,     0.0004],
        [    0.0008,     0.0008,     0.0134,     0.1810,     0.8040],
        [    0.0104,     0.0932,     0.5878,     0.2861,     0.0225],
        [    0.1923,     0.5445,     0.2615,     0.0015,     0.0001],
        [    0.0001,     0.0001,     0.0048,     0.3635,     0.6315],
        [    0.1693,     0.5156,     0.2873,     0.0240,     0.0039],
        [    0.1346,     0.4973,     0.3623,     0.0056,     0.0003],
        [    0.8412,     0.1504,     0.0081,     0.0002,     0.0001],
        [    0.6146,     0.2798,     0.0964,     0.0076,     0.0016],
        [    0.0089,     0.0415,     0.1949,     0.5950,     0.1598],
        [    0.0209,     0.1611,     0.6659,     0.1473,     0.0048],
        [    0.0140,     0.1444,     0.6960,     0.1447,     0.0009]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([2, 3, 4, 0, 1, 3, 4, 3, 4, 2, 1, 0, 4, 4, 4, 4], device='cuda:1')
Preds:  tensor([2, 3, 4, 1, 1, 4, 4, 2, 4, 3, 1, 0, 4, 4, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0290,     0.3561,     0.6018,     0.0127,     0.0004],
        [    0.0002,     0.0011,     0.1150,     0.7644,     0.1192],
        [    0.0006,     0.0003,     0.0011,     0.0126,     0.9854],
        [    0.2316,     0.7247,     0.0429,     0.0005,     0.0003],
        [    0.3238,     0.5341,     0.1305,     0.0088,     0.0028],
        [    0.0009,     0.0012,     0.0281,     0.4026,     0.5672],
        [    0.0005,     0.0002,     0.0013,     0.0097,     0.9883],
        [    0.0067,     0.2914,     0.6903,     0.0115,     0.0002],
        [    0.0004,     0.0004,     0.0001,     0.0017,     0.9975],
        [    0.0002,     0.0035,     0.3868,     0.5995,     0.0100],
        [    0.3429,     0.5369,     0.1185,     0.0016,     0.0002],
        [    0.8898,     0.1058,     0.0041,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0015,     0.0863,     0.9121],
        [    0.0007,     0.0004,     0.0030,     0.0820,     0.9139],
        [    0.0011,     0.0049,     0.2157,     0.6532,     0.1251],
        [    0.0001,     0.0001,     0.0012,     0.1094,     0.8892]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([4, 0, 4, 0, 4, 1, 1, 3, 0, 4, 4, 3, 0, 2, 3, 0], device='cuda:0')
Preds:  tensor([4, 0, 4, 0, 4, 1, 2, 4, 0, 3, 4, 3, 0, 1, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0002,     0.0001,     0.0016,     0.0828,     0.9152],
        [    0.6703,     0.2834,     0.0451,     0.0011,     0.0002],
        [    0.0003,     0.0002,     0.0054,     0.2359,     0.7582],
        [    0.8705,     0.1247,     0.0046,     0.0001,     0.0001],
        [    0.0586,     0.0248,     0.0421,     0.0885,     0.7859],
        [    0.0040,     0.9915,     0.0041,     0.0002,     0.0001],
        [    0.0850,     0.4104,     0.4734,     0.0298,     0.0015],
        [    0.0180,     0.0868,     0.1583,     0.2842,     0.4526],
        [    0.9814,     0.0179,     0.0006,     0.0001,     0.0001],
        [    0.0011,     0.0056,     0.1657,     0.5408,     0.2868],
        [    0.0010,     0.0080,     0.0049,     0.1511,     0.8350],
        [    0.0010,     0.0040,     0.1079,     0.5338,     0.3533],
        [    0.7836,     0.2096,     0.0067,     0.0001,     0.0000],
        [    0.0371,     0.5518,     0.3629,     0.0470,     0.0012],
        [    0.0001,     0.0001,     0.0035,     0.2937,     0.7026],
        [    0.8611,     0.1323,     0.0064,     0.0002,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([3, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 3, 3, 4, 4, 2], device='cuda:0')
Preds:  tensor([3, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 4, 3, 4, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0003,     0.0138,     0.6440,     0.3419],
        [    0.0013,     0.0185,     0.7112,     0.2461,     0.0231],
        [    0.0012,     0.0262,     0.6760,     0.2902,     0.0064],
        [    0.0225,     0.2142,     0.6988,     0.0636,     0.0009],
        [    0.3543,     0.5882,     0.0570,     0.0005,     0.0001],
        [    0.0510,     0.0888,     0.4167,     0.3337,     0.1097],
        [    0.1799,     0.6499,     0.1669,     0.0031,     0.0002],
        [    0.9785,     0.0205,     0.0009,     0.0001,     0.0001],
        [    0.0006,     0.0147,     0.7386,     0.2422,     0.0038],
        [    0.0353,     0.2491,     0.4126,     0.2647,     0.0384],
        [    0.7014,     0.2675,     0.0308,     0.0002,     0.0001],
        [    0.0000,     0.0000,     0.0002,     0.0105,     0.9892],
        [    0.0003,     0.0076,     0.4728,     0.5168,     0.0024],
        [    0.0002,     0.0003,     0.0070,     0.2464,     0.7461],
        [    0.0056,     0.0097,     0.0899,     0.1654,     0.7295],
        [    0.7065,     0.2617,     0.0309,     0.0007,     0.0002]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 2, 3, 0, 2, 4, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:1')
Preds:  tensor([4, 2, 2, 2, 4, 0, 1, 4, 2, 0, 1, 2, 0, 0, 0, 0], device='cuda:1')
Outputs:  tensor([[    0.0002,     0.0004,     0.0087,     0.2515,     0.7392],
        [    0.0015,     0.0340,     0.8376,     0.1252,     0.0017],
        [    0.0013,     0.0307,     0.8931,     0.0742,     0.0007],
        [    0.0032,     0.0266,     0.9689,     0.0012,     0.0001],
        [    0.0001,     0.0001,     0.0021,     0.0865,     0.9112],
        [    0.7878,     0.2013,     0.0106,     0.0002,     0.0001],
        [    0.2107,     0.4552,     0.2737,     0.0533,     0.0071],
        [    0.0004,     0.0007,     0.0063,     0.1259,     0.8667],
        [    0.0086,     0.1396,     0.7993,     0.0520,     0.0004],
        [    0.7097,     0.1690,     0.0712,     0.0230,     0.0271],
        [    0.2197,     0.5954,     0.1829,     0.0019,     0.0001],
        [    0.2430,     0.2755,     0.4070,     0.0615,     0.0130],
        [    0.9944,     0.0033,     0.0010,     0.0004,     0.0009],
        [    0.8549,     0.1099,     0.0247,     0.0051,     0.0053],
        [    0.9898,     0.0099,     0.0002,     0.0000,     0.0001],
        [    0.9972,     0.0019,     0.0003,     0.0002,     0.0005]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 4, 2, 3, 2, 4, 1, 2, 1, 1, 2, 0, 1, 0, 1], device='cuda:1')
Preds:  tensor([4, 4, 4, 1, 3, 3, 4, 1, 2, 2, 0, 2, 0, 0, 0, 2], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0003,     0.0029,     0.0702,     0.9264],
        [    0.0003,     0.0002,     0.0043,     0.2309,     0.7643],
        [    0.0032,     0.0009,     0.0067,     0.1901,     0.7992],
        [    0.2081,     0.4494,     0.3381,     0.0041,     0.0004],
        [    0.0016,     0.0123,     0.3641,     0.5391,     0.0829],
        [    0.0001,     0.0004,     0.0518,     0.8672,     0.0805],
        [    0.0008,     0.0005,     0.0012,     0.0057,     0.9918],
        [    0.0814,     0.4586,     0.4212,     0.0368,     0.0019],
        [    0.0014,     0.0539,     0.8647,     0.0797,     0.0002],
        [    0.0338,     0.2956,     0.6229,     0.0466,     0.0010],
        [    0.7501,     0.2065,     0.0374,     0.0053,     0.0007],
        [    0.0093,     0.2936,     0.6837,     0.0130,     0.0004],
        [    0.9780,     0.0213,     0.0004,     0.0001,     0.0002],
        [    0.7235,     0.2551,     0.0209,     0.0005,     0.0001],
        [    0.7274,     0.2596,     0.0126,     0.0003,     0.0001],
        [    0.0077,     0.0769,     0.6366,     0.2717,     0.0072]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([2, 0, 1, 3, 4, 2, 0, 0, 2, 1, 0, 1, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([4, 0, 2, 2, 4, 2, 0, 0, 1, 0, 0, 0, 0, 1, 3, 4], device='cuda:0')
Outputs:  tensor([[    0.0003,     0.0013,     0.0306,     0.3258,     0.6419],
        [    0.7476,     0.2373,     0.0143,     0.0005,     0.0002],
        [    0.0026,     0.0837,     0.8622,     0.0513,     0.0002],
        [    0.0014,     0.0550,     0.8896,     0.0538,     0.0002],
        [    0.0048,     0.0009,     0.0021,     0.0123,     0.9798],
        [    0.0052,     0.1170,     0.6488,     0.2249,     0.0041],
        [    0.5010,     0.4527,     0.0454,     0.0008,     0.0001],
        [    0.7700,     0.2033,     0.0264,     0.0002,     0.0000],
        [    0.0299,     0.9085,     0.0513,     0.0059,     0.0043],
        [    0.7352,     0.1546,     0.0928,     0.0125,     0.0050],
        [    0.9930,     0.0067,     0.0002,     0.0000,     0.0001],
        [    0.6651,     0.2591,     0.0737,     0.0021,     0.0001],
        [    0.7743,     0.1591,     0.0563,     0.0074,     0.0029],
        [    0.0081,     0.7027,     0.2595,     0.0281,     0.0016],
        [    0.0578,     0.1042,     0.2991,     0.3208,     0.2180],
        [    0.0003,     0.0004,     0.0003,     0.0260,     0.9731]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Labels:  tensor([4, 1, 1, 0, 0, 1, 2, 4, 2, 4, 4, 1, 0, 1, 2, 1], device='cuda:0')
Preds:  tensor([4, 1, 0, 0, 0, 1, 2, 4, 2, 4, 4, 4, 0, 0, 3, 0], device='cuda:0')
Outputs:  tensor([[    0.0007,     0.0006,     0.0025,     0.0362,     0.9600],
        [    0.3638,     0.5753,     0.0604,     0.0004,     0.0001],
        [    0.9644,     0.0345,     0.0009,     0.0001,     0.0000],
        [    0.6739,     0.3016,     0.0227,     0.0009,     0.0008],
        [    0.7554,     0.2261,     0.0173,     0.0007,     0.0005],
        [    0.1226,     0.6314,     0.2444,     0.0015,     0.0001],
        [    0.0641,     0.1628,     0.4632,     0.2779,     0.0320],
        [    0.0001,     0.0002,     0.0005,     0.1230,     0.8761],
        [    0.0169,     0.3057,     0.6579,     0.0194,     0.0001],
        [    0.0005,     0.0004,     0.0045,     0.1645,     0.8300],
        [    0.0057,     0.0065,     0.0212,     0.0781,     0.8885],
        [    0.3295,     0.0606,     0.0929,     0.1119,     0.4051],
        [    0.9581,     0.0412,     0.0006,     0.0000,     0.0000],
        [    0.6150,     0.3158,     0.0667,     0.0022,     0.0003],
        [    0.0252,     0.0474,     0.2386,     0.6024,     0.0864],
        [    0.6470,     0.2742,     0.0679,     0.0083,     0.0026]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 2, 1, 1, 4, 1, 4, 4, 1, 3, 2, 4, 3, 3, 4], device='cuda:1')
Preds:  tensor([2, 1, 1, 0, 1, 4, 0, 4, 4, 1, 2, 2, 4, 2, 4, 3], device='cuda:1')
Outputs:  tensor([[    0.0165,     0.0586,     0.4266,     0.4140,     0.0844],
        [    0.1280,     0.5143,     0.3466,     0.0108,     0.0003],
        [    0.1842,     0.4057,     0.3587,     0.0508,     0.0006],
        [    0.6474,     0.2964,     0.0554,     0.0007,     0.0001],
        [    0.2908,     0.6574,     0.0516,     0.0002,     0.0000],
        [    0.0016,     0.0006,     0.0021,     0.0632,     0.9325],
        [    0.7712,     0.1963,     0.0305,     0.0013,     0.0007],
        [    0.0002,     0.0004,     0.0122,     0.2575,     0.7298],
        [    0.0022,     0.0008,     0.0031,     0.0855,     0.9084],
        [    0.4220,     0.5264,     0.0508,     0.0007,     0.0001],
        [    0.0027,     0.0532,     0.7830,     0.1607,     0.0004],
        [    0.0008,     0.0284,     0.5503,     0.4133,     0.0071],
        [    0.0004,     0.0005,     0.0067,     0.1896,     0.8028],
        [    0.0002,     0.0075,     0.6530,     0.3365,     0.0028],
        [    0.0003,     0.0004,     0.0088,     0.1536,     0.8368],
        [    0.0002,     0.0004,     0.0225,     0.5527,     0.4241]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 2, 0, 0, 4, 4, 1, 4, 2, 0, 1, 1, 2, 2, 1], device='cuda:1')
Preds:  tensor([4, 4, 1, 2, 0, 4, 4, 1, 4, 2, 3, 1, 2, 1, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0005,     0.0003,     0.0074,     0.1678,     0.8240],
        [    0.0002,     0.0003,     0.0050,     0.1423,     0.8522],
        [    0.1618,     0.7951,     0.0377,     0.0041,     0.0012],
        [    0.0880,     0.2984,     0.4733,     0.1241,     0.0162],
        [    0.4261,     0.1132,     0.1964,     0.1046,     0.1596],
        [    0.0012,     0.0004,     0.0008,     0.0140,     0.9836],
        [    0.0003,     0.0003,     0.0031,     0.0757,     0.9206],
        [    0.2778,     0.5579,     0.1638,     0.0004,     0.0001],
        [    0.0001,     0.0001,     0.0027,     0.0989,     0.8983],
        [    0.0565,     0.1787,     0.5868,     0.1724,     0.0056],
        [    0.0023,     0.0238,     0.2589,     0.4929,     0.2221],
        [    0.2544,     0.3894,     0.3123,     0.0309,     0.0130],
        [    0.0006,     0.0216,     0.9308,     0.0463,     0.0007],
        [    0.1215,     0.5196,     0.3545,     0.0043,     0.0001],
        [    0.0057,     0.1895,     0.5443,     0.2405,     0.0200],
        [    0.0064,     0.2025,     0.7320,     0.0581,     0.0009]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Mean loss[0.9362663236578945] | Mean metric[0.6048377257198634]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Labels:  tensor([4, 2, 2, 0, 0, 3, 3, 0, 0, 0, 0, 1, 4, 2, 0, 4], device='cuda:0')
Preds:  tensor([4, 4, 2, 0, 0, 3, 3, 0, 0, 0, 1, 0, 3, 2, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0001,     0.0002,     0.0338,     0.9658],
        [    0.0002,     0.0003,     0.0048,     0.1450,     0.8497],
        [    0.0968,     0.2155,     0.4533,     0.2296,     0.0049],
        [    0.9562,     0.0412,     0.0021,     0.0002,     0.0003],
        [    0.7914,     0.1770,     0.0293,     0.0012,     0.0011],
        [    0.0001,     0.0001,     0.0021,     0.9934,     0.0043],
        [    0.0001,     0.0009,     0.1301,     0.8177,     0.0512],
        [    0.4534,     0.4088,     0.0898,     0.0162,     0.0318],
        [    0.9917,     0.0080,     0.0001,     0.0001,     0.0002],
        [    0.7205,     0.2675,     0.0112,     0.0003,     0.0004],
        [    0.1383,     0.7834,     0.0114,     0.0111,     0.0558],
        [    0.5251,     0.4325,     0.0406,     0.0010,     0.0008],
        [    0.0005,     0.0006,     0.0089,     0.8730,     0.1170],
        [    0.1722,     0.2193,     0.3827,     0.1031,     0.1227],
        [    0.7844,     0.1910,     0.0224,     0.0014,     0.0008],
        [    0.0001,     0.0002,     0.0016,     0.1321,     0.8660]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Mean loss[0.9425578430811331] | Mean metric[0.6020924841386042]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Labels:  tensor([1, 1, 2, 4, 2, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 0], device='cuda:0')
Preds:  tensor([1, 2, 2, 4, 0, 4, 0, 4, 4, 2, 0, 2, 0, 3, 4, 1], device='cuda:0')
Outputs:  tensor([[    0.1930,     0.5568,     0.2493,     0.0008,     0.0001],
        [    0.0364,     0.3088,     0.6200,     0.0338,     0.0010],
        [    0.0004,     0.0046,     0.9900,     0.0048,     0.0003],
        [    0.0002,     0.0002,     0.0002,     0.0395,     0.9599],
        [    0.8820,     0.0980,     0.0185,     0.0010,     0.0006],
        [    0.0007,     0.0002,     0.0006,     0.0149,     0.9836],
        [    0.9773,     0.0220,     0.0003,     0.0001,     0.0002],
        [    0.0001,     0.0000,     0.0003,     0.0242,     0.9754],
        [    0.0001,     0.0001,     0.0046,     0.3004,     0.6948],
        [    0.0412,     0.4148,     0.5379,     0.0059,     0.0001],
        [    0.6473,     0.2926,     0.0582,     0.0018,     0.0001],
        [    0.0045,     0.0749,     0.7471,     0.1697,     0.0038],
        [    0.8141,     0.1459,     0.0313,     0.0052,     0.0035],
        [    0.0005,     0.0006,     0.0154,     0.5456,     0.4378],
        [    0.0148,     0.0067,     0.0225,     0.0575,     0.8984],
        [    0.0806,     0.4707,     0.4211,     0.0273,     0.0003]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Mean loss[0.9374144610659328] | Mean metric[0.6085285505124451]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Labels:  tensor([0, 4, 0, 3, 0, 1, 2, 1, 4, 2, 1, 1, 4, 3, 1, 3], device='cuda:1')
Preds:  tensor([0, 4, 0, 3, 0, 0, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.6631,     0.3206,     0.0158,     0.0002,     0.0003],
        [    0.0003,     0.0002,     0.0014,     0.0565,     0.9415],
        [    0.8373,     0.1531,     0.0091,     0.0004,     0.0001],
        [    0.0000,     0.0002,     0.0310,     0.8890,     0.0797],
        [    0.7635,     0.2328,     0.0035,     0.0001,     0.0000],
        [    0.7171,     0.2657,     0.0167,     0.0004,     0.0001],
        [    0.1363,     0.3567,     0.4534,     0.0498,     0.0039],
        [    0.0539,     0.3023,     0.6158,     0.0264,     0.0016],
        [    0.0923,     0.1265,     0.4111,     0.3323,     0.0378],
        [    0.0005,     0.0035,     0.1613,     0.6433,     0.1913],
        [    0.0485,     0.6034,     0.3459,     0.0021,     0.0001],
        [    0.0149,     0.0933,     0.6205,     0.2691,     0.0022],
        [    0.0004,     0.0003,     0.0138,     0.5692,     0.4163],
        [    0.0001,     0.0002,     0.0377,     0.8441,     0.1179],
        [    0.0025,     0.0677,     0.8042,     0.1241,     0.0014],
        [    0.0102,     0.2576,     0.6976,     0.0322,     0.0023]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Mean loss[0.9378235176489259] | Mean metric[0.6028855539287458]
Stupid loss[0.0] | Naive soulution metric[0.2]
Freezed:  module.bert.embeddings.word_embeddings.weight
Freezed:  module.bert.embeddings.position_embeddings.weight
Freezed:  module.bert.embeddings.token_type_embeddings.weight
Freezed:  module.bert.embeddings.LayerNorm.weight
Freezed:  module.bert.embeddings.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.attention.self.query.weight
Freezed:  module.bert.encoder.layer.0.attention.self.query.bias
Freezed:  module.bert.encoder.layer.0.attention.self.key.weight
Freezed:  module.bert.encoder.layer.0.attention.self.key.bias
Freezed:  module.bert.encoder.layer.0.attention.self.value.weight
Freezed:  module.bert.encoder.layer.0.attention.self.value.bias
Freezed:  module.bert.encoder.layer.0.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.0.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.0.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.0.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.0.output.dense.weight
Freezed:  module.bert.encoder.layer.0.output.dense.bias
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.0.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.attention.self.query.weight
Freezed:  module.bert.encoder.layer.1.attention.self.query.bias
Freezed:  module.bert.encoder.layer.1.attention.self.key.weight
Freezed:  module.bert.encoder.layer.1.attention.self.key.bias
Freezed:  module.bert.encoder.layer.1.attention.self.value.weight
Freezed:  module.bert.encoder.layer.1.attention.self.value.bias
Freezed:  module.bert.encoder.layer.1.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.1.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.1.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.1.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.1.output.dense.weight
Freezed:  module.bert.encoder.layer.1.output.dense.bias
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.1.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.attention.self.query.weight
Freezed:  module.bert.encoder.layer.2.attention.self.query.bias
Freezed:  module.bert.encoder.layer.2.attention.self.key.weight
Freezed:  module.bert.encoder.layer.2.attention.self.key.bias
Freezed:  module.bert.encoder.layer.2.attention.self.value.weight
Freezed:  module.bert.encoder.layer.2.attention.self.value.bias
Freezed:  module.bert.encoder.layer.2.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.2.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.2.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.2.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.2.output.dense.weight
Freezed:  module.bert.encoder.layer.2.output.dense.bias
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.2.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.attention.self.query.weight
Freezed:  module.bert.encoder.layer.3.attention.self.query.bias
Freezed:  module.bert.encoder.layer.3.attention.self.key.weight
Freezed:  module.bert.encoder.layer.3.attention.self.key.bias
Freezed:  module.bert.encoder.layer.3.attention.self.value.weight
Freezed:  module.bert.encoder.layer.3.attention.self.value.bias
Freezed:  module.bert.encoder.layer.3.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.3.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.3.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.3.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.3.output.dense.weight
Freezed:  module.bert.encoder.layer.3.output.dense.bias
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.3.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.attention.self.query.weight
Freezed:  module.bert.encoder.layer.4.attention.self.query.bias
Freezed:  module.bert.encoder.layer.4.attention.self.key.weight
Freezed:  module.bert.encoder.layer.4.attention.self.key.bias
Freezed:  module.bert.encoder.layer.4.attention.self.value.weight
Freezed:  module.bert.encoder.layer.4.attention.self.value.bias
Freezed:  module.bert.encoder.layer.4.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.4.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.4.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.4.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.4.output.dense.weight
Freezed:  module.bert.encoder.layer.4.output.dense.bias
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.4.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.attention.self.query.weight
Freezed:  module.bert.encoder.layer.5.attention.self.query.bias
Freezed:  module.bert.encoder.layer.5.attention.self.key.weight
Freezed:  module.bert.encoder.layer.5.attention.self.key.bias
Freezed:  module.bert.encoder.layer.5.attention.self.value.weight
Freezed:  module.bert.encoder.layer.5.attention.self.value.bias
Freezed:  module.bert.encoder.layer.5.attention.output.dense.weight
Freezed:  module.bert.encoder.layer.5.attention.output.dense.bias
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.attention.output.LayerNorm.bias
Freezed:  module.bert.encoder.layer.5.intermediate.dense.weight
Freezed:  module.bert.encoder.layer.5.intermediate.dense.bias
Freezed:  module.bert.encoder.layer.5.output.dense.weight
Freezed:  module.bert.encoder.layer.5.output.dense.bias
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.weight
Freezed:  module.bert.encoder.layer.5.output.LayerNorm.bias
EPOCH 4
--------------
Step[500] | Loss[0.39213043451309204] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.5622167587280273] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.6177693009376526] | Lr[8.000000000000003e-08]
Step[500] | Loss[0.4867384731769562] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.5099323391914368] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.794552206993103] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.7429337501525879] | Lr[8.000000000000003e-08]
Step[1000] | Loss[0.8238608837127686] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.734208345413208] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.6484864950180054] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.6518476605415344] | Lr[8.000000000000003e-08]
Step[1500] | Loss[0.5046314001083374] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.8125411868095398] | Lr[8.000000000000003e-08]
Step[2000] | Loss[1.036777138710022] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.5598439574241638] | Lr[8.000000000000003e-08]
Step[2000] | Loss[0.5500906705856323] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.8501994609832764] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.5069255232810974] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.5756336450576782] | Lr[8.000000000000003e-08]
Step[2500] | Loss[0.42990225553512573] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.7085649371147156] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.6558042764663696] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.8121429681777954] | Lr[8.000000000000003e-08]
Step[3000] | Loss[0.786891520023346] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.575268030166626] | Lr[8.000000000000003e-08]
Step[3500] | Loss[0.44830530881881714] | Lr[8.000000000000003e-08]Step[3500] | Loss[0.3314403295516968] | Lr[8.000000000000003e-08]

Step[3500] | Loss[0.5706098079681396] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.802234411239624] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.7184986472129822] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.4644528031349182] | Lr[8.000000000000003e-08]
Step[4000] | Loss[0.4677025079727173] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.43465617299079895] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.3545580208301544] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.777400016784668] | Lr[8.000000000000003e-08]
Step[4500] | Loss[0.7651281356811523] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.7759853601455688] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.720982551574707] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.6213828325271606] | Lr[8.000000000000003e-08]
Step[5000] | Loss[0.5736209154129028] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.7478823065757751] | Lr[8.000000000000003e-08]
Step[5500] | Loss[0.6159451007843018] | Lr[8.000000000000003e-08]Step[5500] | Loss[0.4894597828388214] | Lr[8.000000000000003e-08]

Step[5500] | Loss[0.4995209574699402] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.8216165900230408] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.658958911895752] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.4355658292770386] | Lr[8.000000000000003e-08]
Step[6000] | Loss[0.5938679575920105] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.36269411444664] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.45913663506507874] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.7010505199432373] | Lr[8.000000000000003e-08]
Step[6500] | Loss[0.7856191396713257] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.5933351516723633] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.5652608275413513] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.7422043681144714] | Lr[8.000000000000003e-08]
Step[7000] | Loss[0.7031093835830688] | Lr[8.000000000000003e-08]
Step[7500] | Loss[1.0443743467330933] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.5285260677337646] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.42827320098876953] | Lr[8.000000000000003e-08]
Step[7500] | Loss[0.5724002718925476] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.43501877784729004] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.2882763743400574] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.6423951387405396] | Lr[8.000000000000003e-08]
Step[8000] | Loss[0.593230128288269] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.4949508011341095] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.29937562346458435] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.4839167892932892] | Lr[8.000000000000003e-08]
Step[8500] | Loss[0.3816451132297516] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.42820537090301514] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.6946032047271729] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.7094815969467163] | Lr[8.000000000000003e-08]
Step[9000] | Loss[0.5522192716598511] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.6687923073768616] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.8192967176437378] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.6263065338134766] | Lr[8.000000000000003e-08]
Step[9500] | Loss[0.4391268789768219] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.637040376663208] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.7255958914756775] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.4735809862613678] | Lr[8.000000000000003e-08]
Step[10000] | Loss[0.6453821659088135] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.9355999231338501] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.3129463493824005] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.9869769811630249] | Lr[8.000000000000003e-08]
Step[10500] | Loss[0.5160951018333435] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.8183265924453735] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.8646970391273499] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.6493954062461853] | Lr[8.000000000000003e-08]
Step[11000] | Loss[0.5025569796562195] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.7069703340530396] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.6557235717773438] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.6932104825973511] | Lr[8.000000000000003e-08]
Step[11500] | Loss[0.8867108821868896] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.5419185757637024] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.5145574808120728] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.824061393737793] | Lr[8.000000000000003e-08]
Step[12000] | Loss[0.4169237017631531] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.8391042351722717] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.6063581705093384] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.6381497979164124] | Lr[8.000000000000003e-08]
Step[12500] | Loss[0.9813425540924072] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.4973556101322174] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.47499385476112366] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.6090019345283508] | Lr[8.000000000000003e-08]
Step[13000] | Loss[0.6759785413742065] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.3409912586212158] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.41355210542678833] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.6100041270256042] | Lr[8.000000000000003e-08]
Step[13500] | Loss[0.7736945152282715] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.3629254698753357] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.75377357006073] | Lr[8.000000000000003e-08]
Step[14000] | Loss[1.0243134498596191] | Lr[8.000000000000003e-08]
Step[14000] | Loss[0.422505259513855] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.7518783807754517] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.7288364768028259] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.40093037486076355] | Lr[8.000000000000003e-08]
Step[14500] | Loss[0.5322073101997375] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.7517620921134949] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.5918334126472473] | Lr[8.000000000000003e-08]
Step[15000] | Loss[1.0677008628845215] | Lr[8.000000000000003e-08]
Step[15000] | Loss[0.7273383140563965] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.30307894945144653] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.9809520244598389] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.5775699019432068] | Lr[8.000000000000003e-08]
Step[15500] | Loss[0.7817213535308838] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.7213878631591797] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.49052533507347107] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.3537924289703369] | Lr[8.000000000000003e-08]
Step[16000] | Loss[0.4553651511669159] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.6544595956802368] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.5051418542861938] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.6438422799110413] | Lr[8.000000000000003e-08]
Step[16500] | Loss[0.5305539965629578] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.3522823750972748] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.5668253302574158] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.5874690413475037] | Lr[8.000000000000003e-08]
Step[17000] | Loss[0.9759413003921509] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.687961995601654] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.6626641750335693] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.9357893466949463] | Lr[8.000000000000003e-08]
Step[17500] | Loss[0.7463487982749939] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.291150838136673] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.8262472152709961] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.5212119221687317] | Lr[8.000000000000003e-08]
Step[18000] | Loss[0.7683721780776978] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.4461659789085388] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.505448579788208] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.822851300239563] | Lr[8.000000000000003e-08]
Step[18500] | Loss[0.6601478457450867] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.8523091077804565] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.6662086248397827] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.6340617537498474] | Lr[8.000000000000003e-08]
Step[19000] | Loss[0.49194270372390747] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.8528571128845215] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.6819740533828735] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.8112669587135315] | Lr[8.000000000000003e-08]
Step[19500] | Loss[0.6475246548652649] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.6185865998268127] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.6299842000007629] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.47126245498657227] | Lr[8.000000000000003e-08]
Step[20000] | Loss[0.8418576121330261] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.48523974418640137] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.6086000800132751] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.4480810761451721] | Lr[8.000000000000003e-08]
Step[20500] | Loss[0.8023821711540222] | Lr[8.000000000000003e-08]
Step[21000] | Loss[1.0798847675323486] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.5182174444198608] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.42085030674934387] | Lr[8.000000000000003e-08]
Step[21000] | Loss[0.5920240879058838] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.8693172931671143] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.4515458941459656] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.7915458083152771] | Lr[8.000000000000003e-08]
Step[21500] | Loss[0.5697115659713745] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.6904134154319763] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.661249577999115] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.4707238972187042] | Lr[8.000000000000003e-08]
Step[22000] | Loss[0.3384360671043396] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.8255194425582886] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.793139636516571] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.3548007905483246] | Lr[8.000000000000003e-08]
Step[22500] | Loss[0.5595458745956421] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.49651074409484863] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.5153442621231079] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.7288529276847839] | Lr[8.000000000000003e-08]
Step[23000] | Loss[0.5104624032974243] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.6286231875419617] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.7356359958648682] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.5488770008087158] | Lr[8.000000000000003e-08]
Step[23500] | Loss[0.47766807675361633] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.38700008392333984] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.6823878884315491] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.36703377962112427] | Lr[8.000000000000003e-08]
Step[24000] | Loss[0.7353841662406921] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.6430582404136658] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.7014012336730957] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.7039647698402405] | Lr[8.000000000000003e-08]
Step[24500] | Loss[0.712772786617279] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.6965794563293457] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.6665122509002686] | Lr[8.000000000000003e-08]
Step[25000] | Loss[0.8328342437744141] | Lr[8.000000000000003e-08]
Step[25000] | Loss[1.0850974321365356] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.46007096767425537] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.6532043218612671] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.6338316798210144] | Lr[8.000000000000003e-08]
Step[25500] | Loss[0.3418753147125244] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.2840575873851776] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.41795602440834045] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.4306444525718689] | Lr[8.000000000000003e-08]
Step[26000] | Loss[0.33251988887786865] | Lr[8.000000000000003e-08]
Step[26500] | Loss[1.1994307041168213] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.872816801071167] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.6647500991821289] | Lr[8.000000000000003e-08]
Step[26500] | Loss[0.7434373497962952] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.5119683742523193] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.650179922580719] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.4577445387840271] | Lr[8.000000000000003e-08]
Step[27000] | Loss[0.3692167401313782] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.3465852737426758] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.6699261665344238] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.5996658802032471] | Lr[8.000000000000003e-08]
Step[27500] | Loss[0.7204437255859375] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.5246866941452026] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.5316916108131409] | Lr[8.000000000000003e-08]
Step[28000] | Loss[0.486639142036438] | Lr[8.000000000000003e-08]Step[28000] | Loss[0.5543553233146667] | Lr[8.000000000000003e-08]

Step[28500] | Loss[0.45600879192352295] | Lr[8.000000000000003e-08]
Step[28500] | Loss[0.812753438949585] | Lr[8.000000000000003e-08]Step[28500] | Loss[0.7207388877868652] | Lr[8.000000000000003e-08]

Step[28500] | Loss[0.5232899785041809] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.6744289398193359] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.6372684836387634] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.5791628360748291] | Lr[8.000000000000003e-08]
Step[29000] | Loss[0.5664215683937073] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.47944214940071106] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.5641053915023804] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.639783501625061] | Lr[8.000000000000003e-08]
Step[29500] | Loss[0.6568481922149658] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.5336691737174988] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.42870938777923584] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.6809639930725098] | Lr[8.000000000000003e-08]
Step[30000] | Loss[0.5920327305793762] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.7258651256561279] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.4291589856147766] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.6329479217529297] | Lr[8.000000000000003e-08]
Step[30500] | Loss[0.5525197386741638] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.5287237763404846] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.6768684983253479] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.8875290155410767] | Lr[8.000000000000003e-08]
Step[31000] | Loss[0.4228837490081787] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.42770469188690186] | Lr[8.000000000000003e-08]
Step[31500] | Loss[1.0738836526870728] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.3323442041873932] | Lr[8.000000000000003e-08]
Step[31500] | Loss[0.7621296048164368] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.532071590423584] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.4480113685131073] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.6005221009254456] | Lr[8.000000000000003e-08]
Step[32000] | Loss[0.4767157733440399] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.8755186200141907] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.5385874509811401] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.7726234793663025] | Lr[8.000000000000003e-08]
Step[32500] | Loss[0.8109269738197327] | Lr[8.000000000000003e-08]
Step[33000] | Loss[1.0281095504760742] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.6575459837913513] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.2783218324184418] | Lr[8.000000000000003e-08]
Step[33000] | Loss[0.6494849324226379] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.3384738862514496] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.36839139461517334] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.8722703456878662] | Lr[8.000000000000003e-08]
Step[33500] | Loss[0.6007745862007141] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.5900906920433044] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.41368913650512695] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.6069000363349915] | Lr[8.000000000000003e-08]
Step[34000] | Loss[0.44401684403419495] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.5422793030738831] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.5505738258361816] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.8906617760658264] | Lr[8.000000000000003e-08]
Step[34500] | Loss[0.4466114640235901] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.6626620888710022] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.7765122652053833] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.5351704955101013] | Lr[8.000000000000003e-08]
Step[35000] | Loss[0.3774562478065491] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.49109092354774475] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.7370408773422241] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.9183830618858337] | Lr[8.000000000000003e-08]
Step[35500] | Loss[0.5345026850700378] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.6638010740280151] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.6119229793548584] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.7827606201171875] | Lr[8.000000000000003e-08]
Step[36000] | Loss[0.6451604962348938] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.47804296016693115] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.5000007748603821] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.4036596417427063] | Lr[8.000000000000003e-08]
Step[36500] | Loss[0.528440535068512] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.6195648908615112] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.5879661440849304] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.26842543482780457] | Lr[8.000000000000003e-08]
Step[37000] | Loss[0.5687299966812134] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.4984762370586395] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.675588071346283] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.8244308829307556] | Lr[8.000000000000003e-08]
Step[37500] | Loss[0.8075377941131592] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.813590943813324] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.4656958281993866] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.7880298495292664] | Lr[8.000000000000003e-08]
Step[38000] | Loss[0.6046066880226135] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.47268587350845337] | Lr[8.000000000000003e-08]
Step[38500] | Loss[1.0934902429580688] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.48752138018608093] | Lr[8.000000000000003e-08]
Step[38500] | Loss[0.7754194140434265] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.5134732127189636] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.8135403394699097] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.7066589593887329] | Lr[8.000000000000003e-08]
Step[39000] | Loss[0.7114367485046387] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.7590346336364746] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.3895028233528137] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.4278852343559265] | Lr[8.000000000000003e-08]
Step[39500] | Loss[0.5752467513084412] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.9342435002326965] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.45722484588623047] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.32364562153816223] | Lr[8.000000000000003e-08]
Step[40000] | Loss[0.5216872096061707] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.49333858489990234] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.5066858530044556] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.29420262575149536] | Lr[8.000000000000003e-08]
Step[40500] | Loss[0.7749131321907043] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.693088710308075] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.9415534138679504] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.5892833471298218] | Lr[8.000000000000003e-08]
Step[41000] | Loss[0.6340926289558411] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.7366775870323181] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.6440609693527222] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.7910764217376709] | Lr[8.000000000000003e-08]
Step[41500] | Loss[0.6537635326385498] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.3741227984428406] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.4481157064437866] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.7957930564880371] | Lr[8.000000000000003e-08]
Step[42000] | Loss[0.8807771801948547] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.6198604106903076] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.43117907643318176] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.7321786880493164] | Lr[8.000000000000003e-08]
Step[42500] | Loss[0.44827187061309814] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.5504688024520874] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.5177308320999146] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.46451690793037415] | Lr[8.000000000000003e-08]
Step[43000] | Loss[0.7660053372383118] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.5613241195678711] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.7156990766525269] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.6929298043251038] | Lr[8.000000000000003e-08]
Step[43500] | Loss[0.7699775099754333] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.7986997961997986] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.6532929539680481] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.5080923438072205] | Lr[8.000000000000003e-08]
Step[44000] | Loss[0.9164034128189087] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.8771609663963318] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.5330532193183899] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.7952020168304443] | Lr[8.000000000000003e-08]
Step[44500] | Loss[0.5227006077766418] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.3369373083114624] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.8506417870521545] | Lr[8.000000000000003e-08]
Step[45000] | Loss[1.2988314628601074] | Lr[8.000000000000003e-08]
Step[45000] | Loss[0.8040012121200562] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.6654952168464661] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.2767350971698761] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.808125376701355] | Lr[8.000000000000003e-08]
Step[45500] | Loss[0.5932047963142395] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.5833666920661926] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.5437443852424622] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.6911579966545105] | Lr[8.000000000000003e-08]
Step[46000] | Loss[0.4650425612926483] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.8083612322807312] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.2606823146343231] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.4569719433784485] | Lr[8.000000000000003e-08]
Step[46500] | Loss[0.5960798263549805] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.5935266613960266] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.41821029782295227] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.5308430194854736] | Lr[8.000000000000003e-08]
Step[47000] | Loss[0.30845433473587036] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.4396190345287323] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.7308049201965332] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.38507139682769775] | Lr[8.000000000000003e-08]
Step[47500] | Loss[0.2988085150718689] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.4342120289802551] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.21995298564434052] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.6096271276473999] | Lr[8.000000000000003e-08]
Step[48000] | Loss[0.7768513560295105] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.39370930194854736] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.422061949968338] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.40290072560310364] | Lr[8.000000000000003e-08]
Step[48500] | Loss[0.5387763381004333] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.6013315916061401] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.9598027467727661] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.7905533313751221] | Lr[8.000000000000003e-08]
Step[49000] | Loss[0.5543923377990723] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.5194424986839294] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.7639681696891785] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.49460405111312866] | Lr[8.000000000000003e-08]
Step[49500] | Loss[0.41882631182670593] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.6122173070907593] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.7800145745277405] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.6986030340194702] | Lr[8.000000000000003e-08]
Step[50000] | Loss[0.2246372103691101] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.6714993715286255] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.5176674127578735] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.5832511186599731] | Lr[8.000000000000003e-08]
Step[50500] | Loss[0.5332623720169067] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.5499227643013] | Lr[8.000000000000003e-08]
Step[51000] | Loss[1.015264868736267] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.698945164680481] | Lr[8.000000000000003e-08]
Step[51000] | Loss[0.9935999512672424] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.8967449069023132] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.708663821220398] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.6645694971084595] | Lr[8.000000000000003e-08]
Step[51500] | Loss[0.5567061901092529] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.7048035264015198] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.6592790484428406] | Lr[8.000000000000003e-08]
Step[52000] | Loss[0.6301192045211792] | Lr[8.000000000000003e-08]
Step[52000] | Loss[1.0975288152694702] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.6946538686752319] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.6606686115264893] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.5486868619918823] | Lr[8.000000000000003e-08]
Step[52500] | Loss[0.7165036201477051] | Lr[8.000000000000003e-08]
Step[53000] | Loss[0.6338534355163574] | Lr[8.000000000000003e-08]
Step[53000] | Loss[0.5963726043701172] | Lr[8.000000000000003e-08]Step[53000] | Loss[0.3294006288051605] | Lr[8.000000000000003e-08]

Step[53000] | Loss[0.6129311919212341] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.5794129371643066] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.5897104144096375] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.436877578496933] | Lr[8.000000000000003e-08]
Step[53500] | Loss[0.5106469392776489] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.5112732648849487] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.9038522839546204] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.39986366033554077] | Lr[8.000000000000003e-08]
Step[54000] | Loss[0.42941543459892273] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.49338698387145996] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.7500652074813843] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.6276677250862122] | Lr[8.000000000000003e-08]
Step[54500] | Loss[0.822064995765686] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.32385414838790894] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.5519260764122009] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.9617959856987] | Lr[8.000000000000003e-08]
Step[55000] | Loss[0.685097873210907] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.5048721432685852] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.6419312953948975] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.6332640051841736] | Lr[8.000000000000003e-08]
Step[55500] | Loss[0.680038332939148] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.31115639209747314] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.9662573933601379] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.624133825302124] | Lr[8.000000000000003e-08]
Step[56000] | Loss[0.6005518436431885] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.6205583214759827] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.6601845622062683] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.48968788981437683] | Lr[8.000000000000003e-08]
Step[56500] | Loss[0.6187306046485901] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.7191124558448792] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.5346391201019287] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.6948910355567932] | Lr[8.000000000000003e-08]
Step[57000] | Loss[0.31534335017204285] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.5464032292366028] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.6568247079849243] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.5860477089881897] | Lr[8.000000000000003e-08]
Step[57500] | Loss[0.3680330812931061] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.6504408121109009] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.46999990940093994] | Lr[8.000000000000003e-08]
Step[58000] | Loss[0.6372601389884949] | Lr[8.000000000000003e-08]
Step[58000] | Loss[1.1984301805496216] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.5654464364051819] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.5030924677848816] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.5012642741203308] | Lr[8.000000000000003e-08]
Step[58500] | Loss[0.5376341938972473] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.5863800048828125] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.678302526473999] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.3989018499851227] | Lr[8.000000000000003e-08]
Step[59000] | Loss[0.46987295150756836] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.6495901942253113] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.9820096492767334] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.6491313576698303] | Lr[8.000000000000003e-08]
Step[59500] | Loss[0.6317799091339111] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.5523892641067505] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.9603898525238037] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.8749663829803467] | Lr[8.000000000000003e-08]
Step[60000] | Loss[0.516939103603363] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.7388018369674683] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.7755991816520691] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.3171486258506775] | Lr[8.000000000000003e-08]
Step[60500] | Loss[0.7493034601211548] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.5694336295127869] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.9522998332977295] | Lr[8.000000000000003e-08]
Step[61000] | Loss[0.5409551858901978] | Lr[8.000000000000003e-08]
Step[61000] | Loss[1.0721195936203003] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.40941447019577026] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.7326788306236267] | Lr[8.000000000000003e-08]
Step[61500] | Loss[0.7323154807090759] | Lr[8.000000000000003e-08]
Step[61500] | Loss[1.1069285869598389] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.3040020167827606] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.6005287766456604] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.3878338634967804] | Lr[8.000000000000003e-08]
Step[62000] | Loss[0.6027346849441528] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.5489364266395569] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.7085161209106445] | Lr[8.000000000000003e-08]
Step[62500] | Loss[1.2994340658187866] | Lr[8.000000000000003e-08]
Step[62500] | Loss[0.3353113830089569] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.7957363128662109] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.5171399712562561] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.5076196789741516] | Lr[8.000000000000003e-08]
Step[63000] | Loss[0.8388813138008118] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.7055651545524597] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.4050358831882477] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.7045294046401978] | Lr[8.000000000000003e-08]
Step[63500] | Loss[0.28869715332984924] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.3957480192184448] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.43514353036880493] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.33463430404663086] | Lr[8.000000000000003e-08]
Step[64000] | Loss[0.5548456311225891] | Lr[8.000000000000003e-08]
Step[64500] | Loss[1.0009151697158813] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.6920640468597412] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.8385862708091736] | Lr[8.000000000000003e-08]
Step[64500] | Loss[0.3776727318763733] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.6051592230796814] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.5983728766441345] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.4071982502937317] | Lr[8.000000000000003e-08]
Step[65000] | Loss[0.4357864260673523] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.9595404863357544] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.6671677827835083] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.5688222050666809] | Lr[8.000000000000003e-08]
Step[65500] | Loss[0.55104660987854] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.8202396035194397] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.4913543462753296] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.8106328248977661] | Lr[8.000000000000003e-08]
Step[66000] | Loss[0.36816731095314026] | Lr[8.000000000000003e-08]
Step[66500] | Loss[0.5796753168106079] | Lr[8.000000000000003e-08]Step[66500] | Loss[0.264717161655426] | Lr[8.000000000000003e-08]

Step[66500] | Loss[0.6032631993293762] | Lr[8.000000000000003e-08]
Step[66500] | Loss[0.6405221223831177] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.436397910118103] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.5998136401176453] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.5115035772323608] | Lr[8.000000000000003e-08]
Step[67000] | Loss[0.667348325252533] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.561436116695404] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.4917423129081726] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.6955779194831848] | Lr[8.000000000000003e-08]
Step[67500] | Loss[0.3491200804710388] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.7832301259040833] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.700843095779419] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.4234640598297119] | Lr[8.000000000000003e-08]
Step[68000] | Loss[0.48356205224990845] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.6024917364120483] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.6142553687095642] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.64455646276474] | Lr[8.000000000000003e-08]
Step[68500] | Loss[0.7064307928085327] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.6346111297607422] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.4491075575351715] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.21643318235874176] | Lr[8.000000000000003e-08]
Step[69000] | Loss[0.4133935570716858] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.49373573064804077] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.23214936256408691] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.590582013130188] | Lr[8.000000000000003e-08]
Step[69500] | Loss[0.8990294933319092] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.4696338474750519] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.646980881690979] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.2572997510433197] | Lr[8.000000000000003e-08]
Step[70000] | Loss[0.7100445032119751] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.49821460247039795] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.6285403370857239] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.5035672783851624] | Lr[8.000000000000003e-08]
Step[70500] | Loss[0.49760428071022034] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.41553694009780884] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.43805548548698425] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.2025672048330307] | Lr[8.000000000000003e-08]
Step[71000] | Loss[0.6506431698799133] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.6123262643814087] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.857404351234436] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.9854153394699097] | Lr[8.000000000000003e-08]
Step[71500] | Loss[0.4771168828010559] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.7317560315132141] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.7334514856338501] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.3993082046508789] | Lr[8.000000000000003e-08]
Step[72000] | Loss[0.40487077832221985] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.6776428818702698] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.6745372414588928] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.41923609375953674] | Lr[8.000000000000003e-08]
Step[72500] | Loss[0.6260123252868652] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.6623919606208801] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.7517989277839661] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.7703107595443726] | Lr[8.000000000000003e-08]
Step[73000] | Loss[0.4974559247493744] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.6192116141319275] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.6534552574157715] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.9303357601165771] | Lr[8.000000000000003e-08]
Step[73500] | Loss[0.5246108770370483] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.6547344326972961] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.8258557319641113] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.9658685922622681] | Lr[8.000000000000003e-08]
Step[74000] | Loss[0.7126659750938416] | Lr[8.000000000000003e-08]
Step[74500] | Loss[0.6063913702964783] | Lr[8.000000000000003e-08]
Step[74500] | Loss[0.5197617411613464] | Lr[8.000000000000003e-08]Step[74500] | Loss[0.7469097375869751] | Lr[8.000000000000003e-08]

Step[74500] | Loss[0.5106415748596191] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.5747742056846619] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.6419987082481384] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.6689715385437012] | Lr[8.000000000000003e-08]
Step[75000] | Loss[0.8706446290016174] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.3172876834869385] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.48428529500961304] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.5017642378807068] | Lr[8.000000000000003e-08]
Step[75500] | Loss[0.6554349064826965] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.40351659059524536] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.41806915402412415] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.8411375880241394] | Lr[8.000000000000003e-08]
Step[76000] | Loss[0.6370458602905273] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.5052538514137268] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.20858070254325867] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.29486149549484253] | Lr[8.000000000000003e-08]
Step[76500] | Loss[0.7765185236930847] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.6427541375160217] | Lr[8.000000000000003e-08]
Step[77000] | Loss[1.1659950017929077] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.8626424074172974] | Lr[8.000000000000003e-08]
Step[77000] | Loss[0.5679157972335815] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.6745418310165405] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.4244286119937897] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.5757026076316833] | Lr[8.000000000000003e-08]
Step[77500] | Loss[0.5675595998764038] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.36487486958503723] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.40952861309051514] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.6730954647064209] | Lr[8.000000000000003e-08]
Step[78000] | Loss[0.5442697405815125] | Lr[8.000000000000003e-08]
Labels:  tensor([2, 2, 1, 1, 1, 4, 4, 4, 3, 2, 0, 3, 3, 4, 0, 3], device='cuda:0')
Preds:  tensor([3, 3, 3, 1, 0, 4, 4, 4, 2, 2, 2, 2, 3, 4, 0, 3], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0002,     0.0141,     0.4949,     0.4907],
        [    0.0004,     0.0057,     0.3487,     0.6413,     0.0038],
        [    0.0043,     0.0318,     0.3798,     0.4995,     0.0847],
        [    0.1151,     0.5125,     0.3609,     0.0110,     0.0004],
        [    0.6065,     0.3776,     0.0157,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0007,     0.0519,     0.9473],
        [    0.0004,     0.0002,     0.0011,     0.0378,     0.9605],
        [    0.0002,     0.0003,     0.0043,     0.1050,     0.8903],
        [    0.0018,     0.0255,     0.8817,     0.0829,     0.0081],
        [    0.0039,     0.0430,     0.5447,     0.4009,     0.0075],
        [    0.0462,     0.4382,     0.4760,     0.0370,     0.0026],
        [    0.0056,     0.0856,     0.7268,     0.1579,     0.0241],
        [    0.0006,     0.0049,     0.1447,     0.5694,     0.2803],
        [    0.0002,     0.0015,     0.0033,     0.2499,     0.7451],
        [    0.9306,     0.0677,     0.0016,     0.0000,     0.0000],
        [    0.0001,     0.0001,     0.0064,     0.8929,     0.1005]],
       device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
Labels:  Labels:  tensor([3, 0, 0, 1, 0, 3, 0, 0, 3, 1, 4, 4, 4, 1, 2, 0], device='cuda:1')
------------------------
Preds:  tensor([4, 1, 2, 1, 0, 2, 0, 1, 3, 1, 4, 3, 4, 1, 2, 0], device='cuda:1')
Labels:  tensor([0, 4, 2, 3, 0, 1, 2, 2, 3, 2, 2, 4, 4, 1, 1, 2], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0003,     0.0071,     0.3195,     0.6728],
        [    0.3755,     0.5345,     0.0893,     0.0006,     0.0002],
        [    0.1222,     0.3330,     0.5139,     0.0286,     0.0023],
        [    0.4185,     0.4851,     0.0943,     0.0018,     0.0004],
        [    0.8193,     0.1605,     0.0200,     0.0002,     0.0000],
        [    0.0013,     0.0201,     0.7577,     0.2180,     0.0029],
        [    0.9480,     0.0499,     0.0020,     0.0001,     0.0000],
        [    0.3876,     0.5916,     0.0200,     0.0006,     0.0002],
        [    0.0002,     0.0038,     0.3712,     0.5480,     0.0768],
        [    0.2022,     0.4414,     0.3392,     0.0155,     0.0017],
        [    0.1655,     0.1140,     0.2563,     0.1831,     0.2810],
        [    0.0395,     0.0346,     0.3109,     0.5367,     0.0782],
        [    0.0002,     0.0002,     0.0032,     0.0817,     0.9147],
        [    0.1849,     0.6953,     0.1073,     0.0107,     0.0019],
Preds:  tensor([0, 4, 1, 4, 0, 2, 3, 3, 2, 2, 0, 4, 4, 2, 1, 4], device='cuda:1')
        [    0.0009,     0.0441,     0.9518,     0.0031,     0.0001],
        [    0.9766,     0.0229,     0.0004,     0.0001,     0.0001]],
       device='cuda:1')
Outputs:  tensor([[    0.8765,     0.1195,     0.0040,     0.0000,     0.0000],
        [    0.0002,     0.0001,     0.0013,     0.0953,     0.9031],
        [    0.0361,     0.6921,     0.2322,     0.0326,     0.0070],
        [    0.0001,     0.0000,     0.0001,     0.0137,     0.9861],
        [    0.9856,     0.0127,     0.0013,     0.0002,     0.0002],
        [    0.0547,     0.4175,     0.4219,     0.1000,     0.0059],
        [    0.0151,     0.1376,     0.2637,     0.3271,     0.2565],
        [    0.0003,     0.0035,     0.3458,     0.6220,     0.0284],
        [    0.1160,     0.4174,     0.4406,     0.0256,     0.0004],
        [    0.0034,     0.0557,     0.6965,     0.2400,     0.0044],
        [    0.7304,     0.2339,     0.0338,     0.0012,     0.0006],
        [    0.0000,     0.0000,     0.0005,     0.0376,     0.9619],
        [    0.0004,     0.0002,     0.0004,     0.0021,     0.9970],
        [    0.0126,     0.4073,     0.4904,     0.0844,     0.0054],
Metric:  tensor(0.6250, device='cuda:1')
        [    0.0035,     0.9915,     0.0044,     0.0004,     0.0002],
        [    0.0117,     0.0041,     0.0390,     0.2179,     0.7273]],
       device='cuda:1')
------------------------
Metric:  tensor(0.4375, device='cuda:1')
tensor([3, 1, 1, 4, 0, 2, 4, 1, 3, 0, 2, 3, 2, 2, 4, 4], device='cuda:0')
------------------------
Preds:  tensor([3, 1, 0, 4, 0, 2, 4, 0, 3, 0, 1, 4, 1, 2, 3, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0005,     0.0191,     0.9798,     0.0005],
        [    0.4387,     0.4673,     0.0929,     0.0010,     0.0002],
        [    0.7666,     0.2011,     0.0298,     0.0021,     0.0004],
        [    0.0014,     0.0005,     0.0039,     0.0534,     0.9409],
        [    0.8725,     0.0600,     0.0372,     0.0163,     0.0140],
        [    0.0015,     0.0970,     0.6709,     0.2246,     0.0059],
        [    0.0024,     0.0017,     0.0076,     0.0767,     0.9116],
        [    0.5360,     0.4361,     0.0276,     0.0002,     0.0001],
        [    0.0001,     0.0009,     0.2087,     0.7735,     0.0168],
        [    0.6826,     0.2311,     0.0837,     0.0025,     0.0001],
        [    0.3539,     0.5149,     0.1291,     0.0016,     0.0003],
        [    0.0008,     0.0007,     0.0190,     0.4841,     0.4955],
        [    0.0223,     0.7233,     0.2532,     0.0011,     0.0000],
        [    0.0059,     0.0676,     0.6335,     0.2772,     0.0158],
        [    0.0001,     0.0001,     0.0075,     0.6381,     0.3542],
        [    0.0023,     0.0008,     0.0077,     0.2239,     0.7653]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 0, 4, 0, 0, 0, 1, 2], device='cuda:1')
Preds:  tensor([2, 0, 4, 4, 4, 0, 0, 2, 4, 2, 4, 0, 0, 0, 2, 4], device='cuda:1')
Outputs:  tensor([[    0.0006,     0.0030,     0.9773,     0.0165,     0.0025],
        [    0.6132,     0.3403,     0.0423,     0.0030,     0.0012],
        [    0.0003,     0.0014,     0.0008,     0.0150,     0.9825],
        [    0.0001,     0.0000,     0.0001,     0.0132,     0.9865],
        [    0.0002,     0.0003,     0.0075,     0.2231,     0.7689],
        [    0.6526,     0.3236,     0.0222,     0.0009,     0.0006],
        [    0.8483,     0.1436,     0.0077,     0.0003,     0.0001],
        [    0.0478,     0.2928,     0.6333,     0.0252,     0.0007],
        [    0.0002,     0.0002,     0.0055,     0.1699,     0.8241],
        [    0.0547,     0.4422,     0.4929,     0.0097,     0.0004],
        [    0.0032,     0.0031,     0.0590,     0.4545,     0.4803],
        [    0.5208,     0.2324,     0.1722,     0.0394,     0.0351],
        [    0.5228,     0.4484,     0.0284,     0.0003,     0.0001],
        [    0.8786,     0.1205,     0.0008,     0.0000,     0.0001],
        [    0.0126,     0.2623,     0.6995,     0.0255,     0.0002],
        [    0.0610,     0.0621,     0.1087,     0.1040,     0.6641]],
       device='cuda:1')
Metric:  tensor(0.8125, device='cuda:1')
------------------------
Labels:  tensor([0, 1, 4, 2, 3, 2, 1, 0, 2, 2, 2, 0, 2, 4, 3, 4], device='cuda:0')
Preds:  tensor([1, 1, 4, 1, 2, 2, 1, 0, 1, 3, 2, 0, 3, 4, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.4637,     0.5074,     0.0287,     0.0001,     0.0000],
        [    0.1039,     0.8912,     0.0047,     0.0001,     0.0001],
        [    0.0004,     0.0008,     0.0133,     0.1818,     0.8038],
        [    0.3182,     0.5340,     0.1447,     0.0028,     0.0003],
        [    0.1694,     0.3538,     0.4316,     0.0337,     0.0115],
        [    0.0406,     0.0245,     0.9113,     0.0205,     0.0032],
        [    0.3204,     0.5734,     0.1058,     0.0004,     0.0001],
        [    0.9609,     0.0329,     0.0046,     0.0008,     0.0008],
        [    0.4080,     0.4114,     0.1774,     0.0028,     0.0003],
        [    0.0004,     0.0041,     0.2680,     0.6490,     0.0785],
        [    0.0010,     0.0134,     0.4828,     0.4411,     0.0618],
        [    0.9508,     0.0475,     0.0015,     0.0001,     0.0001],
        [    0.0004,     0.0099,     0.2026,     0.7680,     0.0191],
        [    0.0044,     0.0022,     0.0277,     0.2217,     0.7440],
        [    0.6522,     0.2896,     0.0572,     0.0009,     0.0001],
        [    0.0002,     0.0002,     0.0002,     0.0101,     0.9893]],
       device='cuda:0')
Labels:  tensor([1, 1, 1, 1, 2, 1, 0, 0, 4, 4, 2, 1, 3, 0, 0, 1], device='cuda:0')
Metric:  tensor(0.5625, device='cuda:0')
Preds:  tensor([3, 0, 1, 4, 1, 2, 0, 0, 4, 4, 2, 1, 2, 0, 0, 1], device='cuda:0')
------------------------
Outputs:  tensor([[    0.0035,     0.0327,     0.3522,     0.4845,     0.1272],
        [    0.3909,     0.3584,     0.2451,     0.0052,     0.0004],
        [    0.0599,     0.5523,     0.3828,     0.0049,     0.0002],
        [    0.0394,     0.0269,     0.0822,     0.1844,     0.6671],
        [    0.0248,     0.7036,     0.2455,     0.0225,     0.0037],
        [    0.0064,     0.1374,     0.8135,     0.0417,     0.0009],
        [    0.9385,     0.0553,     0.0040,     0.0008,     0.0014],
        [    0.5783,     0.4123,     0.0093,     0.0001,     0.0000],
        [    0.0003,     0.0002,     0.0022,     0.1031,     0.8943],
        [    0.0006,     0.0003,     0.0034,     0.0590,     0.9367],
        [    0.0054,     0.1324,     0.7809,     0.0803,     0.0008],
        [    0.4157,     0.4701,     0.1112,     0.0021,     0.0008],
        [    0.0011,     0.0184,     0.8090,     0.1714,     0.0002],
        [    0.5987,     0.3390,     0.0588,     0.0033,     0.0002],
        [    0.5251,     0.3141,     0.1174,     0.0252,     0.0182],
        [    0.2941,     0.5079,     0.1948,     0.0031,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 1, 3, 4, 3, 0, 3, 1, 1, 0, 0, 3, 2, 0], device='cuda:1')
Preds:  tensor([3, 2, 3, 1, 2, 4, 2, 1, 4, 1, 1, 0, 0, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0019,     0.0102,     0.3498,     0.6298,     0.0083],
        [    0.0012,     0.0392,     0.7374,     0.2186,     0.0036],
        [    0.0074,     0.0061,     0.0816,     0.9025,     0.0024],
        [    0.3305,     0.4651,     0.1996,     0.0044,     0.0003],
        [    0.0008,     0.0239,     0.9443,     0.0306,     0.0004],
        [    0.0008,     0.0008,     0.0142,     0.1851,     0.7991],
        [    0.0107,     0.0950,     0.5913,     0.2807,     0.0222],
        [    0.1940,     0.5488,     0.2556,     0.0015,     0.0001],
        [    0.0001,     0.0001,     0.0050,     0.3723,     0.6226],
        [    0.1704,     0.5149,     0.2870,     0.0238,     0.0039],
        [    0.1356,     0.5038,     0.3551,     0.0053,     0.0003],
        [    0.8364,     0.1553,     0.0080,     0.0002,     0.0001],
        [    0.6130,     0.2831,     0.0949,     0.0074,     0.0016],
        [    0.0100,     0.0448,     0.2011,     0.5819,     0.1622],
        [    0.0212,     0.1628,     0.6664,     0.1449,     0.0047],
        [    0.0141,     0.1450,     0.6961,     0.1440,     0.0009]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([2, 3, 4, 0, 1, 3, 4, 3, 4, 2, 1, 0, 4, 4, 4, 4], device='cuda:1')
Preds:  tensor([2, 3, 4, 1, 1, 4, 4, 2, 4, 3, 1, 0, 4, 4, 3, 4], device='cuda:1')
Outputs:  tensor([[    0.0293,     0.3651,     0.5933,     0.0120,     0.0004],
        [    0.0002,     0.0011,     0.1183,     0.7650,     0.1153],
        [    0.0006,     0.0003,     0.0010,     0.0126,     0.9855],
        [    0.2307,     0.7276,     0.0410,     0.0004,     0.0003],
        [    0.3242,     0.5392,     0.1258,     0.0081,     0.0025],
        [    0.0009,     0.0012,     0.0291,     0.4001,     0.5688],
        [    0.0005,     0.0002,     0.0013,     0.0095,     0.9885],
        [    0.0068,     0.2958,     0.6854,     0.0119,     0.0002],
        [    0.0004,     0.0004,     0.0001,     0.0017,     0.9975],
        [    0.0002,     0.0036,     0.3942,     0.5928,     0.0092],
        [    0.3413,     0.5421,     0.1149,     0.0015,     0.0002],
        [    0.8877,     0.1080,     0.0040,     0.0002,     0.0001],
        [    0.0001,     0.0001,     0.0015,     0.0871,     0.9112],
        [    0.0007,     0.0005,     0.0032,     0.0854,     0.9103],
        [    0.0010,     0.0047,     0.2106,     0.6552,     0.1284],
        [    0.0001,     0.0001,     0.0012,     0.1105,     0.8881]],
       device='cuda:1')
Metric:  tensor(0.6875, device='cuda:1')
------------------------
Labels:  tensor([3, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 3, 3, 4, 4, 2], device='cuda:0')
Preds:  tensor([3, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 4, 3, 4, 4, 0], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0003,     0.0142,     0.6503,     0.3351],
        [    0.0013,     0.0184,     0.7013,     0.2549,     0.0241],
        [    0.0012,     0.0263,     0.6747,     0.2913,     0.0064],
        [    0.0226,     0.2154,     0.6960,     0.0650,     0.0009],
        [    0.3489,     0.5941,     0.0564,     0.0005,     0.0001],
        [    0.0522,     0.0901,     0.4182,     0.3321,     0.1074],
        [    0.1843,     0.6516,     0.1610,     0.0029,     0.0002],
        [    0.9786,     0.0204,     0.0009,     0.0001,     0.0001],
        [    0.0006,     0.0152,     0.7428,     0.2377,     0.0037],
        [    0.0368,     0.2560,     0.4108,     0.2590,     0.0373],
        [    0.6993,     0.2705,     0.0300,     0.0002,     0.0001],
        [    0.0000,     0.0000,     0.0002,     0.0104,     0.9893],
        [    0.0003,     0.0077,     0.4723,     0.5173,     0.0024],
        [    0.0002,     0.0003,     0.0074,     0.2513,     0.7408],
        [    0.0058,     0.0100,     0.0931,     0.1653,     0.7259],
        [    0.7054,     0.2640,     0.0298,     0.0007,     0.0002]],
       device='cuda:0')
Labels:  tensor([4, 0, 4, 0, 4, 1, 1, 3, 0, 4, 4, 3, 0, 2, 3, 0], device='cuda:0')
Metric:  tensor(0.6250, device='cuda:0')
Preds:  tensor([4, 0, 4, 0, 4, 1, 2, 4, 0, 3, 4, 3, 0, 1, 4, 0], device='cuda:0')
------------------------
Outputs:  tensor([[    0.0002,     0.0001,     0.0016,     0.0837,     0.9143],
        [    0.6693,     0.2852,     0.0442,     0.0011,     0.0002],
        [    0.0003,     0.0002,     0.0056,     0.2377,     0.7563],
        [    0.8681,     0.1272,     0.0046,     0.0001,     0.0001],
        [    0.0581,     0.0249,     0.0414,     0.0859,     0.7897],
        [    0.0040,     0.9916,     0.0041,     0.0002,     0.0001],
        [    0.0862,     0.4173,     0.4663,     0.0287,     0.0014],
        [    0.0177,     0.0833,     0.1567,     0.2879,     0.4545],
        [    0.9805,     0.0188,     0.0006,     0.0001,     0.0001],
        [    0.0011,     0.0059,     0.1733,     0.5393,     0.2803],
        [    0.0010,     0.0081,     0.0050,     0.1518,     0.8341],
        [    0.0010,     0.0041,     0.1118,     0.5349,     0.3482],
        [    0.7779,     0.2153,     0.0067,     0.0001,     0.0000],
        [    0.0381,     0.5447,     0.3688,     0.0474,     0.0011],
        [    0.0001,     0.0001,     0.0036,     0.2975,     0.6986],
        [    0.8514,     0.1417,     0.0067,     0.0002,     0.0001]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([3, 2, 3, 2, 3, 0, 2, 4, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:1')
Preds:  tensor([4, 2, 2, 2, 4, 0, 1, 4, 2, 0, 1, 2, 0, 0, 0, 0], device='cuda:1')
Outputs:  tensor([[    0.0002,     0.0004,     0.0091,     0.2554,     0.7348],
        [    0.0016,     0.0344,     0.8342,     0.1281,     0.0017],
        [    0.0013,     0.0308,     0.8909,     0.0762,     0.0007],
        [    0.0032,     0.0274,     0.9680,     0.0012,     0.0001],
        [    0.0001,     0.0001,     0.0021,     0.0869,     0.9107],
        [    0.7838,     0.2054,     0.0105,     0.0002,     0.0001],
        [    0.2138,     0.4573,     0.2703,     0.0518,     0.0069],
        [    0.0005,     0.0008,     0.0067,     0.1280,     0.8641],
        [    0.0086,     0.1401,     0.7984,     0.0525,     0.0004],
        [    0.7154,     0.1706,     0.0688,     0.0212,     0.0240],
        [    0.2203,     0.6005,     0.1773,     0.0019,     0.0001],
        [    0.2431,     0.2767,     0.4054,     0.0614,     0.0133],
        [    0.9947,     0.0032,     0.0010,     0.0004,     0.0008],
        [    0.8548,     0.1114,     0.0241,     0.0048,     0.0050],
        [    0.9894,     0.0103,     0.0002,     0.0000,     0.0001],
        [    0.9972,     0.0019,     0.0003,     0.0002,     0.0005]],
       device='cuda:1')
Metric:  tensor(0.5625, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 4, 2, 3, 2, 4, 1, 2, 1, 1, 2, 0, 1, 0, 1], device='cuda:1')
Preds:  tensor([4, 4, 4, 1, 3, 3, 4, 1, 2, 2, 0, 2, 0, 0, 0, 2], device='cuda:1')
Outputs:  tensor([[    0.0003,     0.0003,     0.0029,     0.0698,     0.9267],
        [    0.0003,     0.0002,     0.0045,     0.2345,     0.7605],
        [    0.0032,     0.0009,     0.0067,     0.1931,     0.7960],
        [    0.2152,     0.4556,     0.3248,     0.0039,     0.0004],
        [    0.0017,     0.0132,     0.3782,     0.5283,     0.0787],
        [    0.0001,     0.0004,     0.0526,     0.8678,     0.0791],
        [    0.0008,     0.0005,     0.0013,     0.0056,     0.9919],
        [    0.0830,     0.4648,     0.4157,     0.0348,     0.0018],
        [    0.0015,     0.0548,     0.8637,     0.0799,     0.0002],
        [    0.0348,     0.3015,     0.6173,     0.0454,     0.0010],
        [    0.7519,     0.2067,     0.0358,     0.0049,     0.0007],
        [    0.0096,     0.2968,     0.6803,     0.0129,     0.0004],
        [    0.9778,     0.0215,     0.0004,     0.0001,     0.0002],
        [    0.7191,     0.2597,     0.0207,     0.0004,     0.0001],
        [    0.7191,     0.2678,     0.0128,     0.0003,     0.0001],
        [    0.0079,     0.0778,     0.6376,     0.2696,     0.0070]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Labels:  tensor([4, 1, 1, 0, 0, 1, 2, 4, 2, 4, 4, 1, 0, 1, 2, 1], device='cuda:0')
Preds:  tensor([4, 1, 0, 0, 0, 1, 2, 4, 2, 4, 4, 0, 0, 0, 3, 0], device='cuda:0')
Outputs:  tensor([[    0.0007,     0.0006,     0.0026,     0.0366,     0.9596],
        [    0.3588,     0.5827,     0.0580,     0.0004,     0.0001],
        [    0.9632,     0.0358,     0.0009,     0.0001,     0.0000],
        [    0.6718,     0.3053,     0.0214,     0.0008,     0.0008],
        [    0.7549,     0.2274,     0.0165,     0.0007,     0.0005],
        [    0.1210,     0.6383,     0.2392,     0.0014,     0.0001],
        [    0.0678,     0.1693,     0.4631,     0.2689,     0.0309],
        [    0.0001,     0.0002,     0.0006,     0.1251,     0.8740],
        [    0.0169,     0.3092,     0.6548,     0.0191,     0.0001],
        [    0.0005,     0.0004,     0.0046,     0.1659,     0.8286],
        [    0.0059,     0.0067,     0.0218,     0.0786,     0.8870],
        [    0.3789,     0.0674,     0.0969,     0.1041,     0.3526],
        [    0.9553,     0.0440,     0.0006,     0.0000,     0.0000],
        [    0.6156,     0.3174,     0.0646,     0.0021,     0.0003],
        [    0.0252,     0.0473,     0.2375,     0.6034,     0.0867],
        [    0.6499,     0.2741,     0.0657,     0.0078,     0.0024]],
       device='cuda:0')
Metric:  tensor(0.6875, device='cuda:0')
------------------------
Labels:  tensor([2, 0, 1, 3, 4, 2, 0, 0, 2, 1, 0, 1, 1, 2, 3, 4], device='cuda:0')
Preds:  tensor([4, 0, 2, 2, 4, 2, 0, 0, 1, 0, 0, 0, 0, 1, 3, 4], device='cuda:0')
Outputs:  tensor([[    0.0003,     0.0013,     0.0319,     0.3303,     0.6361],
        [    0.7530,     0.2325,     0.0138,     0.0005,     0.0002],
        [    0.0026,     0.0841,     0.8619,     0.0512,     0.0002],
        [    0.0014,     0.0555,     0.8877,     0.0552,     0.0002],
        [    0.0049,     0.0010,     0.0022,     0.0126,     0.9793],
        [    0.0053,     0.1194,     0.6486,     0.2227,     0.0039],
        [    0.4949,     0.4595,     0.0447,     0.0008,     0.0001],
        [    0.7679,     0.2062,     0.0257,     0.0002,     0.0000],
        [    0.0289,     0.9093,     0.0518,     0.0059,     0.0042],
        [    0.7420,     0.1532,     0.0885,     0.0117,     0.0046],
        [    0.9928,     0.0069,     0.0002,     0.0000,     0.0001],
        [    0.6658,     0.2603,     0.0718,     0.0020,     0.0001],
        [    0.7796,     0.1569,     0.0537,     0.0070,     0.0028],
        [    0.0082,     0.7016,     0.2610,     0.0276,     0.0015],
        [    0.0595,     0.1067,     0.3030,     0.3189,     0.2119],
        [    0.0003,     0.0004,     0.0003,     0.0264,     0.9727]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Labels:  tensor([1, 1, 2, 1, 1, 4, 1, 4, 4, 1, 3, 2, 4, 3, 3, 4], device='cuda:1')
Preds:  tensor([2, 1, 1, 0, 1, 4, 0, 4, 4, 1, 2, 2, 4, 2, 4, 3], device='cuda:1')
Outputs:  tensor([[    0.0174,     0.0608,     0.4305,     0.4085,     0.0828],
        [    0.1285,     0.5197,     0.3411,     0.0105,     0.0003],
        [    0.1848,     0.4098,     0.3560,     0.0489,     0.0006],
        [    0.6503,     0.2958,     0.0530,     0.0007,     0.0001],
        [    0.2857,     0.6640,     0.0501,     0.0002,     0.0000],
        [    0.0015,     0.0006,     0.0020,     0.0596,     0.9363],
        [    0.7744,     0.1954,     0.0285,     0.0012,     0.0006],
        [    0.0002,     0.0004,     0.0123,     0.2600,     0.7272],
        [    0.0022,     0.0008,     0.0031,     0.0844,     0.9096],
        [    0.4167,     0.5325,     0.0501,     0.0007,     0.0001],
        [    0.0027,     0.0532,     0.7783,     0.1654,     0.0004],
        [    0.0008,     0.0284,     0.5527,     0.4113,     0.0068],
        [    0.0004,     0.0005,     0.0069,     0.1922,     0.8000],
        [    0.0002,     0.0073,     0.6443,     0.3453,     0.0029],
        [    0.0004,     0.0004,     0.0092,     0.1552,     0.8348],
        [    0.0002,     0.0004,     0.0233,     0.5577,     0.4184]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Labels:  tensor([4, 4, 2, 0, 0, 4, 4, 1, 4, 2, 0, 1, 1, 2, 2, 1], device='cuda:1')
Preds:  tensor([4, 4, 1, 2, 0, 4, 4, 1, 4, 2, 3, 1, 2, 1, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.0005,     0.0004,     0.0078,     0.1710,     0.8204],
        [    0.0002,     0.0003,     0.0051,     0.1422,     0.8521],
        [    0.1617,     0.7967,     0.0366,     0.0039,     0.0011],
        [    0.0923,     0.3108,     0.4669,     0.1153,     0.0147],
        [    0.4391,     0.1132,     0.1923,     0.1013,     0.1541],
        [    0.0012,     0.0004,     0.0008,     0.0141,     0.9835],
        [    0.0003,     0.0004,     0.0032,     0.0768,     0.9194],
        [    0.2797,     0.5626,     0.1572,     0.0004,     0.0001],
        [    0.0001,     0.0001,     0.0027,     0.0988,     0.8983],
        [    0.0587,     0.1819,     0.5825,     0.1712,     0.0057],
        [    0.0023,     0.0241,     0.2630,     0.4987,     0.2119],
        [    0.2570,     0.3937,     0.3072,     0.0295,     0.0125],
        [    0.0006,     0.0218,     0.9288,     0.0480,     0.0008],
        [    0.1215,     0.5230,     0.3510,     0.0044,     0.0001],
        [    0.0057,     0.1868,     0.5442,     0.2432,     0.0201],
        [    0.0065,     0.2052,     0.7307,     0.0566,     0.0009]],
       device='cuda:1')
Metric:  tensor(0.6250, device='cuda:1')
------------------------
Mean loss[0.9334318255901104] | Mean metric[0.6059663250366032]
Stupid loss[0.0] | Naive soulution metric[0.2]
Labels:  tensor([1, 1, 2, 4, 2, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 0], device='cuda:0')
Preds:  tensor([1, 2, 2, 4, 0, 4, 0, 4, 4, 2, 0, 2, 0, 3, 4, 1], device='cuda:0')
Outputs:  tensor([[    0.1980,     0.5619,     0.2392,     0.0008,     0.0001],
        [    0.0374,     0.3158,     0.6135,     0.0324,     0.0010],
        [    0.0004,     0.0046,     0.9899,     0.0048,     0.0003],
        [    0.0002,     0.0002,     0.0002,     0.0407,     0.9587],
        [    0.8847,     0.0964,     0.0174,     0.0009,     0.0006],
        [    0.0007,     0.0002,     0.0006,     0.0151,     0.9834],
        [    0.9759,     0.0235,     0.0003,     0.0001,     0.0002],
        [    0.0001,     0.0000,     0.0004,     0.0244,     0.9751],
        [    0.0001,     0.0001,     0.0049,     0.3102,     0.6847],
        [    0.0414,     0.4213,     0.5316,     0.0057,     0.0001],
        [    0.6442,     0.2971,     0.0568,     0.0017,     0.0001],
        [    0.0048,     0.0780,     0.7468,     0.1666,     0.0037],
        [    0.8162,     0.1450,     0.0305,     0.0050,     0.0033],
        [    0.0005,     0.0006,     0.0158,     0.5457,     0.4373],
        [    0.0151,     0.0068,     0.0229,     0.0579,     0.8973],
        [    0.0812,     0.4749,     0.4168,     0.0268,     0.0003]],
       device='cuda:0')
Metric:  tensor(0.5000, device='cuda:0')
------------------------
Mean loss[0.9344747907478906] | Mean metric[0.608803074670571]
Stupid loss[0.0] | Naive soulution metric[0.2]
Labels:  tensor([4, 2, 2, 0, 0, 3, 3, 0, 0, 0, 0, 1, 4, 2, 0, 4], device='cuda:0')
Preds:  tensor([4, 4, 2, 0, 0, 3, 3, 0, 0, 0, 1, 0, 3, 2, 0, 4], device='cuda:0')
Outputs:  tensor([[    0.0001,     0.0001,     0.0002,     0.0347,     0.9649],
        [    0.0002,     0.0003,     0.0051,     0.1484,     0.8460],
        [    0.0980,     0.2179,     0.4534,     0.2260,     0.0047],
        [    0.9552,     0.0422,     0.0021,     0.0002,     0.0003],
        [    0.7902,     0.1792,     0.0283,     0.0012,     0.0011],
        [    0.0001,     0.0001,     0.0023,     0.9933,     0.0043],
        [    0.0001,     0.0010,     0.1325,     0.8161,     0.0503],
        [    0.4548,     0.4134,     0.0883,     0.0151,     0.0284],
        [    0.9914,     0.0082,     0.0001,     0.0001,     0.0002],
        [    0.7138,     0.2744,     0.0111,     0.0003,     0.0004],
        [    0.1461,     0.7827,     0.0113,     0.0103,     0.0496],
        [    0.5178,     0.4399,     0.0406,     0.0010,     0.0008],
        [    0.0005,     0.0006,     0.0089,     0.8792,     0.1108],
        [    0.1745,     0.2219,     0.3828,     0.1012,     0.1196],
        [    0.7825,     0.1938,     0.0218,     0.0013,     0.0007],
        [    0.0001,     0.0002,     0.0016,     0.1337,     0.8644]],
       device='cuda:0')
Metric:  tensor(0.7500, device='cuda:0')
------------------------
Mean loss[0.9393698051535484] | Mean metric[0.6031905807711079]
Stupid loss[0.0] | Naive soulution metric[0.2]
Labels:  tensor([0, 4, 0, 3, 0, 1, 2, 1, 4, 2, 1, 1, 4, 3, 1, 3], device='cuda:1')
Preds:  tensor([0, 4, 0, 3, 0, 0, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2], device='cuda:1')
Outputs:  tensor([[    0.6538,     0.3299,     0.0158,     0.0002,     0.0003],
        [    0.0003,     0.0002,     0.0014,     0.0566,     0.9415],
        [    0.8358,     0.1550,     0.0088,     0.0004,     0.0001],
        [    0.0000,     0.0002,     0.0322,     0.8878,     0.0797],
        [    0.7598,     0.2365,     0.0036,     0.0001,     0.0000],
        [    0.7247,     0.2585,     0.0162,     0.0004,     0.0001],
        [    0.1355,     0.3572,     0.4531,     0.0503,     0.0038],
        [    0.0542,     0.3087,     0.6107,     0.0249,     0.0015],
        [    0.0926,     0.1260,     0.4104,     0.3337,     0.0373],
        [    0.0006,     0.0037,     0.1663,     0.6420,     0.1874],
        [    0.0484,     0.6149,     0.3346,     0.0020,     0.0001],
        [    0.0154,     0.0947,     0.6179,     0.2695,     0.0024],
        [    0.0004,     0.0003,     0.0148,     0.5736,     0.4109],
        [    0.0001,     0.0002,     0.0393,     0.8462,     0.1142],
        [    0.0026,     0.0695,     0.8050,     0.1215,     0.0014],
        [    0.0102,     0.2591,     0.6959,     0.0324,     0.0024]],
       device='cuda:1')
Metric:  tensor(0.5000, device='cuda:1')
------------------------
Mean loss[0.9348582609971597] | Mean metric[0.6045021961932651]
Stupid loss[0.0] | Naive soulution metric[0.2]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Distance of parambert.embeddings.word_embeddings.weight: 0.0
Distance of parambert.embeddings.position_embeddings.weight: 0.0
Distance of parambert.embeddings.token_type_embeddings.weight: 0.0
Distance of parambert.embeddings.LayerNorm.weight: 0.0
Distance of parambert.embeddings.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.0.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.0.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.0.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.0.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.0.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.0.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.0.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.0.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.0.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.0.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.0.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.0.output.dense.weight: 0.0
Distance of parambert.encoder.layer.0.output.dense.bias: 0.0
Distance of parambert.encoder.layer.0.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.0.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.1.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.1.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.1.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.1.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.1.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.1.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.1.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.1.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.1.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.1.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.1.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.1.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.1.output.dense.weight: 0.0
Distance of parambert.encoder.layer.1.output.dense.bias: 0.0
Distance of parambert.encoder.layer.1.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.1.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.2.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.2.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.2.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.2.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.2.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.2.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.2.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.2.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.2.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.2.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.2.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.2.output.dense.weight: 0.0
Distance of parambert.encoder.layer.2.output.dense.bias: 0.0
Distance of parambert.encoder.layer.2.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.2.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.3.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.3.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.3.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.3.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.3.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.3.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.3.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.3.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.3.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.3.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.3.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.3.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.3.output.dense.weight: 0.0
Distance of parambert.encoder.layer.3.output.dense.bias: 0.0
Distance of parambert.encoder.layer.3.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.3.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.4.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.4.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.4.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.4.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.4.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.4.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.4.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.4.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.4.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.4.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.4.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.4.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.4.output.dense.weight: 0.0
Distance of parambert.encoder.layer.4.output.dense.bias: 0.0
Distance of parambert.encoder.layer.4.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.4.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.5.attention.self.query.weight: 0.0
Distance of parambert.encoder.layer.5.attention.self.query.bias: 0.0
Distance of parambert.encoder.layer.5.attention.self.key.weight: 0.0
Distance of parambert.encoder.layer.5.attention.self.key.bias: 0.0
Distance of parambert.encoder.layer.5.attention.self.value.weight: 0.0
Distance of parambert.encoder.layer.5.attention.self.value.bias: 0.0
Distance of parambert.encoder.layer.5.attention.output.dense.weight: 0.0
Distance of parambert.encoder.layer.5.attention.output.dense.bias: 0.0
Distance of parambert.encoder.layer.5.attention.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.5.attention.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.5.intermediate.dense.weight: 0.0
Distance of parambert.encoder.layer.5.intermediate.dense.bias: 0.0
Distance of parambert.encoder.layer.5.output.dense.weight: 0.0
Distance of parambert.encoder.layer.5.output.dense.bias: 0.0
Distance of parambert.encoder.layer.5.output.LayerNorm.weight: 0.0
Distance of parambert.encoder.layer.5.output.LayerNorm.bias: 0.0
Distance of parambert.encoder.layer.6.attention.self.query.weight: 9.887931823730469
Distance of parambert.encoder.layer.6.attention.self.query.bias: 0.21495100855827332
Distance of parambert.encoder.layer.6.attention.self.key.weight: 9.800729751586914
Distance of parambert.encoder.layer.6.attention.self.key.bias: 0.004994324874132872
Distance of parambert.encoder.layer.6.attention.self.value.weight: 9.285116195678711
Distance of parambert.encoder.layer.6.attention.self.value.bias: 0.08522280305624008
Distance of parambert.encoder.layer.6.attention.output.dense.weight: 9.360760688781738
Distance of parambert.encoder.layer.6.attention.output.dense.bias: 0.3226233422756195
Distance of parambert.encoder.layer.6.attention.output.LayerNorm.weight: 0.8004459142684937
Distance of parambert.encoder.layer.6.attention.output.LayerNorm.bias: 0.32335159182548523
Distance of parambert.encoder.layer.6.intermediate.dense.weight: 21.66263771057129
Distance of parambert.encoder.layer.6.intermediate.dense.bias: 0.3378083109855652
Distance of parambert.encoder.layer.6.output.dense.weight: 19.503633499145508
Distance of parambert.encoder.layer.6.output.dense.bias: 0.1271132528781891
Distance of parambert.encoder.layer.6.output.LayerNorm.weight: 1.3715420961380005
Distance of parambert.encoder.layer.6.output.LayerNorm.bias: 0.19198225438594818
Distance of parambert.encoder.layer.7.attention.self.query.weight: 10.098793029785156
Distance of parambert.encoder.layer.7.attention.self.query.bias: 0.19126716256141663
Distance of parambert.encoder.layer.7.attention.self.key.weight: 9.913848876953125
Distance of parambert.encoder.layer.7.attention.self.key.bias: 0.0053238035179674625
Distance of parambert.encoder.layer.7.attention.self.value.weight: 9.178035736083984
Distance of parambert.encoder.layer.7.attention.self.value.bias: 0.09004232287406921
Distance of parambert.encoder.layer.7.attention.output.dense.weight: 9.054731369018555
Distance of parambert.encoder.layer.7.attention.output.dense.bias: 0.2603413760662079
Distance of parambert.encoder.layer.7.attention.output.LayerNorm.weight: 0.8657201528549194
Distance of parambert.encoder.layer.7.attention.output.LayerNorm.bias: 0.18553657829761505
Distance of parambert.encoder.layer.7.intermediate.dense.weight: 21.477453231811523
Distance of parambert.encoder.layer.7.intermediate.dense.bias: 0.35122057795524597
Distance of parambert.encoder.layer.7.output.dense.weight: 19.18505859375
Distance of parambert.encoder.layer.7.output.dense.bias: 0.12206616252660751
Distance of parambert.encoder.layer.7.output.LayerNorm.weight: 1.4814833402633667
Distance of parambert.encoder.layer.7.output.LayerNorm.bias: 0.13790307939052582
Distance of parambert.encoder.layer.8.attention.self.query.weight: 9.924836158752441
Distance of parambert.encoder.layer.8.attention.self.query.bias: 0.21702790260314941
Distance of parambert.encoder.layer.8.attention.self.key.weight: 9.775944709777832
Distance of parambert.encoder.layer.8.attention.self.key.bias: 0.0066503011621534824
Distance of parambert.encoder.layer.8.attention.self.value.weight: 8.867313385009766
Distance of parambert.encoder.layer.8.attention.self.value.bias: 0.08672790974378586
Distance of parambert.encoder.layer.8.attention.output.dense.weight: 8.734424591064453
Distance of parambert.encoder.layer.8.attention.output.dense.bias: 0.27968907356262207
Distance of parambert.encoder.layer.8.attention.output.LayerNorm.weight: 0.9000259041786194
Distance of parambert.encoder.layer.8.attention.output.LayerNorm.bias: 0.25720277428627014
Distance of parambert.encoder.layer.8.intermediate.dense.weight: 21.286745071411133
Distance of parambert.encoder.layer.8.intermediate.dense.bias: 0.3981660008430481
Distance of parambert.encoder.layer.8.output.dense.weight: 18.886642456054688
Distance of parambert.encoder.layer.8.output.dense.bias: 0.15296407043933868
Distance of parambert.encoder.layer.8.output.LayerNorm.weight: 1.6550278663635254
Distance of parambert.encoder.layer.8.output.LayerNorm.bias: 0.12961554527282715
Distance of parambert.encoder.layer.9.attention.self.query.weight: 9.99609088897705
Distance of parambert.encoder.layer.9.attention.self.query.bias: 0.23910462856292725
Distance of parambert.encoder.layer.9.attention.self.key.weight: 9.866815567016602
Distance of parambert.encoder.layer.9.attention.self.key.bias: 0.005843049846589565
Distance of parambert.encoder.layer.9.attention.self.value.weight: 8.411100387573242
Distance of parambert.encoder.layer.9.attention.self.value.bias: 0.1055138111114502
Distance of parambert.encoder.layer.9.attention.output.dense.weight: 8.249357223510742
Distance of parambert.encoder.layer.9.attention.output.dense.bias: 0.3219423294067383
Distance of parambert.encoder.layer.9.attention.output.LayerNorm.weight: 1.0538983345031738
Distance of parambert.encoder.layer.9.attention.output.LayerNorm.bias: 0.3277609646320343
Distance of parambert.encoder.layer.9.intermediate.dense.weight: 20.795246124267578
Distance of parambert.encoder.layer.9.intermediate.dense.bias: 0.4913843274116516
Distance of parambert.encoder.layer.9.output.dense.weight: 18.40884017944336
Distance of parambert.encoder.layer.9.output.dense.bias: 0.2116076946258545
Distance of parambert.encoder.layer.9.output.LayerNorm.weight: 1.841841459274292
Distance of parambert.encoder.layer.9.output.LayerNorm.bias: 0.16596674919128418
Distance of parambert.encoder.layer.10.attention.self.query.weight: 9.66669750213623
Distance of parambert.encoder.layer.10.attention.self.query.bias: 0.23482219874858856
Distance of parambert.encoder.layer.10.attention.self.key.weight: 9.63133716583252
Distance of parambert.encoder.layer.10.attention.self.key.bias: 0.0054488941095769405
Distance of parambert.encoder.layer.10.attention.self.value.weight: 7.909025192260742
Distance of parambert.encoder.layer.10.attention.self.value.bias: 0.12053657323122025
Distance of parambert.encoder.layer.10.attention.output.dense.weight: 7.497908592224121
Distance of parambert.encoder.layer.10.attention.output.dense.bias: 0.30822911858558655
Distance of parambert.encoder.layer.10.attention.output.LayerNorm.weight: 1.3669058084487915
Distance of parambert.encoder.layer.10.attention.output.LayerNorm.bias: 0.3595384657382965
Distance of parambert.encoder.layer.10.intermediate.dense.weight: 19.498823165893555
Distance of parambert.encoder.layer.10.intermediate.dense.bias: 0.7331405878067017
Distance of parambert.encoder.layer.10.output.dense.weight: 16.830686569213867
Distance of parambert.encoder.layer.10.output.dense.bias: 0.2989332377910614
Distance of parambert.encoder.layer.10.output.LayerNorm.weight: 2.1829233169555664
Distance of parambert.encoder.layer.10.output.LayerNorm.bias: 0.19866161048412323
Distance of parambert.encoder.layer.11.attention.self.query.weight: 9.063405990600586
Distance of parambert.encoder.layer.11.attention.self.query.bias: 0.2754600942134857
Distance of parambert.encoder.layer.11.attention.self.key.weight: 9.162816047668457
Distance of parambert.encoder.layer.11.attention.self.key.bias: 0.005033715628087521
Distance of parambert.encoder.layer.11.attention.self.value.weight: 7.091556549072266
Distance of parambert.encoder.layer.11.attention.self.value.bias: 0.1179051324725151
Distance of parambert.encoder.layer.11.attention.output.dense.weight: 6.652976036071777
Distance of parambert.encoder.layer.11.attention.output.dense.bias: 0.34315183758735657
Distance of parambert.encoder.layer.11.attention.output.LayerNorm.weight: 1.877869725227356
Distance of parambert.encoder.layer.11.attention.output.LayerNorm.bias: 0.4336029887199402
Distance of parambert.encoder.layer.11.intermediate.dense.weight: 20.51558494567871
Distance of parambert.encoder.layer.11.intermediate.dense.bias: 1.5144553184509277
Distance of parambert.encoder.layer.11.output.dense.weight: 15.283734321594238
Distance of parambert.encoder.layer.11.output.dense.bias: 0.4363690912723541
Distance of parambert.encoder.layer.11.output.LayerNorm.weight: 2.170612096786499
Distance of parambert.encoder.layer.11.output.LayerNorm.bias: 0.5238528847694397
Distance of parambert.pooler.dense.weight: 7.6026692390441895
Distance of parambert.pooler.dense.bias: 0.25732406973838806
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0007195472717285156 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 8.79548192024231 seconds
